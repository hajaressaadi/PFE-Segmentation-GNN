{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11717602,"sourceType":"datasetVersion","datasetId":7355439}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Sélection des bandes discriminantes pour les images hyperspectrales\n# Méthode : IOU VS ES IOU","metadata":{}},{"cell_type":"markdown","source":"**Dans ce notebook, nous développons une méthode de classification multiclasse en utilisant des réseaux de neurones MLP avec une approche de sélection de bandes basée sur l'analyse \"worst-case\". Nous chargeons les résultats de séparabilité calculés précédemment et implémentons deux stratégies de sélection : la première sélectionne les meilleures bandes selon leur score worst-case global (top 5, 10, 15, 20), tandis que la seconde utilise une approche \"equal spacing\" qui divise le spectre en segments égaux et sélectionne la meilleure bande de chaque segment. Nous entraînons des modèles MLP avec architecture 512-64-64-32 incluant BatchNormalization et Dropout pour la classification de toutes les classes simultanément (incluant le background), puis comparons les performances en termes d'accuracy, temps d'entraînement et F1-score par classe pour déterminer la stratégie de sélection de bandes la plus efficace pour la classification multiclasse.**","metadata":{}},{"cell_type":"markdown","source":"### Dataset : Indian Pines ","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.io import loadmat\nfrom tqdm import tqdm\nimport os\nimport time\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, BatchNormalization, Input, Dropout, Activation\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.utils import to_categorical\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:14:59.199391Z","iopub.execute_input":"2025-05-07T16:14:59.199992Z","iopub.status.idle":"2025-05-07T16:14:59.204722Z","shell.execute_reply.started":"2025-05-07T16:14:59.199970Z","shell.execute_reply":"2025-05-07T16:14:59.204042Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Définir le chemin du dataset\ndataset_path = \"/kaggle/input/dataset-indian\"  # Chemin vers le dossier du dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:15:03.086080Z","iopub.execute_input":"2025-05-07T16:15:03.086412Z","iopub.status.idle":"2025-05-07T16:15:03.090721Z","shell.execute_reply.started":"2025-05-07T16:15:03.086390Z","shell.execute_reply":"2025-05-07T16:15:03.089664Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Affichage des variables \nprint(\"Variables dans Indian_pines.mat:\")\nchemin_image = os.path.join(dataset_path, \"Indian_pines.mat\")\ntry:\n    donnees_mat = loadmat(chemin_image)\n    image_keys = [key for key in donnees_mat.keys() if not key.startswith('__')]\n    print(image_keys)\n    print(f\"Forme des données: {donnees_mat[image_keys[0]].shape if len(image_keys) > 0 else 'N/A'}\")\nexcept Exception as e:\n    print(f\"Erreur lors du chargement: {e}\")\n\n# Afficher les variables du fichier d'image corrigée\nprint(\"\\nVariables dans Indian_pines_corrected.mat:\")\nchemin_image_corr = os.path.join(dataset_path, \"Indian_pines_corrected.mat\")\ntry:\n    donnees_mat_corr = loadmat(chemin_image_corr)\n    image_corr_keys = [key for key in donnees_mat_corr.keys() if not key.startswith('__')]\n    print(image_corr_keys)\n    print(f\"Forme des données: {donnees_mat_corr[image_corr_keys[0]].shape if len(image_corr_keys) > 0 else 'N/A'}\")\nexcept Exception as e:\n    print(f\"Erreur lors du chargement: {e}\")\n\n# Afficher les variables du fichier de vérité terrain\nprint(\"\\nVariables dans Indian_pines_gt.mat:\")\nchemin_gt = os.path.join(dataset_path, \"Indian_pines_gt.mat\")\ntry:\n    gt_mat = loadmat(chemin_gt)\n    gt_keys = [key for key in gt_mat.keys() if not key.startswith('__')]\n    print(gt_keys)\n    print(f\"Forme des données: {gt_mat[gt_keys[0]].shape if len(gt_keys) > 0 else 'N/A'}\")\nexcept Exception as e:\n    print(f\"Erreur lors du chargement: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:15:06.379718Z","iopub.execute_input":"2025-05-07T16:15:06.379957Z","iopub.status.idle":"2025-05-07T16:15:06.534917Z","shell.execute_reply.started":"2025-05-07T16:15:06.379940Z","shell.execute_reply":"2025-05-07T16:15:06.534197Z"}},"outputs":[{"name":"stdout","text":"Variables dans Indian_pines.mat:\n['indian_pines']\nForme des données: (145, 145, 220)\n\nVariables dans Indian_pines_corrected.mat:\n['indian_pines_corrected']\nForme des données: (145, 145, 200)\n\nVariables dans Indian_pines_gt.mat:\n['indian_pines_gt']\nForme des données: (145, 145)\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"###   ÉTAPE 1: Charger les données hyperspectrales et les vérités terrain","metadata":{}},{"cell_type":"code","source":"def charger_donnees(dataset_path):\n    \"\"\"\n    Charge les images hyperspectrales et les vérités terrain.\n    \n    Args:\n        dataset_path: Chemin vers le dossier contenant les fichiers .mat\n    \n    Returns:\n        donnees_hyperspectrales: Données spectrales (n_rows, n_cols, n_bands)\n        verite_terrain: Vérités terrain (n_rows, n_cols)\n    \"\"\"\n    print(\"Chargement des données hyperspectrales...\")\n    \n    # Chargement des données corrigées\n    chemin_image = os.path.join(dataset_path, \"Indian_pines_corrected.mat\")\n    donnees_mat = loadmat(chemin_image)\n    \n    # Utiliser le nom de variable exact pour l'image hyperspectrale\n    donnees_hyperspectrales = donnees_mat['indian_pines_corrected']  # Variable: 'indian_pines_corrected'\n    print(f\"Dimensions de l'image hyperspectrale: {donnees_hyperspectrales.shape}\")\n    \n    # Charger la vérité terrain\n    chemin_gt = os.path.join(dataset_path, \"Indian_pines_gt.mat\")\n    gt_mat = loadmat(chemin_gt)\n    \n    # Utiliser le nom de variable exact pour la vérité terrain\n    verite_terrain = gt_mat['indian_pines_gt']  # Variable: 'indian_pines_gt'\n    print(f\"Dimensions de la vérité terrain: {verite_terrain.shape}\")\n    \n    # Afficher des informations sur les données chargées\n    print(f\"Nombre de classes uniques dans la vérité terrain: {len(np.unique(verite_terrain))}\")\n    print(f\"Classes uniques: {np.unique(verite_terrain)}\")\n    \n    return donnees_hyperspectrales, verite_terrain","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:15:09.676468Z","iopub.execute_input":"2025-05-07T16:15:09.677237Z","iopub.status.idle":"2025-05-07T16:15:09.681801Z","shell.execute_reply.started":"2025-05-07T16:15:09.677200Z","shell.execute_reply":"2025-05-07T16:15:09.681135Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"### ÉTAPE 2: Préparer les données pour l'analyse","metadata":{}},{"cell_type":"code","source":"def preparer_donnees(donnees_hyperspectrales, verite_terrain):\n    \"\"\"\n    Prépare les données pour l'analyse de séparabilité.\n    \n    Args:\n        donnees_hyperspectrales: Données spectrales (n_rows, n_cols, n_bands)\n        verite_terrain: Vérités terrain (n_rows, n_cols)\n    \n    Returns:\n        pixels: Données spectrales (n_pixels, n_bands)\n        classes: Étiquettes de classe pour chaque pixel (n_pixels)\n        classes_uniques: Liste des classes uniques\n        class_names: Noms des classes\n    \"\"\"\n    # Définir les noms des classes pour Indian Pines\n    # Les 16 classes + background (classe 0) d'après la documentation\n    class_names = [\n        \"Background\",             # 0\n        \"Alfalfa\",                # 1\n        \"Corn-notill\",            # 2\n        \"Corn-mintill\",           # 3\n        \"Corn\",                   # 4\n        \"Grass-pasture\",          # 5\n        \"Grass-trees\",            # 6\n        \"Grass-pasture-mowed\",    # 7\n        \"Hay-windrowed\",          # 8\n        \"Oats\",                   # 9\n        \"Soybean-notill\",         # 10\n        \"Soybean-mintill\",        # 11\n        \"Soybean-clean\",          # 12\n        \"Wheat\",                  # 13\n        \"Woods\",                  # 14\n        \"Buildings-Grass-Trees-Drives\", # 15\n        \"Stone-Steel-Towers\"      # 16\n    ]\n    \n    # Obtenir les dimensions des données\n    height, width, n_bands = donnees_hyperspectrales.shape\n    \n    # Réorganiser les données pour l'analyse\n    pixels = donnees_hyperspectrales.reshape(height * width, n_bands)\n    classes = verite_terrain.reshape(height * width)\n    \n    # Extraire les classes uniques (y compris le background - classe 0)\n    classes_uniques = np.unique(classes)\n    \n    print(f\"Données préparées: {pixels.shape[0]} pixels avec {pixels.shape[1]} bandes\")\n    print(f\"Nombre de classes (avec background): {len(classes_uniques)}\")\n    \n    # Compter le nombre de pixels par classe\n    for classe in classes_uniques:\n        n_pixels = np.sum(classes == classe)\n        nom_classe = class_names[classe] if classe < len(class_names) else f\"Classe {classe}\"\n        print(f\"Classe {classe} ({nom_classe}): {n_pixels} pixels\")\n    \n    return pixels, classes, classes_uniques, class_names","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:15:16.355205Z","iopub.execute_input":"2025-05-07T16:15:16.355873Z","iopub.status.idle":"2025-05-07T16:15:16.361899Z","shell.execute_reply.started":"2025-05-07T16:15:16.355849Z","shell.execute_reply":"2025-05-07T16:15:16.361128Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"### ÉTAPE 3: Fonction de calcul du chevauchement","metadata":{}},{"cell_type":"code","source":"def calculer_chevauchement(classe_A_min, classe_A_max, classe_B_min, classe_B_max):\n    \"\"\"\n    Calcule le chevauchement entre deux plages de valeurs.\n    \n    Args:\n        classe_A_min, classe_A_max: Valeurs min et max pour la classe A\n        classe_B_min, classe_B_max: Valeurs min et max pour la classe B\n    \n    Returns:\n        Chevauchement normalisé (0 signifie aucun chevauchement, valeur positive indique un chevauchement)\n    \"\"\"\n    # Calcul des bornes de chevauchement\n    a = max(classe_A_min, classe_B_min)  # La plus grande des valeurs minimales\n    b = min(classe_A_max, classe_B_max)  # La plus petite des valeurs maximales\n    \n    # Calcul du chevauchement brut\n    c = b - a\n# Calcul de l'étendue totale\n    etendue_totale = max(classe_A_max, classe_B_max) - min(classe_A_min, classe_B_min)\n    \n    # Normalisation du chevauchement\n    if etendue_totale > 0:\n        c_normalise = c / etendue_totale\n    else:\n        c_normalise = 0\n    \n    # Retourne max(0, c_normalise)\n    return max(0, c_normalise)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:15:25.516190Z","iopub.execute_input":"2025-05-07T16:15:25.516459Z","iopub.status.idle":"2025-05-07T16:15:25.522334Z","shell.execute_reply.started":"2025-05-07T16:15:25.516440Z","shell.execute_reply":"2025-05-07T16:15:25.521500Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"### ÉTAPE 4: Fonction principale pour calculer la séparabilité entre paires de classes","metadata":{}},{"cell_type":"code","source":"def calculer_separabilite_paires_classes(pixels, classes, classes_uniques, class_names):\n    \"\"\"\n    Calcule la séparabilité entre chaque paire de classes pour chaque bande spectrale.\n    \n    Args:\n        pixels: Données spectrales (n_pixels x n_bandes)\n        classes: Étiquettes de classe pour chaque pixel\n        classes_uniques: Liste des classes uniques à considérer\n        class_names: Noms des classes\n    \n    Returns:\n        DataFrame contenant les résultats de séparabilité par paires\n    \"\"\"\n    # Créer une liste pour stocker les résultats\n    resultats_list = []\n    \n    # Nombre total d'itérations pour la barre de progression\n    total_iterations = len(classes_uniques) * (len(classes_uniques) - 1) // 2 * pixels.shape[1]\n    \n    # Utiliser tqdm pour afficher une barre de progression\n    with tqdm(total=total_iterations, desc=\"Calcul de séparabilité par paires\") as pbar:\n        # Pour chaque paire de classes\n        for i, classe_A in enumerate(classes_uniques):\n            for classe_B in classes_uniques[i+1:]:  # Ne considérer que les paires uniques\n                # Obtenir les noms des classes\n                nom_classe_A = class_names[classe_A] if classe_A < len(class_names) else f\"Classe {classe_A}\"\n                nom_classe_B = class_names[classe_B] if classe_B < len(class_names) else f\"Classe {classe_B}\"\n                \n                # Créer les masques pour les deux classes\n                mask_A = classes == classe_A\n                mask_B = classes == classe_B\n                \n                # Pour chaque bande spectrale\n                for bande in range(pixels.shape[1]):\n                    # Extraire les valeurs de la bande pour les deux classes\n                    valeurs_A = pixels[mask_A, bande]\n                    valeurs_B = pixels[mask_B, bande]\n                    \n                    # Vérifier que les deux classes ont des pixels\n                    if len(valeurs_A) > 0 and len(valeurs_B) > 0:\n                        # Calculer les min et max pour chaque classe\n                        classe_A_min = np.min(valeurs_A)\n                        classe_A_max = np.max(valeurs_A)\n                        classe_B_min = np.min(valeurs_B)\n                        classe_B_max = np.max(valeurs_B)\n                        \n                        # Calculer le chevauchement normalisé\n                        chevauchement = calculer_chevauchement(classe_A_min, classe_A_max, classe_B_min, classe_B_max)\n                        \n                        # Calculer la séparabilité (1 - chevauchement)\n                        separabilite = 1 - chevauchement\n                        \n                        # Stocker les résultats dans la liste\n                        resultats_list.append({\n                            'ClasseA': int(classe_A),\n                            'ClasseB': int(classe_B),\n                            'NomClasseA': nom_classe_A,\n                            'NomClasseB': nom_classe_B,\n                            'Bande': int(bande),\n                            'Separabilite': float(separabilite),\n                            'Chevauchement': float(chevauchement)\n                        })\n                    \n                    # Mettre à jour la barre de progression\n                    pbar.update(1)\n    \n    # Créer un DataFrame à partir de la liste de résultats\n    resultats_paires = pd.DataFrame(resultats_list)\n    \n    return resultats_paires","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:15:35.042970Z","iopub.execute_input":"2025-05-07T16:15:35.043899Z","iopub.status.idle":"2025-05-07T16:15:35.051933Z","shell.execute_reply.started":"2025-05-07T16:15:35.043874Z","shell.execute_reply":"2025-05-07T16:15:35.051229Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"### ÉTAPE 5: Analyser et trier les bandes selon leur worst-case ","metadata":{}},{"cell_type":"code","source":"def analyser_worst_case(resultats_paires, top_n=20):\n    \"\"\"\n    Analyse les bandes par leur worst-case de séparabilité et affiche les top N.\n    \n    Args:\n        resultats_paires: DataFrame contenant les résultats de séparabilité\n        top_n: Nombre de meilleures bandes à afficher\n    \n    Returns:\n        DataFrame contenant les bandes triées par leur worst-case\n    \"\"\"\n    print(\"Identification du worst-case pour chaque bande...\")\n    \n    # Pour chaque bande, trouver le pire cas de séparabilité (minimum)\n    worst_case_par_bande = (resultats_paires\n                           .groupby('Bande')\n                           .agg({\n                               'Separabilite': 'min',  # Prendre la séparabilité minimum\n                               'Chevauchement': 'max'   # Le chevauchement maximum correspondant\n                           })\n                           .reset_index())\n\n    # Trier les bandes par leur worst-case de séparabilité (ordre décroissant)\n    worst_case_par_bande = worst_case_par_bande.sort_values('Separabilite', ascending=False)\n\n    # Afficher les top_n meilleures bandes selon leur worst-case\n    print(f\"\\nTop {top_n} des bandes selon leur pire cas de séparabilité:\")\n    print(worst_case_par_bande.head(top_n))\n    \n    # Sauvegarder le tableau complet\n    worst_case_par_bande.to_csv('worst_case_par_bande.csv', index=False)\n    print(f\"Tableau des worst-case par bande sauvegardé dans 'worst_case_par_bande.csv'\")\n    \n    return worst_case_par_bande\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:15:39.945115Z","iopub.execute_input":"2025-05-07T16:15:39.945414Z","iopub.status.idle":"2025-05-07T16:15:39.950958Z","shell.execute_reply.started":"2025-05-07T16:15:39.945394Z","shell.execute_reply":"2025-05-07T16:15:39.950257Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"### ÉTAPE 6: Fonction principale pour exécuter l'analyse complète.","metadata":{}},{"cell_type":"code","source":"def main():\n    \"\"\"\n    Fonction principale pour exécuter l'analyse complète.\n    \"\"\"\n    # Créer un dossier de sortie pour les résultats\n    output_dir = \"resultats_analyse_bandes\"\n    os.makedirs(output_dir, exist_ok=True)\n    os.chdir(output_dir)\n    \n    # 1. Charger les données\n    donnees_hyperspectrales, verite_terrain = charger_donnees(dataset_path)\n    \n    # 2. Préparer les données\n    pixels, classes, classes_uniques, class_names = preparer_donnees(donnees_hyperspectrales, verite_terrain)\n    \n    # 3. Calculer la séparabilité par paires\n    print(\"\\nCalcul de la séparabilité entre paires de classes pour chaque bande...\")\n    resultats_paires = calculer_separabilite_paires_classes(pixels, classes, classes_uniques, class_names)\n    \n    # 4. Afficher quelques statistiques\n    n_paires = len(set(resultats_paires['ClasseA'].astype(str) + \"_\" + resultats_paires['ClasseB'].astype(str)))\n    n_bandes = len(set(resultats_paires['Bande']))\n    \n    print(f\"Analyse terminée. Calculé la séparabilité pour {n_paires} paires de classes à travers {n_bandes} bandes.\")\n    print(f\"Dimensions du tableau de résultats: {resultats_paires.shape}\")\n    \n    # 5. Exporter les résultats complets au format CSV\n    resultats_paires.to_csv('separabilite_paires_classes.csv', index=False)\n    print(\"Résultats exportés dans 'separabilite_paires_classes.csv'\")\n    \n    # 6. Analyser les bandes par leur worst-case et afficher les TOP 20\n    print(\"\\nAnalyse des bandes par leur worst-case...\")\n    worst_case_par_bande = analyser_worst_case(resultats_paires, top_n=20)\n    \n    # 7. Sélectionner les meilleures bandes selon le critère worst-case\n    n_bandes_a_selectionner = 20  # Nombre de bandes à sélectionner\n    bandes_selectionnees = worst_case_par_bande.head(n_bandes_a_selectionner)['Bande'].tolist()\n    \n    # 8. Sauvegarder la liste des bandes sélectionnées\n    np.savetxt('bandes_selectionnees.txt', bandes_selectionnees, fmt='%d')\n    print(f\"\\nLes {n_bandes_a_selectionner} meilleures bandes ont été sélectionnées et sauvegardées dans 'bandes_selectionnees.txt'\")\n    \n    print(\"\\nAnalyse terminée avec succès! Vous pouvez télécharger les fichiers suivants:\")\n    print(\"- separabilite_paires_classes.csv : Tableau complet des paires de classes pour chaque bande\")\n    print(\"- worst_case_par_bande.csv : Tableau des worst-case par bande\")\n    print(\"- bandes_selectionnees.txt : Liste des meilleures bandes sélectionnées\")\n    \n    return resultats_paires, worst_case_par_bande, bandes_selectionnees\n\n# Exécuter le programme principal\nif __name__ == \"__main__\":\n    resultats_paires, worst_case_par_bande, bandes_selectionnees = main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:15:43.504240Z","iopub.execute_input":"2025-05-07T16:15:43.504497Z","iopub.status.idle":"2025-05-07T16:15:47.392233Z","shell.execute_reply.started":"2025-05-07T16:15:43.504480Z","shell.execute_reply":"2025-05-07T16:15:47.391444Z"}},"outputs":[{"name":"stdout","text":"Chargement des données hyperspectrales...\nDimensions de l'image hyperspectrale: (145, 145, 200)\nDimensions de la vérité terrain: (145, 145)\nNombre de classes uniques dans la vérité terrain: 17\nClasses uniques: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16]\nDonnées préparées: 21025 pixels avec 200 bandes\nNombre de classes (avec background): 17\nClasse 0 (Background): 10776 pixels\nClasse 1 (Alfalfa): 46 pixels\nClasse 2 (Corn-notill): 1428 pixels\nClasse 3 (Corn-mintill): 830 pixels\nClasse 4 (Corn): 237 pixels\nClasse 5 (Grass-pasture): 483 pixels\nClasse 6 (Grass-trees): 730 pixels\nClasse 7 (Grass-pasture-mowed): 28 pixels\nClasse 8 (Hay-windrowed): 478 pixels\nClasse 9 (Oats): 20 pixels\nClasse 10 (Soybean-notill): 972 pixels\nClasse 11 (Soybean-mintill): 2455 pixels\nClasse 12 (Soybean-clean): 593 pixels\nClasse 13 (Wheat): 205 pixels\nClasse 14 (Woods): 1265 pixels\nClasse 15 (Buildings-Grass-Trees-Drives): 386 pixels\nClasse 16 (Stone-Steel-Towers): 93 pixels\n\nCalcul de la séparabilité entre paires de classes pour chaque bande...\n","output_type":"stream"},{"name":"stderr","text":"Calcul de séparabilité par paires:  14%|█▍        | 3781/27200 [00:00<00:04, 5759.01it/s]/tmp/ipykernel_31/1229269973.py:18: RuntimeWarning: overflow encountered in scalar subtract\n  c = b - a\nCalcul de séparabilité par paires: 100%|██████████| 27200/27200 [00:03<00:00, 7542.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"Analyse terminée. Calculé la séparabilité pour 136 paires de classes à travers 200 bandes.\nDimensions du tableau de résultats: (27200, 7)\nRésultats exportés dans 'separabilite_paires_classes.csv'\n\nAnalyse des bandes par leur worst-case...\nIdentification du worst-case pour chaque bande...\n\nTop 20 des bandes selon leur pire cas de séparabilité:\n     Bande  Separabilite  Chevauchement\n44      44      0.071371       0.928629\n142    142      0.063063       0.936937\n47      47      0.055490       0.944510\n38      38      0.054247       0.945753\n48      48      0.044321       0.955679\n52      52      0.039662       0.960338\n51      51      0.038882       0.961118\n102    102      0.037975       0.962025\n40      40      0.037552       0.962448\n42      42      0.036434       0.963566\n49      49      0.035108       0.964892\n104    104      0.033898       0.966102\n50      50      0.032051       0.967949\n41      41      0.031207       0.968793\n37      37      0.029887       0.970113\n43      43      0.027229       0.972771\n144    144      0.022727       0.977273\n145    145      0.021739       0.978261\n45      45      0.020833       0.979167\n93      93      0.014379       0.985621\nTableau des worst-case par bande sauvegardé dans 'worst_case_par_bande.csv'\n\nLes 20 meilleures bandes ont été sélectionnées et sauvegardées dans 'bandes_selectionnees.txt'\n\nAnalyse terminée avec succès! Vous pouvez télécharger les fichiers suivants:\n- separabilite_paires_classes.csv : Tableau complet des paires de classes pour chaque bande\n- worst_case_par_bande.csv : Tableau des worst-case par bande\n- bandes_selectionnees.txt : Liste des meilleures bandes sélectionnées\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"### Étape 7: Classification multiclasse avec MLP en utilisant différents ensembles de bandes discriminantes","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, BatchNormalization, Input, Dropout, Activation\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.utils import to_categorical\nimport os\nfrom scipy.io import loadmat\nfrom tqdm import tqdm\n\n# Définir explicitement les noms des classes\nclass_names_custom = [\n    'Background',\n    'Alfalfa',\n    'Corn-notill',\n    'Corn-mintill',\n    'Corn',\n    'Grass-pasture',\n    'Grass-trees',\n    'Grass-pasture-mowed',\n    'Hay-windrowed',\n    'Oats',\n    'Soybean-notill',\n    'Soybean-mintill',\n    'Soybean-clean',\n    'Wheat',\n    'Woods',\n    'Buildings-Grass-Trees-Drives',\n    'Stone-Steel-Towers'\n]\n\n# Définir le chemin du dataset\ndataset_path = \"/kaggle/input/dataset-indian\"\n\n# Charger les données hyperspectrales et vérités terrain\ndef charger_donnees_pour_mlp():\n    print(\"Chargement des données pour le MLP...\")\n    # Charger l'image hyperspectrale\n    chemin_image = os.path.join(dataset_path, \"Indian_pines_corrected.mat\")\n    donnees_mat = loadmat(chemin_image)\n    donnees_hyperspectrales = donnees_mat['indian_pines_corrected']\n    \n    # Charger la vérité terrain\n    chemin_gt = os.path.join(dataset_path, \"Indian_pines_gt.mat\")\n    gt_mat = loadmat(chemin_gt)\n    verite_terrain = gt_mat['indian_pines_gt']\n    \n    # Réorganiser les données\n    height, width, n_bands = donnees_hyperspectrales.shape\n    pixels = donnees_hyperspectrales.reshape(height * width, n_bands)\n    classes = verite_terrain.reshape(height * width)\n    \n    print(f\"Dimensions des données: pixels {pixels.shape}, classes {classes.shape}\")\n    return pixels, classes\n\n# Charger les données\npixels, classes = charger_donnees_pour_mlp()\n\n# Charger les résultats worst-case\nprint(\"Chargement des résultats worst-case...\")\nworst_case_par_bande = pd.read_csv('worst_case_par_bande.csv')\n\n# Sélection des meilleures bandes selon les différentes configurations\nprint(\"Sélection des meilleures bandes selon différentes configurations...\")\ntop5_bandes = worst_case_par_bande.head(5)['Bande'].values\ntop10_bandes = worst_case_par_bande.head(10)['Bande'].values\ntop15_bandes = worst_case_par_bande.head(15)['Bande'].values\ntop20_bandes = worst_case_par_bande.head(20)['Bande'].values\n\nprint(f\"Top 5 bandes: {top5_bandes}\")\nprint(f\"Top 10 bandes: {top10_bandes}\")\nprint(f\"Top 15 bandes: {top15_bandes}\")\nprint(f\"Top 20 bandes: {top20_bandes}\")\n\n# 1. Fonction pour préparer les données selon les bandes sélectionnées\ndef preparer_donnees_mlp(pixels, classes, bandes_selectionnees):\n    \"\"\"\n    Prépare les données pour l'entraînement avec les bandes sélectionnées.\n    Inclut la classe de fond (background).\n    \n    Args:\n        pixels: Données spectrales (n_pixels x n_bandes)\n        classes: Étiquettes de classe pour chaque pixel\n        bandes_selectionnees: Liste des indices des bandes à utiliser\n    \n    Returns:\n        X_train, X_test, y_train, y_test: Ensembles d'entraînement et de test\n    \"\"\"\n    # Utiliser tous les pixels, y compris la classe de fond (0)\n    X = pixels   # Toutes les bandes\n    y = classes  # Étiquettes (garder les indices originaux y compris 0)\n    \n    # Nombre de classes (incluant le fond)\n    n_classes = len(np.unique(y))\n    print(f\"Nombre de classes (avec background): {n_classes}\")\n    \n    # Sélectionner uniquement les bandes choisies\n    X_selected = X[:, bandes_selectionnees]\n    print(f\"Dimensions des données: {X_selected.shape}\")\n    \n    # Standardisation des données\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X_selected)\n    \n    # Conversion des étiquettes en format one-hot\n    y_onehot = to_categorical(y)\n    \n    # Diviser en ensembles d'entraînement et de test\n    X_train, X_test, y_train, y_test = train_test_split(\n        X_scaled, y_onehot, test_size=0.3, random_state=42, stratify=y\n    )\n    \n    print(f\"Ensemble d'entraînement: {X_train.shape}, {y_train.shape}\")\n    print(f\"Ensemble de test: {X_test.shape}, {y_test.shape}\")\n    \n    return X_train, X_test, y_train, y_test, n_classes\n\n# 2. Définition du modèle MLP multiclasse avec BatchNorm avant activation\ndef creer_modele_mlp_multiclasse(input_dim, n_classes):\n    \"\"\"\n    Crée un modèle MLP pour la classification multiclasse avec architecture 512-64-64-32\n    et BatchNorm avant activation.\n    \"\"\"\n    inputs = Input(shape=(input_dim,))\n    \n    # Première couche cachée\n    x = Dense(512)(inputs)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    \n    # Deuxième couche cachée\n    x = Dense(64)(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x = Dropout(0.3)(x)\n    \n    # Troisième couche cachée\n    x = Dense(64)(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    \n    # Quatrième couche cachée\n    x = Dense(32)(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    \n    # Couche de sortie avec softmax pour la classification multiclasse\n    outputs = Dense(n_classes, activation='softmax')(x)\n    \n    model = Model(inputs=inputs, outputs=outputs)\n    \n    # Compiler le modèle\n    model.compile(\n        optimizer=Adam(learning_rate=0.001),\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    \n    return model\n\n# 3. Fonction pour entraîner et évaluer un modèle\ndef entrainer_evaluer_modele(X_train, X_test, y_train, y_test, n_classes, \n                           bandes_selectionnees, nom_modele):\n    \"\"\"\n    Entraîne et évalue un modèle MLP avec les données fournies.\n    \n    Args:\n        X_train, X_test, y_train, y_test: Ensembles d'entraînement et de test\n        n_classes: Nombre de classes\n        bandes_selectionnees: Liste des indices des bandes utilisées\n        nom_modele: Nom pour sauvegarder le modèle et les résultats\n    \n    Returns:\n        model: Le modèle entraîné\n        history: L'historique d'entraînement\n        metrics: Dictionnaire des métriques d'évaluation\n    \"\"\"\n    # Créer un dossier pour les résultats de ce modèle\n    os.makedirs(f\"resultats_{nom_modele}\", exist_ok=True)\n    \n    print(f\"\\nCréation et entraînement du modèle {nom_modele}...\")\n    start_time = time.time()\n    \n    # Créer le modèle\n    model = creer_modele_mlp_multiclasse(input_dim=len(bandes_selectionnees), n_classes=n_classes)\n    model.summary()\n    \n    # Définir l'early stopping\n    early_stopping = EarlyStopping(\n        monitor='val_loss',\n        patience=15,\n        restore_best_weights=True,\n        verbose=1\n    )\n    \n    # Entraîner le modèle\n    history = model.fit(\n        X_train, y_train,\n        validation_split=0.2,\n        epochs=100,\n        batch_size=32,\n        callbacks=[early_stopping],\n        verbose=1\n    )\n    \n    train_time = time.time() - start_time\n    print(f\"\\nTemps d'entraînement: {train_time:.2f} secondes\")\n    \n    # Évaluation du modèle\n    print(\"\\nÉvaluation du modèle sur l'ensemble de test...\")\n    start_time = time.time()\n    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n    predict_time = time.time() - start_time\n    \n    print(f\"Précision (accuracy): {accuracy:.4f}\")\n    print(f\"Temps de prédiction: {predict_time:.2f} secondes\")\n    \n    # Générer les prédictions\n    y_pred_prob = model.predict(X_test, verbose=0)\n    y_pred = np.argmax(y_pred_prob, axis=1)\n    y_true = np.argmax(y_test, axis=1)\n    \n    # Utiliser tous les noms de classes, y compris le background\n    class_labels = class_names_custom[:n_classes]\n    \n    # Rapport de classification détaillé\n    report = classification_report(y_true, y_pred, target_names=class_labels, zero_division=0)\n    print(\"\\nRapport de classification:\")\n    print(report)\n    \n    # Sauvegarder le rapport dans un fichier\n    with open(f\"resultats_{nom_modele}/rapport_classification_{nom_modele}.txt\", \"w\") as f:\n        f.write(f\"Précision (accuracy): {accuracy:.4f}\\n\")\n        f.write(f\"Temps d'entraînement: {train_time:.2f} secondes\\n\")\n        f.write(f\"Temps de prédiction: {predict_time:.2f} secondes\\n\\n\")\n        f.write(report)\n    \n    # Visualisations\n    # Courbes d'apprentissage\n    plt.figure(figsize=(12, 5))\n    \n    # Loss\n    plt.subplot(1, 2, 1)\n    plt.plot(history.history['loss'], label='Training Loss')\n    plt.plot(history.history['val_loss'], label='Validation Loss')\n    plt.title('Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    \n    # Accuracy\n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['accuracy'], label='Training Accuracy')\n    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n    plt.title('Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    \n    plt.tight_layout()\n    plt.savefig(f\"resultats_{nom_modele}/learning_curves_{nom_modele}.png\")\n    plt.close()\n    \n    # Matrice de confusion\n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(16, 14))\n    \n    # Utiliser des étiquettes sécurisées pour les axes\n    x_labels = [label[:10] for label in class_labels]\n    y_labels = [label[:10] for label in class_labels]\n    \n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n                xticklabels=x_labels, \n                yticklabels=y_labels)\n    plt.title(f'Matrice de confusion - {nom_modele}')\n    plt.xlabel('Prédit')\n    plt.ylabel('Réel')\n    plt.tight_layout()\n    plt.savefig(f\"resultats_{nom_modele}/confusion_matrix_{nom_modele}.png\")\n    plt.close()\n    \n    # Précision par classe\n    # Récupérer le rapport sous forme de dictionnaire\n    report_dict = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n    \n    # Préparer les données pour la visualisation\n    precision_by_class = []\n    for i, classe in enumerate(class_labels):\n        if classe in report_dict:\n            classe_dict = report_dict[classe]\n            precision_by_class.append({\n                'Classe': classe,\n                'Précision': classe_dict['precision'],\n                'Rappel': classe_dict['recall'],\n                'F1-score': classe_dict['f1-score'],\n                'Support': classe_dict['support']\n            })\n    \n    # Créer le DataFrame et trier\n    precision_df = pd.DataFrame(precision_by_class)\n    if not precision_df.empty:\n        precision_df = precision_df.sort_values('F1-score', ascending=False)\n        \n        # Visualisation\n        plt.figure(figsize=(14, 8))\n        sns.barplot(x='Classe', y='F1-score', data=precision_df)\n        plt.title(f'F1-score par classe - {nom_modele}')\n        plt.xlabel('Classe')\n        plt.ylabel('F1-score')\n        plt.xticks(rotation=90)\n        plt.tight_layout()\n        plt.savefig(f\"resultats_{nom_modele}/f1score_by_class_{nom_modele}.png\")\n        plt.close()\n        \n        # Sauvegarder les F1-scores par classe\n        precision_df.to_csv(f\"resultats_{nom_modele}/f1scores_{nom_modele}.csv\", index=False)\n    else:\n        print(\"Impossible de créer la visualisation du F1-score par classe - données insuffisantes\")\n    \n    # Sauvegarde du modèle\n    model.save(f\"resultats_{nom_modele}/model_{nom_modele}.h5\")\n    print(f\"\\nModèle sauvegardé sous 'resultats_{nom_modele}/model_{nom_modele}.h5'\")\n    \n    # Enregistrer les informations sur les bandes sélectionnées\n    pd.DataFrame({\n        'Bande': bandes_selectionnees,\n        'Separabilite': [float(worst_case_par_bande[worst_case_par_bande['Bande'] == b]['Separabilite'].values[0]) \n                         for b in bandes_selectionnees]\n    }).to_csv(f\"resultats_{nom_modele}/bandes_selectionnees_{nom_modele}.csv\", index=False)\n    \n    # Rassembler les métriques pour la comparaison finale\n    metrics = {\n        'accuracy': accuracy,\n        'train_time': train_time,\n        'predict_time': predict_time,\n        'n_bands': len(bandes_selectionnees)\n    }\n    \n    return model, history, metrics\n\n# 4. Fonction principale pour entraîner tous les modèles\ndef entrainer_tous_modeles():\n    \"\"\"\n    Fonction principale pour entraîner et évaluer tous les modèles.\n    \"\"\"\n    # Créer un dossier de sortie pour les résultats globaux\n    os.makedirs(\"resultats_comparaison\", exist_ok=True)\n    \n    # Liste pour stocker les métriques de tous les modèles\n    all_metrics = []\n    \n    # 1. Modèle avec Top 5 bandes\n    print(\"\\n========== MODÈLE TOP 5 BANDES ==========\")\n    X_train, X_test, y_train, y_test, n_classes = preparer_donnees_mlp(pixels, classes, top5_bandes)\n    _, _, metrics_top5 = entrainer_evaluer_modele(\n        X_train, X_test, y_train, y_test, n_classes, top5_bandes, \"top5\"\n    )\n    metrics_top5['model'] = 'Top 5 bandes'\n    all_metrics.append(metrics_top5)\n    \n    # 2. Modèle avec Top 10 bandes\n    print(\"\\n========== MODÈLE TOP 10 BANDES ==========\")\n    X_train, X_test, y_train, y_test, n_classes = preparer_donnees_mlp(pixels, classes, top10_bandes)\n    _, _, metrics_top10 = entrainer_evaluer_modele(\n        X_train, X_test, y_train, y_test, n_classes, top10_bandes, \"top10\"\n    )\n    metrics_top10['model'] = 'Top 10 bandes'\n    all_metrics.append(metrics_top10)\n    \n    # 3. Modèle avec Top 15 bandes\n    print(\"\\n========== MODÈLE TOP 15 BANDES ==========\")\n    X_train, X_test, y_train, y_test, n_classes = preparer_donnees_mlp(pixels, classes, top15_bandes)\n    _, _, metrics_top15 = entrainer_evaluer_modele(\n        X_train, X_test, y_train, y_test, n_classes, top15_bandes, \"top15\"\n    )\n    metrics_top15['model'] = 'Top 15 bandes'\n    all_metrics.append(metrics_top15)\n    \n    # 4. Modèle avec Top 20 bandes\n    print(\"\\n========== MODÈLE TOP 20 BANDES ==========\")\n    X_train, X_test, y_train, y_test, n_classes = preparer_donnees_mlp(pixels, classes, top20_bandes)\n    _, _, metrics_top20 = entrainer_evaluer_modele(\n        X_train, X_test, y_train, y_test, n_classes, top20_bandes, \"top20\"\n    )\n    metrics_top20['model'] = 'Top 20 bandes'\n    all_metrics.append(metrics_top20)\n    \n    # Créer un tableau de comparaison\n    comparison_df = pd.DataFrame(all_metrics)\n    comparison_df = comparison_df[['model', 'n_bands', 'accuracy', 'train_time', 'predict_time']]\n    comparison_df.columns = ['Modèle', 'Nombre de bandes', 'Précision', 'Temps d\\'entraînement (s)', 'Temps de prédiction (s)']\n    \n    # Sauvegarder le tableau de comparaison\n    comparison_df.to_csv(\"resultats_comparaison/comparaison_modeles.csv\", index=False)\n    print(\"\\nTableau de comparaison sauvegardé dans 'resultats_comparaison/comparaison_modeles.csv'\")\n    \n    # Visualiser la comparaison des précisions\n    plt.figure(figsize=(10, 6))\n    sns.barplot(x='Modèle', y='Précision', data=comparison_df)\n    plt.title('Comparaison de la précision des modèles')\n    plt.xlabel('Modèle')\n    plt.ylabel('Précision')\n    plt.ylim(0, 1)\n    plt.tight_layout()\n    plt.savefig(\"resultats_comparaison/comparaison_precision.png\")\n    plt.close()\n    \n    # Visualiser la comparaison des temps d'entraînement\n    plt.figure(figsize=(10, 6))\n    sns.barplot(x='Modèle', y='Temps d\\'entraînement (s)', data=comparison_df)\n    plt.title('Comparaison des temps d\\'entraînement')\n    plt.xlabel('Modèle')\n    plt.ylabel('Temps d\\'entraînement (s)')\n    plt.tight_layout()\n    plt.savefig(\"resultats_comparaison/comparaison_temps_entrainement.png\")\n    plt.close()\n    \n    print(\"\\nAnalyse comparative terminée!\")\n    print(\"\\nRécapitulatif des précisions:\")\n    for metric in all_metrics:\n        print(f\"{metric['model']}: {metric['accuracy']:.4f}\")\n    \n    return comparison_df\n\n# Exécuter l'entraînement des modèles\nif __name__ == \"__main__\":\n    comparison_results = entrainer_tous_modeles()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:24:19.296574Z","iopub.execute_input":"2025-05-07T16:24:19.297140Z","iopub.status.idle":"2025-05-07T16:29:06.368520Z","shell.execute_reply.started":"2025-05-07T16:24:19.297117Z","shell.execute_reply":"2025-05-07T16:29:06.367893Z"}},"outputs":[{"name":"stdout","text":"Chargement des données pour le MLP...\nDimensions des données: pixels (21025, 200), classes (21025,)\nChargement des résultats worst-case...\nSélection des meilleures bandes selon différentes configurations...\nTop 5 bandes: [ 44 142  47  38  48]\nTop 10 bandes: [ 44 142  47  38  48  52  51 102  40  42]\nTop 15 bandes: [ 44 142  47  38  48  52  51 102  40  42  49 104  50  41  37]\nTop 20 bandes: [ 44 142  47  38  48  52  51 102  40  42  49 104  50  41  37  43 144 145\n  45  93]\n\n========== MODÈLE TOP 5 BANDES ==========\nNombre de classes (avec background): 17\nDimensions des données: (21025, 5)\nEnsemble d'entraînement: (14717, 5), (14717, 17)\nEnsemble de test: (6308, 5), (6308, 17)\n\nCréation et entraînement du modèle top5...\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1746635061.320227      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1746635061.320897      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)                   │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │           \u001b[38;5;34m3,072\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │           \u001b[38;5;34m2,048\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation (\u001b[38;5;33mActivation\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │          \u001b[38;5;34m32,832\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_1                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │             \u001b[38;5;34m256\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_1 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │           \u001b[38;5;34m4,160\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_2                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │             \u001b[38;5;34m256\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_2 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │           \u001b[38;5;34m2,080\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_3                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │             \u001b[38;5;34m128\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_3 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m)                  │             \u001b[38;5;34m561\u001b[0m │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)                   │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">3,072</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">32,832</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_1                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_2                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_3                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">561</span> │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m45,393\u001b[0m (177.32 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">45,393</span> (177.32 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m44,049\u001b[0m (172.07 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">44,049</span> (172.07 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,344\u001b[0m (5.25 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,344</span> (5.25 KB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/100\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1746635067.231110     109 service.cc:148] XLA service 0x7f6248027600 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1746635067.231925     109 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1746635067.231943     109 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\nI0000 00:00:1746635067.728376     109 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m 92/368\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3181 - loss: 2.4757","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1746635069.895798     109 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.4582 - loss: 1.9372 - val_accuracy: 0.5418 - val_loss: 1.4033\nEpoch 2/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5778 - loss: 1.2612 - val_accuracy: 0.6026 - val_loss: 1.1335\nEpoch 3/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5815 - loss: 1.1882 - val_accuracy: 0.5995 - val_loss: 1.1074\nEpoch 4/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5819 - loss: 1.1852 - val_accuracy: 0.6016 - val_loss: 1.0995\nEpoch 5/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5929 - loss: 1.1385 - val_accuracy: 0.6002 - val_loss: 1.1033\nEpoch 6/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5955 - loss: 1.1297 - val_accuracy: 0.6080 - val_loss: 1.0808\nEpoch 7/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5887 - loss: 1.1425 - val_accuracy: 0.6077 - val_loss: 1.0704\nEpoch 8/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5926 - loss: 1.1232 - val_accuracy: 0.6111 - val_loss: 1.0650\nEpoch 9/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5857 - loss: 1.1232 - val_accuracy: 0.6070 - val_loss: 1.0621\nEpoch 10/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6010 - loss: 1.1034 - val_accuracy: 0.6107 - val_loss: 1.0722\nEpoch 11/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5873 - loss: 1.1232 - val_accuracy: 0.6053 - val_loss: 1.0641\nEpoch 12/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5955 - loss: 1.1169 - val_accuracy: 0.6158 - val_loss: 1.0505\nEpoch 13/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5987 - loss: 1.0907 - val_accuracy: 0.6121 - val_loss: 1.0609\nEpoch 14/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5883 - loss: 1.1081 - val_accuracy: 0.6087 - val_loss: 1.0686\nEpoch 15/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5909 - loss: 1.1112 - val_accuracy: 0.6097 - val_loss: 1.0538\nEpoch 16/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5928 - loss: 1.0984 - val_accuracy: 0.6124 - val_loss: 1.0516\nEpoch 17/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5948 - loss: 1.0982 - val_accuracy: 0.6131 - val_loss: 1.0605\nEpoch 18/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5962 - loss: 1.0880 - val_accuracy: 0.6084 - val_loss: 1.0649\nEpoch 19/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5897 - loss: 1.1084 - val_accuracy: 0.6172 - val_loss: 1.0482\nEpoch 20/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5943 - loss: 1.0871 - val_accuracy: 0.6097 - val_loss: 1.0596\nEpoch 21/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5981 - loss: 1.0820 - val_accuracy: 0.6118 - val_loss: 1.0467\nEpoch 22/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5984 - loss: 1.0877 - val_accuracy: 0.6168 - val_loss: 1.0450\nEpoch 23/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5962 - loss: 1.0952 - val_accuracy: 0.6131 - val_loss: 1.0432\nEpoch 24/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5942 - loss: 1.0966 - val_accuracy: 0.6046 - val_loss: 1.0509\nEpoch 25/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5941 - loss: 1.0982 - val_accuracy: 0.6121 - val_loss: 1.0423\nEpoch 26/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5859 - loss: 1.0985 - val_accuracy: 0.6094 - val_loss: 1.0505\nEpoch 27/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5980 - loss: 1.0897 - val_accuracy: 0.6090 - val_loss: 1.0564\nEpoch 28/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5926 - loss: 1.0922 - val_accuracy: 0.6124 - val_loss: 1.0361\nEpoch 29/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6014 - loss: 1.0644 - val_accuracy: 0.6060 - val_loss: 1.0428\nEpoch 30/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5995 - loss: 1.0738 - val_accuracy: 0.6114 - val_loss: 1.0399\nEpoch 31/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6017 - loss: 1.0693 - val_accuracy: 0.6114 - val_loss: 1.0540\nEpoch 32/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6033 - loss: 1.0709 - val_accuracy: 0.6080 - val_loss: 1.0490\nEpoch 33/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5975 - loss: 1.0758 - val_accuracy: 0.6063 - val_loss: 1.0518\nEpoch 34/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5960 - loss: 1.0803 - val_accuracy: 0.6145 - val_loss: 1.0495\nEpoch 35/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5959 - loss: 1.0774 - val_accuracy: 0.6084 - val_loss: 1.0476\nEpoch 36/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6054 - loss: 1.0673 - val_accuracy: 0.6043 - val_loss: 1.0530\nEpoch 37/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5959 - loss: 1.0730 - val_accuracy: 0.6050 - val_loss: 1.0432\nEpoch 38/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5985 - loss: 1.0838 - val_accuracy: 0.6189 - val_loss: 1.0370\nEpoch 39/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5987 - loss: 1.0646 - val_accuracy: 0.6192 - val_loss: 1.0467\nEpoch 40/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5973 - loss: 1.0741 - val_accuracy: 0.6185 - val_loss: 1.0384\nEpoch 41/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5991 - loss: 1.0805 - val_accuracy: 0.6182 - val_loss: 1.0302\nEpoch 42/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6040 - loss: 1.0652 - val_accuracy: 0.6155 - val_loss: 1.0307\nEpoch 43/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6021 - loss: 1.0797 - val_accuracy: 0.6087 - val_loss: 1.0507\nEpoch 44/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5917 - loss: 1.0808 - val_accuracy: 0.6213 - val_loss: 1.0245\nEpoch 45/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6023 - loss: 1.0658 - val_accuracy: 0.6135 - val_loss: 1.0409\nEpoch 46/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5927 - loss: 1.0833 - val_accuracy: 0.6131 - val_loss: 1.0347\nEpoch 47/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6051 - loss: 1.0549 - val_accuracy: 0.6033 - val_loss: 1.0536\nEpoch 48/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6002 - loss: 1.0773 - val_accuracy: 0.6172 - val_loss: 1.0262\nEpoch 49/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6048 - loss: 1.0653 - val_accuracy: 0.6128 - val_loss: 1.0346\nEpoch 50/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6064 - loss: 1.0547 - val_accuracy: 0.6230 - val_loss: 1.0291\nEpoch 51/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5925 - loss: 1.0817 - val_accuracy: 0.6077 - val_loss: 1.0396\nEpoch 52/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6034 - loss: 1.0535 - val_accuracy: 0.6101 - val_loss: 1.0398\nEpoch 53/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6071 - loss: 1.0476 - val_accuracy: 0.6247 - val_loss: 1.0305\nEpoch 54/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5901 - loss: 1.0759 - val_accuracy: 0.6141 - val_loss: 1.0337\nEpoch 55/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6044 - loss: 1.0485 - val_accuracy: 0.6162 - val_loss: 1.0249\nEpoch 56/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6021 - loss: 1.0474 - val_accuracy: 0.6124 - val_loss: 1.0364\nEpoch 57/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6075 - loss: 1.0523 - val_accuracy: 0.6219 - val_loss: 1.0288\nEpoch 58/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6029 - loss: 1.0528 - val_accuracy: 0.6172 - val_loss: 1.0386\nEpoch 59/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5951 - loss: 1.0602 - val_accuracy: 0.6135 - val_loss: 1.0294\nEpoch 59: early stopping\nRestoring model weights from the end of the best epoch: 44.\n\nTemps d'entraînement: 60.54 secondes\n\nÉvaluation du modèle sur l'ensemble de test...\nPrécision (accuracy): 0.6218\nTemps de prédiction: 1.31 secondes\n\nRapport de classification:\n                              precision    recall  f1-score   support\n\n                  Background       0.72      0.89      0.79      3233\n                     Alfalfa       0.00      0.00      0.00        14\n                 Corn-notill       0.48      0.35      0.41       428\n                Corn-mintill       0.38      0.17      0.24       249\n                        Corn       0.64      0.20      0.30        71\n               Grass-pasture       0.00      0.00      0.00       145\n                 Grass-trees       0.00      0.00      0.00       219\n         Grass-pasture-mowed       0.00      0.00      0.00         8\n               Hay-windrowed       0.74      0.95      0.83       143\n                        Oats       0.00      0.00      0.00         6\n              Soybean-notill       0.25      0.02      0.03       292\n             Soybean-mintill       0.42      0.80      0.55       737\n               Soybean-clean       0.43      0.18      0.25       178\n                       Wheat       0.62      0.54      0.58        61\n                       Woods       0.33      0.07      0.11       380\nBuildings-Grass-Trees-Drives       0.00      0.00      0.00       116\n          Stone-Steel-Towers       0.70      0.57      0.63        28\n\n                    accuracy                           0.62      6308\n                   macro avg       0.34      0.28      0.28      6308\n                weighted avg       0.54      0.62      0.55      6308\n\nImpossible de créer la visualisation du F1-score par classe - données insuffisantes\n\nModèle sauvegardé sous 'resultats_top5/model_top5.h5'\n\n========== MODÈLE TOP 10 BANDES ==========\nNombre de classes (avec background): 17\nDimensions des données: (21025, 10)\nEnsemble d'entraînement: (14717, 10), (14717, 17)\nEnsemble de test: (6308, 10), (6308, 17)\n\nCréation et entraînement du modèle top10...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_1\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │           \u001b[38;5;34m5,632\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_4                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │           \u001b[38;5;34m2,048\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_4 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │          \u001b[38;5;34m32,832\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_5                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │             \u001b[38;5;34m256\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_5 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │           \u001b[38;5;34m4,160\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_6                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │             \u001b[38;5;34m256\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_6 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │           \u001b[38;5;34m2,080\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_7                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │             \u001b[38;5;34m128\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_7 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m)                  │             \u001b[38;5;34m561\u001b[0m │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">5,632</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_4                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">32,832</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_5                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_6                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_7                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">561</span> │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m47,953\u001b[0m (187.32 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">47,953</span> (187.32 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m46,609\u001b[0m (182.07 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">46,609</span> (182.07 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,344\u001b[0m (5.25 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,344</span> (5.25 KB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9ms/step - accuracy: 0.4358 - loss: 1.9804 - val_accuracy: 0.5676 - val_loss: 1.2941\nEpoch 2/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5833 - loss: 1.2494 - val_accuracy: 0.6016 - val_loss: 1.1361\nEpoch 3/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5859 - loss: 1.1913 - val_accuracy: 0.6039 - val_loss: 1.1070\nEpoch 4/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5924 - loss: 1.1459 - val_accuracy: 0.6053 - val_loss: 1.0953\nEpoch 5/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5870 - loss: 1.1457 - val_accuracy: 0.6121 - val_loss: 1.0613\nEpoch 6/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5908 - loss: 1.1143 - val_accuracy: 0.6131 - val_loss: 1.0440\nEpoch 7/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5905 - loss: 1.1115 - val_accuracy: 0.6043 - val_loss: 1.0545\nEpoch 8/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5861 - loss: 1.1165 - val_accuracy: 0.6097 - val_loss: 1.0390\nEpoch 9/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5986 - loss: 1.0899 - val_accuracy: 0.6148 - val_loss: 1.0360\nEpoch 10/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5911 - loss: 1.1027 - val_accuracy: 0.6053 - val_loss: 1.0431\nEpoch 11/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6011 - loss: 1.0808 - val_accuracy: 0.6067 - val_loss: 1.0661\nEpoch 12/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5926 - loss: 1.1053 - val_accuracy: 0.6165 - val_loss: 1.0317\nEpoch 13/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6036 - loss: 1.0716 - val_accuracy: 0.6118 - val_loss: 1.0237\nEpoch 14/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6092 - loss: 1.0611 - val_accuracy: 0.6118 - val_loss: 1.0302\nEpoch 15/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5984 - loss: 1.0706 - val_accuracy: 0.6216 - val_loss: 1.0197\nEpoch 16/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5917 - loss: 1.0767 - val_accuracy: 0.6165 - val_loss: 1.0296\nEpoch 17/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6047 - loss: 1.0655 - val_accuracy: 0.6050 - val_loss: 1.0589\nEpoch 18/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5996 - loss: 1.0630 - val_accuracy: 0.6223 - val_loss: 1.0116\nEpoch 19/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6026 - loss: 1.0685 - val_accuracy: 0.6199 - val_loss: 1.0109\nEpoch 20/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6066 - loss: 1.0602 - val_accuracy: 0.6199 - val_loss: 1.0204\nEpoch 21/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6039 - loss: 1.0618 - val_accuracy: 0.6294 - val_loss: 1.0086\nEpoch 22/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6060 - loss: 1.0583 - val_accuracy: 0.6087 - val_loss: 1.0308\nEpoch 23/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6115 - loss: 1.0594 - val_accuracy: 0.6223 - val_loss: 1.0056\nEpoch 24/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6038 - loss: 1.0566 - val_accuracy: 0.6213 - val_loss: 1.0113\nEpoch 25/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6039 - loss: 1.0609 - val_accuracy: 0.6226 - val_loss: 1.0006\nEpoch 26/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6075 - loss: 1.0512 - val_accuracy: 0.6145 - val_loss: 1.0208\nEpoch 27/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6084 - loss: 1.0499 - val_accuracy: 0.6291 - val_loss: 1.0013\nEpoch 28/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6132 - loss: 1.0344 - val_accuracy: 0.6318 - val_loss: 0.9959\nEpoch 29/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6022 - loss: 1.0571 - val_accuracy: 0.6168 - val_loss: 1.0150\nEpoch 30/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6008 - loss: 1.0470 - val_accuracy: 0.6213 - val_loss: 1.0077\nEpoch 31/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6100 - loss: 1.0369 - val_accuracy: 0.6240 - val_loss: 0.9881\nEpoch 32/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6024 - loss: 1.0465 - val_accuracy: 0.6253 - val_loss: 1.0055\nEpoch 33/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6058 - loss: 1.0328 - val_accuracy: 0.6298 - val_loss: 1.0067\nEpoch 34/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6098 - loss: 1.0395 - val_accuracy: 0.6253 - val_loss: 0.9995\nEpoch 35/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6067 - loss: 1.0490 - val_accuracy: 0.6345 - val_loss: 0.9832\nEpoch 36/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6143 - loss: 1.0346 - val_accuracy: 0.6315 - val_loss: 0.9906\nEpoch 37/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6141 - loss: 1.0192 - val_accuracy: 0.6264 - val_loss: 1.0031\nEpoch 38/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6166 - loss: 1.0351 - val_accuracy: 0.6213 - val_loss: 0.9865\nEpoch 39/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6116 - loss: 1.0248 - val_accuracy: 0.6311 - val_loss: 1.0066\nEpoch 40/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6124 - loss: 1.0290 - val_accuracy: 0.6206 - val_loss: 0.9972\nEpoch 41/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6110 - loss: 1.0267 - val_accuracy: 0.6359 - val_loss: 0.9906\nEpoch 42/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6119 - loss: 1.0387 - val_accuracy: 0.6141 - val_loss: 1.0125\nEpoch 43/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6124 - loss: 1.0249 - val_accuracy: 0.6213 - val_loss: 1.0002\nEpoch 44/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6072 - loss: 1.0358 - val_accuracy: 0.6206 - val_loss: 1.0075\nEpoch 45/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6110 - loss: 1.0251 - val_accuracy: 0.6382 - val_loss: 0.9909\nEpoch 46/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6101 - loss: 1.0381 - val_accuracy: 0.6240 - val_loss: 0.9939\nEpoch 47/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6088 - loss: 1.0166 - val_accuracy: 0.6301 - val_loss: 0.9757\nEpoch 48/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6158 - loss: 1.0167 - val_accuracy: 0.6250 - val_loss: 0.9883\nEpoch 49/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6110 - loss: 1.0179 - val_accuracy: 0.6213 - val_loss: 0.9915\nEpoch 50/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6166 - loss: 1.0152 - val_accuracy: 0.6281 - val_loss: 0.9864\nEpoch 51/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6196 - loss: 1.0210 - val_accuracy: 0.6291 - val_loss: 0.9907\nEpoch 52/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6163 - loss: 1.0091 - val_accuracy: 0.6284 - val_loss: 0.9869\nEpoch 53/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6082 - loss: 1.0313 - val_accuracy: 0.6267 - val_loss: 0.9838\nEpoch 54/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6138 - loss: 1.0263 - val_accuracy: 0.6345 - val_loss: 0.9766\nEpoch 55/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6148 - loss: 1.0189 - val_accuracy: 0.6267 - val_loss: 0.9838\nEpoch 56/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6099 - loss: 1.0256 - val_accuracy: 0.6264 - val_loss: 0.9808\nEpoch 57/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6116 - loss: 1.0300 - val_accuracy: 0.6233 - val_loss: 0.9830\nEpoch 58/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6193 - loss: 0.9972 - val_accuracy: 0.6192 - val_loss: 0.9865\nEpoch 59/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6103 - loss: 1.0286 - val_accuracy: 0.6247 - val_loss: 0.9943\nEpoch 60/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6149 - loss: 1.0266 - val_accuracy: 0.6335 - val_loss: 0.9950\nEpoch 61/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6134 - loss: 1.0247 - val_accuracy: 0.6284 - val_loss: 0.9839\nEpoch 62/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6119 - loss: 1.0129 - val_accuracy: 0.6274 - val_loss: 0.9806\nEpoch 62: early stopping\nRestoring model weights from the end of the best epoch: 47.\n\nTemps d'entraînement: 57.59 secondes\n\nÉvaluation du modèle sur l'ensemble de test...\nPrécision (accuracy): 0.6359\nTemps de prédiction: 1.05 secondes\n\nRapport de classification:\n                              precision    recall  f1-score   support\n\n                  Background       0.72      0.88      0.79      3233\n                     Alfalfa       0.00      0.00      0.00        14\n                 Corn-notill       0.51      0.38      0.44       428\n                Corn-mintill       0.52      0.18      0.27       249\n                        Corn       0.76      0.18      0.30        71\n               Grass-pasture       0.78      0.32      0.46       145\n                 Grass-trees       0.54      0.18      0.27       219\n         Grass-pasture-mowed       0.00      0.00      0.00         8\n               Hay-windrowed       0.74      0.97      0.84       143\n                        Oats       0.00      0.00      0.00         6\n              Soybean-notill       0.25      0.06      0.10       292\n             Soybean-mintill       0.43      0.79      0.56       737\n               Soybean-clean       0.63      0.26      0.37       178\n                       Wheat       0.67      0.54      0.60        61\n                       Woods       0.00      0.00      0.00       380\nBuildings-Grass-Trees-Drives       0.00      0.00      0.00       116\n          Stone-Steel-Towers       0.59      0.79      0.68        28\n\n                    accuracy                           0.64      6308\n                   macro avg       0.42      0.33      0.33      6308\n                weighted avg       0.57      0.64      0.58      6308\n\nImpossible de créer la visualisation du F1-score par classe - données insuffisantes\n\nModèle sauvegardé sous 'resultats_top10/model_top10.h5'\n\n========== MODÈLE TOP 15 BANDES ==========\nNombre de classes (avec background): 17\nDimensions des données: (21025, 15)\nEnsemble d'entraînement: (14717, 15), (14717, 17)\nEnsemble de test: (6308, 15), (6308, 17)\n\nCréation et entraînement du modèle top15...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_2\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │           \u001b[38;5;34m8,192\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_8                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │           \u001b[38;5;34m2,048\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_8 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │          \u001b[38;5;34m32,832\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_9                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │             \u001b[38;5;34m256\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_9 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │           \u001b[38;5;34m4,160\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_10               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │             \u001b[38;5;34m256\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_10 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_13 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │           \u001b[38;5;34m2,080\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_11               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │             \u001b[38;5;34m128\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_11 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_14 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m)                  │             \u001b[38;5;34m561\u001b[0m │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,192</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_8                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">32,832</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_9                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_10               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_11               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">561</span> │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m50,513\u001b[0m (197.32 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">50,513</span> (197.32 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m49,169\u001b[0m (192.07 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">49,169</span> (192.07 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,344\u001b[0m (5.25 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,344</span> (5.25 KB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9ms/step - accuracy: 0.4603 - loss: 1.9371 - val_accuracy: 0.5975 - val_loss: 1.2380\nEpoch 2/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5850 - loss: 1.2274 - val_accuracy: 0.6026 - val_loss: 1.1410\nEpoch 3/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5825 - loss: 1.1830 - val_accuracy: 0.6016 - val_loss: 1.0887\nEpoch 4/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5892 - loss: 1.1455 - val_accuracy: 0.6114 - val_loss: 1.0781\nEpoch 5/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5859 - loss: 1.1197 - val_accuracy: 0.6084 - val_loss: 1.0799\nEpoch 6/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5951 - loss: 1.1047 - val_accuracy: 0.6084 - val_loss: 1.0581\nEpoch 7/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5951 - loss: 1.0968 - val_accuracy: 0.6158 - val_loss: 1.0307\nEpoch 8/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5918 - loss: 1.0980 - val_accuracy: 0.6226 - val_loss: 1.0199\nEpoch 9/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5963 - loss: 1.0817 - val_accuracy: 0.6158 - val_loss: 1.0278\nEpoch 10/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6025 - loss: 1.0759 - val_accuracy: 0.6196 - val_loss: 1.0092\nEpoch 11/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5982 - loss: 1.0771 - val_accuracy: 0.6189 - val_loss: 1.0320\nEpoch 12/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6068 - loss: 1.0683 - val_accuracy: 0.6318 - val_loss: 1.0091\nEpoch 13/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6121 - loss: 1.0418 - val_accuracy: 0.6196 - val_loss: 1.0042\nEpoch 14/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5983 - loss: 1.0580 - val_accuracy: 0.6233 - val_loss: 0.9980\nEpoch 15/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6111 - loss: 1.0384 - val_accuracy: 0.6236 - val_loss: 1.0004\nEpoch 16/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6119 - loss: 1.0420 - val_accuracy: 0.6182 - val_loss: 0.9996\nEpoch 17/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6129 - loss: 1.0280 - val_accuracy: 0.6301 - val_loss: 0.9936\nEpoch 18/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6043 - loss: 1.0535 - val_accuracy: 0.6355 - val_loss: 0.9838\nEpoch 19/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6049 - loss: 1.0396 - val_accuracy: 0.6172 - val_loss: 1.0008\nEpoch 20/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6095 - loss: 1.0300 - val_accuracy: 0.5938 - val_loss: 1.0672\nEpoch 21/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6044 - loss: 1.0380 - val_accuracy: 0.6328 - val_loss: 0.9753\nEpoch 22/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6182 - loss: 1.0118 - val_accuracy: 0.6192 - val_loss: 0.9922\nEpoch 23/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6147 - loss: 1.0134 - val_accuracy: 0.6338 - val_loss: 0.9828\nEpoch 24/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6111 - loss: 1.0232 - val_accuracy: 0.6301 - val_loss: 0.9676\nEpoch 25/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6141 - loss: 1.0155 - val_accuracy: 0.6352 - val_loss: 0.9968\nEpoch 26/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6140 - loss: 1.0192 - val_accuracy: 0.6162 - val_loss: 0.9983\nEpoch 27/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6103 - loss: 1.0195 - val_accuracy: 0.6362 - val_loss: 0.9650\nEpoch 28/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6244 - loss: 1.0061 - val_accuracy: 0.6192 - val_loss: 0.9850\nEpoch 29/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6172 - loss: 1.0111 - val_accuracy: 0.6369 - val_loss: 0.9733\nEpoch 30/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6147 - loss: 1.0078 - val_accuracy: 0.6372 - val_loss: 0.9749\nEpoch 31/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6198 - loss: 1.0119 - val_accuracy: 0.6365 - val_loss: 0.9646\nEpoch 32/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6147 - loss: 1.0083 - val_accuracy: 0.6311 - val_loss: 0.9715\nEpoch 33/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6186 - loss: 1.0048 - val_accuracy: 0.6365 - val_loss: 0.9559\nEpoch 34/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6283 - loss: 0.9836 - val_accuracy: 0.6338 - val_loss: 0.9574\nEpoch 35/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6188 - loss: 0.9874 - val_accuracy: 0.6355 - val_loss: 0.9743\nEpoch 36/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6155 - loss: 1.0021 - val_accuracy: 0.6440 - val_loss: 0.9606\nEpoch 37/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6208 - loss: 1.0077 - val_accuracy: 0.6308 - val_loss: 0.9898\nEpoch 38/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6145 - loss: 1.0021 - val_accuracy: 0.6406 - val_loss: 0.9718\nEpoch 39/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6154 - loss: 1.0063 - val_accuracy: 0.6488 - val_loss: 0.9529\nEpoch 40/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6201 - loss: 0.9994 - val_accuracy: 0.6501 - val_loss: 0.9537\nEpoch 41/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6138 - loss: 0.9977 - val_accuracy: 0.6457 - val_loss: 0.9524\nEpoch 42/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6248 - loss: 0.9882 - val_accuracy: 0.6369 - val_loss: 0.9574\nEpoch 43/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6262 - loss: 0.9856 - val_accuracy: 0.6342 - val_loss: 0.9591\nEpoch 44/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6208 - loss: 0.9957 - val_accuracy: 0.6396 - val_loss: 0.9513\nEpoch 45/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6258 - loss: 0.9975 - val_accuracy: 0.6410 - val_loss: 0.9477\nEpoch 46/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6222 - loss: 0.9801 - val_accuracy: 0.6427 - val_loss: 0.9389\nEpoch 47/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6241 - loss: 0.9910 - val_accuracy: 0.6369 - val_loss: 0.9583\nEpoch 48/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6292 - loss: 0.9843 - val_accuracy: 0.6399 - val_loss: 0.9539\nEpoch 49/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6226 - loss: 0.9903 - val_accuracy: 0.6467 - val_loss: 0.9426\nEpoch 50/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6134 - loss: 0.9944 - val_accuracy: 0.6386 - val_loss: 0.9527\nEpoch 51/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6158 - loss: 0.9900 - val_accuracy: 0.6416 - val_loss: 0.9487\nEpoch 52/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6301 - loss: 0.9736 - val_accuracy: 0.6382 - val_loss: 0.9457\nEpoch 53/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6204 - loss: 0.9738 - val_accuracy: 0.6491 - val_loss: 0.9386\nEpoch 54/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6305 - loss: 0.9705 - val_accuracy: 0.6467 - val_loss: 0.9369\nEpoch 55/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6221 - loss: 0.9896 - val_accuracy: 0.6376 - val_loss: 0.9605\nEpoch 56/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6247 - loss: 0.9803 - val_accuracy: 0.6416 - val_loss: 0.9472\nEpoch 57/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6281 - loss: 0.9660 - val_accuracy: 0.6389 - val_loss: 0.9540\nEpoch 58/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6297 - loss: 0.9752 - val_accuracy: 0.6382 - val_loss: 0.9401\nEpoch 59/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6231 - loss: 0.9844 - val_accuracy: 0.6450 - val_loss: 0.9348\nEpoch 60/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6309 - loss: 0.9915 - val_accuracy: 0.6427 - val_loss: 0.9405\nEpoch 61/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6294 - loss: 0.9738 - val_accuracy: 0.6365 - val_loss: 0.9568\nEpoch 62/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6295 - loss: 0.9729 - val_accuracy: 0.6464 - val_loss: 0.9283\nEpoch 63/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6145 - loss: 0.9929 - val_accuracy: 0.6440 - val_loss: 0.9337\nEpoch 64/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6256 - loss: 0.9654 - val_accuracy: 0.6495 - val_loss: 0.9351\nEpoch 65/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6169 - loss: 0.9741 - val_accuracy: 0.6345 - val_loss: 0.9476\nEpoch 66/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6234 - loss: 0.9827 - val_accuracy: 0.6433 - val_loss: 0.9367\nEpoch 67/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6249 - loss: 0.9681 - val_accuracy: 0.6474 - val_loss: 0.9340\nEpoch 68/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6275 - loss: 0.9763 - val_accuracy: 0.6386 - val_loss: 0.9443\nEpoch 69/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6229 - loss: 0.9716 - val_accuracy: 0.6491 - val_loss: 0.9436\nEpoch 70/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6188 - loss: 0.9833 - val_accuracy: 0.6389 - val_loss: 0.9559\nEpoch 71/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6238 - loss: 0.9708 - val_accuracy: 0.6410 - val_loss: 0.9391\nEpoch 72/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6265 - loss: 0.9611 - val_accuracy: 0.6444 - val_loss: 0.9389\nEpoch 73/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6266 - loss: 0.9587 - val_accuracy: 0.6372 - val_loss: 0.9387\nEpoch 74/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6294 - loss: 0.9747 - val_accuracy: 0.6410 - val_loss: 0.9412\nEpoch 75/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6341 - loss: 0.9647 - val_accuracy: 0.6569 - val_loss: 0.9315\nEpoch 76/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6326 - loss: 0.9674 - val_accuracy: 0.6457 - val_loss: 0.9447\nEpoch 77/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6257 - loss: 0.9717 - val_accuracy: 0.6427 - val_loss: 0.9575\nEpoch 77: early stopping\nRestoring model weights from the end of the best epoch: 62.\n\nTemps d'entraînement: 72.06 secondes\n\nÉvaluation du modèle sur l'ensemble de test...\nPrécision (accuracy): 0.6601\nTemps de prédiction: 1.03 secondes\n\nRapport de classification:\n                              precision    recall  f1-score   support\n\n                  Background       0.75      0.86      0.80      3233\n                     Alfalfa       0.00      0.00      0.00        14\n                 Corn-notill       0.55      0.44      0.49       428\n                Corn-mintill       0.47      0.25      0.33       249\n                        Corn       0.44      0.15      0.23        71\n               Grass-pasture       0.69      0.52      0.59       145\n                 Grass-trees       0.63      0.44      0.52       219\n         Grass-pasture-mowed       0.00      0.00      0.00         8\n               Hay-windrowed       0.72      1.00      0.84       143\n                        Oats       0.00      0.00      0.00         6\n              Soybean-notill       0.38      0.15      0.22       292\n             Soybean-mintill       0.48      0.77      0.59       737\n               Soybean-clean       0.60      0.46      0.52       178\n                       Wheat       0.66      0.85      0.74        61\n                       Woods       0.52      0.12      0.19       380\nBuildings-Grass-Trees-Drives       0.00      0.00      0.00       116\n          Stone-Steel-Towers       0.65      0.79      0.71        28\n\n                    accuracy                           0.66      6308\n                   macro avg       0.44      0.40      0.40      6308\n                weighted avg       0.63      0.66      0.62      6308\n\nImpossible de créer la visualisation du F1-score par classe - données insuffisantes\n\nModèle sauvegardé sous 'resultats_top15/model_top15.h5'\n\n========== MODÈLE TOP 20 BANDES ==========\nNombre de classes (avec background): 17\nDimensions des données: (21025, 20)\nEnsemble d'entraînement: (14717, 20), (14717, 17)\nEnsemble de test: (6308, 20), (6308, 17)\n\nCréation et entraînement du modèle top20...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_3\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ input_layer_3 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_15 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │          \u001b[38;5;34m10,752\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_12               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │           \u001b[38;5;34m2,048\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_12 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_16 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │          \u001b[38;5;34m32,832\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_13               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │             \u001b[38;5;34m256\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_13 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_17 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │           \u001b[38;5;34m4,160\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_14               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │             \u001b[38;5;34m256\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_14 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_18 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │           \u001b[38;5;34m2,080\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_15               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │             \u001b[38;5;34m128\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_15 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_19 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m)                  │             \u001b[38;5;34m561\u001b[0m │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ input_layer_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">10,752</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_12               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">32,832</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_13               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_14               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_15               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">561</span> │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m53,073\u001b[0m (207.32 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">53,073</span> (207.32 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m51,729\u001b[0m (202.07 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">51,729</span> (202.07 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,344\u001b[0m (5.25 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,344</span> (5.25 KB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9ms/step - accuracy: 0.4015 - loss: 2.0437 - val_accuracy: 0.6067 - val_loss: 1.2172\nEpoch 2/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5785 - loss: 1.2348 - val_accuracy: 0.6050 - val_loss: 1.1146\nEpoch 3/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5917 - loss: 1.1571 - val_accuracy: 0.6033 - val_loss: 1.0853\nEpoch 4/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5958 - loss: 1.1245 - val_accuracy: 0.6168 - val_loss: 1.0415\nEpoch 5/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5973 - loss: 1.1059 - val_accuracy: 0.6315 - val_loss: 1.0441\nEpoch 6/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5946 - loss: 1.0991 - val_accuracy: 0.6352 - val_loss: 1.0091\nEpoch 7/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6033 - loss: 1.0702 - val_accuracy: 0.6165 - val_loss: 1.0266\nEpoch 8/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5964 - loss: 1.0747 - val_accuracy: 0.6382 - val_loss: 1.0016\nEpoch 9/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6069 - loss: 1.0521 - val_accuracy: 0.6433 - val_loss: 0.9808\nEpoch 10/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6141 - loss: 1.0408 - val_accuracy: 0.6294 - val_loss: 0.9848\nEpoch 11/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6113 - loss: 1.0282 - val_accuracy: 0.6457 - val_loss: 0.9681\nEpoch 12/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6161 - loss: 1.0346 - val_accuracy: 0.6529 - val_loss: 0.9640\nEpoch 13/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6165 - loss: 1.0077 - val_accuracy: 0.6325 - val_loss: 0.9721\nEpoch 14/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6207 - loss: 1.0203 - val_accuracy: 0.6423 - val_loss: 0.9637\nEpoch 15/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6125 - loss: 1.0311 - val_accuracy: 0.6362 - val_loss: 0.9658\nEpoch 16/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6178 - loss: 1.0191 - val_accuracy: 0.6495 - val_loss: 0.9381\nEpoch 17/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6277 - loss: 1.0020 - val_accuracy: 0.6216 - val_loss: 1.0022\nEpoch 18/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6251 - loss: 1.0025 - val_accuracy: 0.6406 - val_loss: 0.9456\nEpoch 19/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6307 - loss: 0.9962 - val_accuracy: 0.6454 - val_loss: 0.9500\nEpoch 20/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6232 - loss: 1.0063 - val_accuracy: 0.6491 - val_loss: 0.9339\nEpoch 21/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6302 - loss: 0.9888 - val_accuracy: 0.6185 - val_loss: 0.9721\nEpoch 22/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6318 - loss: 0.9819 - val_accuracy: 0.6539 - val_loss: 0.9239\nEpoch 23/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6261 - loss: 0.9929 - val_accuracy: 0.6532 - val_loss: 0.9215\nEpoch 24/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6340 - loss: 0.9832 - val_accuracy: 0.6495 - val_loss: 0.9391\nEpoch 25/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6356 - loss: 0.9727 - val_accuracy: 0.6647 - val_loss: 0.9080\nEpoch 26/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6249 - loss: 0.9789 - val_accuracy: 0.6532 - val_loss: 0.9162\nEpoch 27/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6332 - loss: 0.9877 - val_accuracy: 0.6627 - val_loss: 0.9183\nEpoch 28/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6365 - loss: 0.9594 - val_accuracy: 0.6566 - val_loss: 0.9131\nEpoch 29/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6334 - loss: 0.9668 - val_accuracy: 0.6600 - val_loss: 0.9018\nEpoch 30/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6305 - loss: 0.9619 - val_accuracy: 0.6444 - val_loss: 0.9414\nEpoch 31/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6309 - loss: 0.9615 - val_accuracy: 0.6529 - val_loss: 0.9098\nEpoch 32/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6368 - loss: 0.9594 - val_accuracy: 0.6695 - val_loss: 0.9008\nEpoch 33/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6346 - loss: 0.9643 - val_accuracy: 0.6590 - val_loss: 0.9024\nEpoch 34/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6435 - loss: 0.9600 - val_accuracy: 0.6573 - val_loss: 0.9026\nEpoch 35/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6338 - loss: 0.9551 - val_accuracy: 0.6498 - val_loss: 0.9137\nEpoch 36/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6350 - loss: 0.9669 - val_accuracy: 0.6467 - val_loss: 0.9180\nEpoch 37/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6501 - loss: 0.9353 - val_accuracy: 0.6607 - val_loss: 0.9021\nEpoch 38/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6329 - loss: 0.9387 - val_accuracy: 0.6610 - val_loss: 0.9018\nEpoch 39/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6515 - loss: 0.9320 - val_accuracy: 0.6675 - val_loss: 0.8818\nEpoch 40/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6430 - loss: 0.9500 - val_accuracy: 0.6675 - val_loss: 0.8930\nEpoch 41/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6375 - loss: 0.9532 - val_accuracy: 0.6698 - val_loss: 0.8940\nEpoch 42/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6459 - loss: 0.9419 - val_accuracy: 0.6671 - val_loss: 0.8851\nEpoch 43/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6345 - loss: 0.9442 - val_accuracy: 0.6593 - val_loss: 0.8965\nEpoch 44/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6460 - loss: 0.9328 - val_accuracy: 0.6647 - val_loss: 0.8839\nEpoch 45/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6500 - loss: 0.9379 - val_accuracy: 0.6566 - val_loss: 0.9014\nEpoch 46/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6406 - loss: 0.9397 - val_accuracy: 0.6641 - val_loss: 0.8945\nEpoch 47/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6373 - loss: 0.9442 - val_accuracy: 0.6634 - val_loss: 0.8979\nEpoch 48/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6400 - loss: 0.9471 - val_accuracy: 0.6692 - val_loss: 0.8823\nEpoch 49/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6493 - loss: 0.9260 - val_accuracy: 0.6603 - val_loss: 0.9050\nEpoch 50/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6491 - loss: 0.9307 - val_accuracy: 0.6634 - val_loss: 0.8890\nEpoch 51/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6454 - loss: 0.9206 - val_accuracy: 0.6539 - val_loss: 0.8928\nEpoch 52/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6573 - loss: 0.9084 - val_accuracy: 0.6512 - val_loss: 0.9308\nEpoch 53/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6486 - loss: 0.9293 - val_accuracy: 0.6685 - val_loss: 0.8765\nEpoch 54/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6511 - loss: 0.9163 - val_accuracy: 0.6736 - val_loss: 0.8728\nEpoch 55/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6478 - loss: 0.9241 - val_accuracy: 0.6637 - val_loss: 0.8879\nEpoch 56/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6470 - loss: 0.9331 - val_accuracy: 0.6637 - val_loss: 0.8790\nEpoch 57/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6474 - loss: 0.9200 - val_accuracy: 0.6539 - val_loss: 0.9115\nEpoch 58/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6406 - loss: 0.9340 - val_accuracy: 0.6396 - val_loss: 0.9539\nEpoch 59/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6483 - loss: 0.9283 - val_accuracy: 0.6658 - val_loss: 0.8899\nEpoch 60/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6472 - loss: 0.9146 - val_accuracy: 0.6654 - val_loss: 0.8866\nEpoch 61/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6510 - loss: 0.9137 - val_accuracy: 0.6583 - val_loss: 0.9091\nEpoch 62/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6472 - loss: 0.9229 - val_accuracy: 0.6709 - val_loss: 0.8796\nEpoch 63/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6453 - loss: 0.9253 - val_accuracy: 0.6556 - val_loss: 0.8902\nEpoch 64/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6562 - loss: 0.9073 - val_accuracy: 0.6668 - val_loss: 0.8818\nEpoch 65/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6454 - loss: 0.9092 - val_accuracy: 0.6613 - val_loss: 0.8816\nEpoch 66/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6400 - loss: 0.9238 - val_accuracy: 0.6525 - val_loss: 0.9069\nEpoch 67/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6405 - loss: 0.9208 - val_accuracy: 0.6715 - val_loss: 0.8794\nEpoch 68/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6573 - loss: 0.8999 - val_accuracy: 0.6681 - val_loss: 0.8726\nEpoch 69/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6449 - loss: 0.9133 - val_accuracy: 0.6712 - val_loss: 0.8720\nEpoch 70/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6520 - loss: 0.9146 - val_accuracy: 0.6525 - val_loss: 0.8889\nEpoch 71/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6448 - loss: 0.9246 - val_accuracy: 0.6763 - val_loss: 0.8642\nEpoch 72/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6459 - loss: 0.9173 - val_accuracy: 0.6753 - val_loss: 0.8740\nEpoch 73/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6512 - loss: 0.9099 - val_accuracy: 0.6569 - val_loss: 0.8785\nEpoch 74/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6502 - loss: 0.9043 - val_accuracy: 0.6675 - val_loss: 0.8723\nEpoch 75/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6566 - loss: 0.9090 - val_accuracy: 0.6715 - val_loss: 0.8738\nEpoch 76/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6585 - loss: 0.8993 - val_accuracy: 0.6681 - val_loss: 0.8734\nEpoch 77/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6438 - loss: 0.9219 - val_accuracy: 0.6681 - val_loss: 0.8690\nEpoch 78/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6574 - loss: 0.9060 - val_accuracy: 0.6770 - val_loss: 0.8608\nEpoch 79/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6461 - loss: 0.9247 - val_accuracy: 0.6668 - val_loss: 0.8762\nEpoch 80/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6531 - loss: 0.9039 - val_accuracy: 0.6715 - val_loss: 0.8658\nEpoch 81/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6535 - loss: 0.8985 - val_accuracy: 0.6736 - val_loss: 0.8654\nEpoch 82/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6529 - loss: 0.8923 - val_accuracy: 0.6668 - val_loss: 0.8793\nEpoch 83/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6472 - loss: 0.8991 - val_accuracy: 0.6675 - val_loss: 0.8732\nEpoch 84/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6541 - loss: 0.9015 - val_accuracy: 0.6671 - val_loss: 0.8797\nEpoch 85/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6541 - loss: 0.9088 - val_accuracy: 0.6760 - val_loss: 0.8668\nEpoch 86/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6456 - loss: 0.9108 - val_accuracy: 0.6688 - val_loss: 0.8746\nEpoch 87/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6620 - loss: 0.9011 - val_accuracy: 0.6681 - val_loss: 0.8776\nEpoch 88/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6604 - loss: 0.9091 - val_accuracy: 0.6661 - val_loss: 0.8711\nEpoch 89/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6498 - loss: 0.9058 - val_accuracy: 0.6675 - val_loss: 0.8839\nEpoch 90/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6565 - loss: 0.9006 - val_accuracy: 0.6698 - val_loss: 0.8856\nEpoch 91/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6499 - loss: 0.9165 - val_accuracy: 0.6685 - val_loss: 0.8710\nEpoch 92/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6557 - loss: 0.8924 - val_accuracy: 0.6610 - val_loss: 0.8871\nEpoch 93/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6552 - loss: 0.8877 - val_accuracy: 0.6668 - val_loss: 0.8714\nEpoch 93: early stopping\nRestoring model weights from the end of the best epoch: 78.\n\nTemps d'entraînement: 82.76 secondes\n\nÉvaluation du modèle sur l'ensemble de test...\nPrécision (accuracy): 0.6780\nTemps de prédiction: 1.04 secondes\n\nRapport de classification:\n                              precision    recall  f1-score   support\n\n                  Background       0.78      0.84      0.81      3233\n                     Alfalfa       0.00      0.00      0.00        14\n                 Corn-notill       0.55      0.41      0.47       428\n                Corn-mintill       0.65      0.31      0.42       249\n                        Corn       0.47      0.23      0.30        71\n               Grass-pasture       0.75      0.66      0.70       145\n                 Grass-trees       0.60      0.65      0.62       219\n         Grass-pasture-mowed       0.00      0.00      0.00         8\n               Hay-windrowed       0.73      1.00      0.85       143\n                        Oats       0.00      0.00      0.00         6\n              Soybean-notill       0.40      0.29      0.34       292\n             Soybean-mintill       0.49      0.78      0.60       737\n               Soybean-clean       0.67      0.46      0.54       178\n                       Wheat       0.78      0.85      0.81        61\n                       Woods       0.59      0.27      0.37       380\nBuildings-Grass-Trees-Drives       0.00      0.00      0.00       116\n          Stone-Steel-Towers       0.77      0.71      0.74        28\n\n                    accuracy                           0.68      6308\n                   macro avg       0.48      0.44      0.45      6308\n                weighted avg       0.66      0.68      0.66      6308\n\nImpossible de créer la visualisation du F1-score par classe - données insuffisantes\n\nModèle sauvegardé sous 'resultats_top20/model_top20.h5'\n\nTableau de comparaison sauvegardé dans 'resultats_comparaison/comparaison_modeles.csv'\n\nAnalyse comparative terminée!\n\nRécapitulatif des précisions:\nTop 5 bandes: 0.6218\nTop 10 bandes: 0.6359\nTop 15 bandes: 0.6601\nTop 20 bandes: 0.6780\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"### Étape 8: Classification multiclasse avec MLP en utilisant des bandes discriminantes par segments égaux","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, BatchNormalization, Input, Dropout, Activation\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.utils import to_categorical\nimport os\nfrom scipy.io import loadmat\nfrom tqdm import tqdm\n\n# Définir explicitement les noms des classes\nclass_names_custom = [\n    'Background',\n    'Alfalfa',\n    'Corn-notill',\n    'Corn-mintill',\n    'Corn',\n    'Grass-pasture',\n    'Grass-trees',\n    'Grass-pasture-mowed',\n    'Hay-windrowed',\n    'Oats',\n    'Soybean-notill',\n    'Soybean-mintill',\n    'Soybean-clean',\n    'Wheat',\n    'Woods',\n    'Buildings-Grass-Trees-Drives',\n    'Stone-Steel-Towers'\n]\n\n# Définir le chemin du dataset\ndataset_path = \"/kaggle/input/dataset-indian\"\n\n# Charger les données hyperspectrales et vérités terrain\ndef charger_donnees_pour_mlp():\n    print(\"Chargement des données pour le MLP...\")\n    # Charger l'image hyperspectrale\n    chemin_image = os.path.join(dataset_path, \"Indian_pines_corrected.mat\")\n    donnees_mat = loadmat(chemin_image)\n    donnees_hyperspectrales = donnees_mat['indian_pines_corrected']\n    \n    # Charger la vérité terrain\n    chemin_gt = os.path.join(dataset_path, \"Indian_pines_gt.mat\")\n    gt_mat = loadmat(chemin_gt)\n    verite_terrain = gt_mat['indian_pines_gt']\n    \n    # Réorganiser les données\n    height, width, n_bands = donnees_hyperspectrales.shape\n    pixels = donnees_hyperspectrales.reshape(height * width, n_bands)\n    classes = verite_terrain.reshape(height * width)\n    \n    print(f\"Dimensions des données: pixels {pixels.shape}, classes {classes.shape}\")\n    return pixels, classes\n\n# Charger les données\npixels, classes = charger_donnees_pour_mlp()\n\n# Charger les résultats worst-case\nprint(\"Chargement des résultats worst-case...\")\nworst_case_par_bande = pd.read_csv('worst_case_par_bande.csv')\n\n# Fonction pour sélectionner les meilleures bandes par segments égaux\ndef selectionner_meilleures_bandes_par_segment(worst_case_df, nb_segments):\n    \"\"\"\n    Sélectionne la meilleure bande (selon le critère worst-case) dans chaque segment spectral.\n    \n    Args:\n        worst_case_df: DataFrame contenant les résultats worst-case pour chaque bande\n        nb_segments: Nombre de segments spectraux à considérer\n    \n    Returns:\n        Liste des indices des bandes sélectionnées\n    \"\"\"\n    # Nombre total de bandes\n    nb_bandes_total = len(worst_case_df)\n    \n    # Taille approximative de chaque segment\n    taille_segment = nb_bandes_total // nb_segments\n    \n    bandes_selectionnees = []\n    \n    # Pour chaque segment spectral\n    for i in range(nb_segments):\n        # Calculer les limites du segment\n        debut = i * taille_segment\n        fin = min((i + 1) * taille_segment - 1, nb_bandes_total - 1)\n        \n        # Sélectionner les bandes dans ce segment\n        segment_df = worst_case_df[(worst_case_df['Bande'] >= debut) & (worst_case_df['Bande'] <= fin)]\n        \n        # Trier le segment par séparabilité décroissante\n        segment_df = segment_df.sort_values('Separabilite', ascending=False)\n        \n        # Trouver la bande avec la meilleure séparabilité dans ce segment\n        if not segment_df.empty:\n            meilleure_bande = segment_df.iloc[0]['Bande']\n            bandes_selectionnees.append(int(meilleure_bande))\n    \n    return bandes_selectionnees\n\n# Sélection des meilleures bandes selon les différentes configurations de segments\nprint(\"Sélection des meilleures bandes selon différentes configurations de segments...\")\nequal5_bandes = selectionner_meilleures_bandes_par_segment(worst_case_par_bande, 5)\nequal10_bandes = selectionner_meilleures_bandes_par_segment(worst_case_par_bande, 10)\nequal15_bandes = selectionner_meilleures_bandes_par_segment(worst_case_par_bande, 15)\nequal20_bandes = selectionner_meilleures_bandes_par_segment(worst_case_par_bande, 20)\n\nprint(f\"Equal 5 bandes (1 par segment): {equal5_bandes}\")\nprint(f\"Equal 10 bandes (1 par segment): {equal10_bandes}\")\nprint(f\"Equal 15 bandes (1 par segment): {equal15_bandes}\")\nprint(f\"Equal 20 bandes (1 par segment): {equal20_bandes}\")\n\n# 1. Fonction pour préparer les données selon les bandes sélectionnées\ndef preparer_donnees_mlp(pixels, classes, bandes_selectionnees):\n    \"\"\"\n    Prépare les données pour l'entraînement avec les bandes sélectionnées.\n    Inclut la classe de fond (background).\n    \n    Args:\n        pixels: Données spectrales (n_pixels x n_bandes)\n        classes: Étiquettes de classe pour chaque pixel\n        bandes_selectionnees: Liste des indices des bandes à utiliser\n    \n    Returns:\n        X_train, X_test, y_train, y_test: Ensembles d'entraînement et de test\n    \"\"\"\n    # Utiliser tous les pixels, y compris la classe de fond (0)\n    X = pixels   # Toutes les bandes\n    y = classes  # Étiquettes (garder les indices originaux y compris 0)\n    \n    # Nombre de classes (incluant le fond)\n    n_classes = len(np.unique(y))\n    print(f\"Nombre de classes (avec background): {n_classes}\")\n    \n    # Sélectionner uniquement les bandes choisies\n    X_selected = X[:, bandes_selectionnees]\n    print(f\"Dimensions des données: {X_selected.shape}\")\n    \n    # Standardisation des données\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X_selected)\n    \n    # Conversion des étiquettes en format one-hot\n    y_onehot = to_categorical(y)\n    \n    # Diviser en ensembles d'entraînement et de test\n    X_train, X_test, y_train, y_test = train_test_split(\n        X_scaled, y_onehot, test_size=0.3, random_state=42, stratify=y\n    )\n    \n    print(f\"Ensemble d'entraînement: {X_train.shape}, {y_train.shape}\")\n    print(f\"Ensemble de test: {X_test.shape}, {y_test.shape}\")\n    \n    return X_train, X_test, y_train, y_test, n_classes\n\n# 2. Définition du modèle MLP multiclasse avec BatchNorm avant activation\ndef creer_modele_mlp_multiclasse(input_dim, n_classes):\n    \"\"\"\n    Crée un modèle MLP pour la classification multiclasse avec architecture 512-64-64-32\n    et BatchNorm avant activation.\n    \"\"\"\n    inputs = Input(shape=(input_dim,))\n    \n    # Première couche cachée\n    x = Dense(512)(inputs)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    \n    # Deuxième couche cachée\n    x = Dense(64)(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x = Dropout(0.3)(x)\n    \n    # Troisième couche cachée\n    x = Dense(64)(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    \n    # Quatrième couche cachée\n    x = Dense(32)(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    \n    # Couche de sortie avec softmax pour la classification multiclasse\n    outputs = Dense(n_classes, activation='softmax')(x)\n    \n    model = Model(inputs=inputs, outputs=outputs)\n    \n    # Compiler le modèle\n    model.compile(\n        optimizer=Adam(learning_rate=0.001),\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    \n    return model\n\n# 3. Fonction pour entraîner et évaluer un modèle\ndef entrainer_evaluer_modele(X_train, X_test, y_train, y_test, n_classes, \n                           bandes_selectionnees, nom_modele):\n    \"\"\"\n    Entraîne et évalue un modèle MLP avec les données fournies.\n    \n    Args:\n        X_train, X_test, y_train, y_test: Ensembles d'entraînement et de test\n        n_classes: Nombre de classes\n        bandes_selectionnees: Liste des indices des bandes utilisées\n        nom_modele: Nom pour sauvegarder le modèle et les résultats\n    \n    Returns:\n        model: Le modèle entraîné\n        history: L'historique d'entraînement\n        metrics: Dictionnaire des métriques d'évaluation\n    \"\"\"\n    # Créer un dossier pour les résultats de ce modèle\n    os.makedirs(f\"resultats_{nom_modele}\", exist_ok=True)\n    \n    print(f\"\\nCréation et entraînement du modèle {nom_modele}...\")\n    start_time = time.time()\n    \n    # Créer le modèle\n    model = creer_modele_mlp_multiclasse(input_dim=len(bandes_selectionnees), n_classes=n_classes)\n    model.summary()\n    \n    # Définir l'early stopping\n    early_stopping = EarlyStopping(\n        monitor='val_loss',\n        patience=15,\n        restore_best_weights=True,\n        verbose=1\n    )\n    \n    # Entraîner le modèle\n    history = model.fit(\n        X_train, y_train,\n        validation_split=0.2,\n        epochs=100,\n        batch_size=32,\n        callbacks=[early_stopping],\n        verbose=1\n    )\n    \n    train_time = time.time() - start_time\n    print(f\"\\nTemps d'entraînement: {train_time:.2f} secondes\")\n    \n    # Évaluation du modèle\n    print(\"\\nÉvaluation du modèle sur l'ensemble de test...\")\n    start_time = time.time()\n    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n    predict_time = time.time() - start_time\n    \n    print(f\"Précision (accuracy): {accuracy:.4f}\")\n    print(f\"Temps de prédiction: {predict_time:.2f} secondes\")\n    \n    # Générer les prédictions\n    y_pred_prob = model.predict(X_test, verbose=0)\n    y_pred = np.argmax(y_pred_prob, axis=1)\n    y_true = np.argmax(y_test, axis=1)\n    \n    # Utiliser tous les noms de classes, y compris le background\n    class_labels = class_names_custom[:n_classes]\n    \n    # Rapport de classification détaillé\n    report = classification_report(y_true, y_pred, target_names=class_labels, zero_division=0)\n    print(\"\\nRapport de classification:\")\n    print(report)\n    \n    # Sauvegarder le rapport dans un fichier\n    with open(f\"resultats_{nom_modele}/rapport_classification_{nom_modele}.txt\", \"w\") as f:\n        f.write(f\"Précision (accuracy): {accuracy:.4f}\\n\")\n        f.write(f\"Temps d'entraînement: {train_time:.2f} secondes\\n\")\n        f.write(f\"Temps de prédiction: {predict_time:.2f} secondes\\n\\n\")\n        f.write(report)\n    \n    # Visualisations\n    # Courbes d'apprentissage\n    plt.figure(figsize=(12, 5))\n    \n    # Loss\n    plt.subplot(1, 2, 1)\n    plt.plot(history.history['loss'], label='Training Loss')\n    plt.plot(history.history['val_loss'], label='Validation Loss')\n    plt.title('Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    \n    # Accuracy\n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['accuracy'], label='Training Accuracy')\n    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n    plt.title('Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    \n    plt.tight_layout()\n    plt.savefig(f\"resultats_{nom_modele}/learning_curves_{nom_modele}.png\")\n    plt.close()\n    \n    # Matrice de confusion\n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(16, 14))\n    \n    # Utiliser des étiquettes sécurisées pour les axes\n    x_labels = [label[:10] for label in class_labels]\n    y_labels = [label[:10] for label in class_labels]\n    \n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n                xticklabels=x_labels, \n                yticklabels=y_labels)\n    plt.title(f'Matrice de confusion - {nom_modele}')\n    plt.xlabel('Prédit')\n    plt.ylabel('Réel')\n    plt.tight_layout()\n    plt.savefig(f\"resultats_{nom_modele}/confusion_matrix_{nom_modele}.png\")\n    plt.close()\n    \n    # Précision par classe\n    # Récupérer le rapport sous forme de dictionnaire\n    report_dict = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n    \n    # Préparer les données pour la visualisation\n    precision_by_class = []\n    for i, classe in enumerate(class_labels):\n        if classe in report_dict:\n            classe_dict = report_dict[classe]\n            precision_by_class.append({\n                'Classe': classe,\n                'Précision': classe_dict['precision'],\n                'Rappel': classe_dict['recall'],\n                'F1-score': classe_dict['f1-score'],\n                'Support': classe_dict['support']\n            })\n    \n    # Créer le DataFrame et trier\n    precision_df = pd.DataFrame(precision_by_class)\n    if not precision_df.empty:\n        precision_df = precision_df.sort_values('F1-score', ascending=False)\n        \n        # Visualisation\n        plt.figure(figsize=(14, 8))\n        sns.barplot(x='Classe', y='F1-score', data=precision_df)\n        plt.title(f'F1-score par classe - {nom_modele}')\n        plt.xlabel('Classe')\n        plt.ylabel('F1-score')\n        plt.xticks(rotation=90)\n        plt.tight_layout()\n        plt.savefig(f\"resultats_{nom_modele}/f1score_by_class_{nom_modele}.png\")\n        plt.close()\n        \n        # Sauvegarder les F1-scores par classe\n        precision_df.to_csv(f\"resultats_{nom_modele}/f1scores_{nom_modele}.csv\", index=False)\n    else:\n        print(\"Impossible de créer la visualisation du F1-score par classe - données insuffisantes\")\n    \n    # Sauvegarde du modèle\n    model.save(f\"resultats_{nom_modele}/model_{nom_modele}.h5\")\n    print(f\"\\nModèle sauvegardé sous 'resultats_{nom_modele}/model_{nom_modele}.h5'\")\n    \n    # Enregistrer les informations sur les bandes sélectionnées\n    pd.DataFrame({\n        'Bande': bandes_selectionnees,\n        'Segment': range(1, len(bandes_selectionnees) + 1),\n        'Separabilite': [float(worst_case_par_bande[worst_case_par_bande['Bande'] == b]['Separabilite'].values[0]) \n                         for b in bandes_selectionnees]\n    }).to_csv(f\"resultats_{nom_modele}/bandes_selectionnees_{nom_modele}.csv\", index=False)\n    \n    # Rassembler les métriques pour la comparaison finale\n    metrics = {\n        'accuracy': accuracy,\n        'train_time': train_time,\n        'predict_time': predict_time,\n        'n_bands': len(bandes_selectionnees)\n    }\n    \n    return model, history, metrics\n\n# 4. Fonction principale pour entraîner tous les modèles\ndef entrainer_tous_modeles_equal_spacing():\n    \"\"\"\n    Fonction principale pour entraîner et évaluer tous les modèles avec des bandes\n    sélectionnées par segments égaux.\n    \"\"\"\n    # Créer un dossier de sortie pour les résultats globaux\n    os.makedirs(\"resultats_comparaison_equal\", exist_ok=True)\n    \n    # Liste pour stocker les métriques de tous les modèles\n    all_metrics = []\n    \n    # 1. Modèle avec 5 segments égaux (1 bande par segment)\n    print(\"\\n========== MODÈLE EQUAL 5 BANDES ==========\")\n    X_train, X_test, y_train, y_test, n_classes = preparer_donnees_mlp(pixels, classes, equal5_bandes)\n    _, _, metrics_equal5 = entrainer_evaluer_modele(\n        X_train, X_test, y_train, y_test, n_classes, equal5_bandes, \"equal5\"\n    )\n    metrics_equal5['model'] = '5 segments (5 bandes)'\n    all_metrics.append(metrics_equal5)\n    \n    # 2. Modèle avec 10 segments égaux (1 bande par segment)\n    print(\"\\n========== MODÈLE EQUAL 10 BANDES ==========\")\n    X_train, X_test, y_train, y_test, n_classes = preparer_donnees_mlp(pixels, classes, equal10_bandes)\n    _, _, metrics_equal10 = entrainer_evaluer_modele(\n        X_train, X_test, y_train, y_test, n_classes, equal10_bandes, \"equal10\"\n    )\n    metrics_equal10['model'] = '10 segments (10 bandes)'\n    all_metrics.append(metrics_equal10)\n    \n    # 3. Modèle avec 15 segments égaux (1 bande par segment)\n    print(\"\\n========== MODÈLE EQUAL 15 BANDES ==========\")\n    X_train, X_test, y_train, y_test, n_classes = preparer_donnees_mlp(pixels, classes, equal15_bandes)\n    _, _, metrics_equal15 = entrainer_evaluer_modele(\n        X_train, X_test, y_train, y_test, n_classes, equal15_bandes, \"equal15\"\n    )\n    metrics_equal15['model'] = '15 segments (15 bandes)'\n    all_metrics.append(metrics_equal15)\n    \n    # 4. Modèle avec 20 segments égaux (1 bande par segment)\n    print(\"\\n========== MODÈLE EQUAL 20 BANDES ==========\")\n    X_train, X_test, y_train, y_test, n_classes = preparer_donnees_mlp(pixels, classes, equal20_bandes)\n    _, _, metrics_equal20 = entrainer_evaluer_modele(\n        X_train, X_test, y_train, y_test, n_classes, equal20_bandes, \"equal20\"\n    )\n    metrics_equal20['model'] = '20 segments (20 bandes)'\n    all_metrics.append(metrics_equal20)\n    \n    # Créer un tableau de comparaison\n    comparison_df = pd.DataFrame(all_metrics)\n    comparison_df = comparison_df[['model', 'n_bands', 'accuracy', 'train_time', 'predict_time']]\n    comparison_df.columns = ['Modèle', 'Nombre de bandes', 'Précision', 'Temps d\\'entraînement (s)', 'Temps de prédiction (s)']\n    \n    # Sauvegarder le tableau de comparaison\n    comparison_df.to_csv(\"resultats_comparaison_equal/comparaison_modeles_equal.csv\", index=False)\n    print(\"\\nTableau de comparaison sauvegardé dans 'resultats_comparaison_equal/comparaison_modeles_equal.csv'\")\n    \n    # Visualiser la comparaison des précisions\n    plt.figure(figsize=(10, 6))\n    sns.barplot(x='Modèle', y='Précision', data=comparison_df)\n    plt.title('Comparaison de la précision des modèles (Equal Spacing)')\n    plt.xlabel('Modèle')\n    plt.ylabel('Précision')\n    plt.ylim(0, 1)\n    plt.tight_layout()\n    plt.savefig(\"resultats_comparaison_equal/comparaison_precision_equal.png\")\n    plt.close()\n    \n    # Visualiser la comparaison des temps d'entraînement\n    plt.figure(figsize=(10, 6))\n    sns.barplot(x='Modèle', y='Temps d\\'entraînement (s)', data=comparison_df)\n    plt.title('Comparaison des temps d\\'entraînement (Equal Spacing)')\n    plt.xlabel('Modèle')\n    plt.ylabel('Temps d\\'entraînement (s)')\n    plt.tight_layout()\n    plt.savefig(\"resultats_comparaison_equal/comparaison_temps_entrainement_equal.png\")\n    plt.close()\n    \n    # Visualiser la distribution des bandes sélectionnées\n    plt.figure(figsize=(15, 8))\n    \n    # Créer une matrice pour représenter toutes les bandes\n    all_bands = np.zeros(200)\n    markers = ['o', 's', 'D', '^']\n    colors = ['blue', 'green', 'red', 'purple']\n    labels = ['5 segments', '10 segments', '15 segments', '20 segments']\n    \n    # Tracer la distribution des bandes pour chaque configuration\n    for i, bandes in enumerate([equal5_bandes, equal10_bandes, equal15_bandes, equal20_bandes]):\n        plt.scatter(bandes, np.ones(len(bandes))*i+1, marker=markers[i], \n                   color=colors[i], s=100, label=labels[i])\n    \n    # Ajouter des lignes verticales pour montrer les segments\n    for i in range(1, 20):\n        plt.axvline(x=i*10, color='gray', linestyle='--', alpha=0.3)\n    \n    plt.title('Distribution des bandes sélectionnées par l\\'approche Equal Spacing')\n    plt.xlabel('Indice de bande')\n    plt.yticks([1, 2, 3, 4], labels)\n    plt.xlim(-5, 205)\n    plt.grid(axis='x', alpha=0.3)\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(\"resultats_comparaison_equal/distribution_bandes_equal.png\")\n    plt.close()\n    \n    print(\"\\nAnalyse comparative terminée!\")\n    print(\"\\nRécapitulatif des précisions:\")\n    for metric in all_metrics:\n        print(f\"{metric['model']}: {metric['accuracy']:.4f}\")\n    \n    return comparison_df\n\n# Exécuter l'entraînement des modèles avec sélection par segments égaux\nif __name__ == \"__main__\":\n    comparison_results = entrainer_tous_modeles_equal_spacing()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:50:31.777760Z","iopub.execute_input":"2025-05-07T16:50:31.778302Z","iopub.status.idle":"2025-05-07T16:55:43.852733Z","shell.execute_reply.started":"2025-05-07T16:50:31.778262Z","shell.execute_reply":"2025-05-07T16:55:43.851863Z"}},"outputs":[{"name":"stdout","text":"Chargement des données pour le MLP...\nDimensions des données: pixels (21025, 200), classes (21025,)\nChargement des résultats worst-case...\nSélection des meilleures bandes selon différentes configurations de segments...\nEqual 5 bandes (1 par segment): [38, 44, 102, 142, 199]\nEqual 10 bandes (1 par segment): [0, 38, 44, 60, 93, 102, 125, 142, 163, 199]\nEqual 15 bandes (1 par segment): [0, 25, 38, 44, 52, 74, 84, 102, 104, 117, 142, 144, 159, 172, 194]\nEqual 20 bandes (1 par segment): [0, 19, 27, 38, 44, 52, 60, 74, 84, 93, 102, 116, 125, 130, 142, 159, 163, 172, 180, 199]\n\n========== MODÈLE EQUAL 5 BANDES ==========\nNombre de classes (avec background): 17\nDimensions des données: (21025, 5)\nEnsemble d'entraînement: (14717, 5), (14717, 17)\nEnsemble de test: (6308, 5), (6308, 17)\n\nCréation et entraînement du modèle equal5...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_4\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_4\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ input_layer_4 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)                   │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_20 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │           \u001b[38;5;34m3,072\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_16               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │           \u001b[38;5;34m2,048\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_16 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_21 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │          \u001b[38;5;34m32,832\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_17               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │             \u001b[38;5;34m256\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_17 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_22 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │           \u001b[38;5;34m4,160\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_18               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │             \u001b[38;5;34m256\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_18 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_23 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │           \u001b[38;5;34m2,080\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_19               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │             \u001b[38;5;34m128\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_19 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_24 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m)                  │             \u001b[38;5;34m561\u001b[0m │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ input_layer_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)                   │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">3,072</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_16               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">32,832</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_17               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_18               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_19               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">561</span> │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m45,393\u001b[0m (177.32 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">45,393</span> (177.32 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m44,049\u001b[0m (172.07 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">44,049</span> (172.07 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,344\u001b[0m (5.25 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,344</span> (5.25 KB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.3922 - loss: 2.1428 - val_accuracy: 0.5707 - val_loss: 1.3104\nEpoch 2/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5756 - loss: 1.2868 - val_accuracy: 0.5978 - val_loss: 1.1795\nEpoch 3/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5772 - loss: 1.2402 - val_accuracy: 0.5927 - val_loss: 1.1488\nEpoch 4/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5798 - loss: 1.2096 - val_accuracy: 0.5995 - val_loss: 1.1320\nEpoch 5/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5826 - loss: 1.1960 - val_accuracy: 0.6009 - val_loss: 1.1315\nEpoch 6/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5803 - loss: 1.1722 - val_accuracy: 0.5985 - val_loss: 1.1157\nEpoch 7/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5905 - loss: 1.1654 - val_accuracy: 0.6046 - val_loss: 1.1039\nEpoch 8/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5884 - loss: 1.1520 - val_accuracy: 0.5968 - val_loss: 1.1096\nEpoch 9/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5899 - loss: 1.1548 - val_accuracy: 0.6067 - val_loss: 1.0980\nEpoch 10/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5849 - loss: 1.1458 - val_accuracy: 0.6063 - val_loss: 1.0935\nEpoch 11/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5858 - loss: 1.1597 - val_accuracy: 0.6026 - val_loss: 1.0838\nEpoch 12/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5891 - loss: 1.1287 - val_accuracy: 0.6080 - val_loss: 1.0878\nEpoch 13/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5893 - loss: 1.1413 - val_accuracy: 0.5988 - val_loss: 1.0916\nEpoch 14/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5882 - loss: 1.1312 - val_accuracy: 0.6094 - val_loss: 1.0849\nEpoch 15/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5882 - loss: 1.1356 - val_accuracy: 0.6060 - val_loss: 1.0914\nEpoch 16/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5947 - loss: 1.1316 - val_accuracy: 0.6087 - val_loss: 1.0888\nEpoch 17/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5903 - loss: 1.1286 - val_accuracy: 0.6070 - val_loss: 1.0873\nEpoch 18/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5885 - loss: 1.1246 - val_accuracy: 0.6077 - val_loss: 1.0880\nEpoch 19/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5911 - loss: 1.1294 - val_accuracy: 0.6101 - val_loss: 1.0826\nEpoch 20/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5913 - loss: 1.1249 - val_accuracy: 0.6033 - val_loss: 1.0795\nEpoch 21/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5952 - loss: 1.1265 - val_accuracy: 0.6053 - val_loss: 1.0774\nEpoch 22/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5840 - loss: 1.1229 - val_accuracy: 0.6063 - val_loss: 1.0721\nEpoch 23/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5970 - loss: 1.1165 - val_accuracy: 0.6087 - val_loss: 1.0794\nEpoch 24/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5986 - loss: 1.1014 - val_accuracy: 0.5995 - val_loss: 1.1022\nEpoch 25/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5864 - loss: 1.1221 - val_accuracy: 0.5934 - val_loss: 1.0802\nEpoch 26/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5894 - loss: 1.1175 - val_accuracy: 0.6172 - val_loss: 1.0657\nEpoch 27/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5865 - loss: 1.1249 - val_accuracy: 0.6084 - val_loss: 1.0730\nEpoch 28/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5945 - loss: 1.1082 - val_accuracy: 0.6043 - val_loss: 1.0770\nEpoch 29/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5909 - loss: 1.1092 - val_accuracy: 0.6073 - val_loss: 1.0697\nEpoch 30/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5965 - loss: 1.1119 - val_accuracy: 0.6158 - val_loss: 1.0673\nEpoch 31/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5903 - loss: 1.1137 - val_accuracy: 0.6168 - val_loss: 1.0741\nEpoch 32/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5969 - loss: 1.0853 - val_accuracy: 0.6053 - val_loss: 1.0732\nEpoch 33/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5992 - loss: 1.1010 - val_accuracy: 0.6097 - val_loss: 1.0647\nEpoch 34/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5892 - loss: 1.1137 - val_accuracy: 0.6056 - val_loss: 1.0647\nEpoch 35/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5960 - loss: 1.1009 - val_accuracy: 0.6121 - val_loss: 1.0751\nEpoch 36/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6026 - loss: 1.0939 - val_accuracy: 0.6087 - val_loss: 1.0719\nEpoch 37/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5894 - loss: 1.1002 - val_accuracy: 0.6067 - val_loss: 1.0638\nEpoch 38/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5987 - loss: 1.1021 - val_accuracy: 0.6111 - val_loss: 1.0686\nEpoch 39/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5937 - loss: 1.1065 - val_accuracy: 0.6097 - val_loss: 1.0680\nEpoch 40/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5937 - loss: 1.0918 - val_accuracy: 0.6036 - val_loss: 1.0692\nEpoch 41/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5984 - loss: 1.1009 - val_accuracy: 0.6101 - val_loss: 1.0649\nEpoch 42/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5849 - loss: 1.1167 - val_accuracy: 0.6002 - val_loss: 1.0754\nEpoch 43/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5997 - loss: 1.0896 - val_accuracy: 0.6056 - val_loss: 1.0685\nEpoch 44/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5918 - loss: 1.1158 - val_accuracy: 0.6114 - val_loss: 1.0704\nEpoch 45/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5981 - loss: 1.1069 - val_accuracy: 0.6060 - val_loss: 1.0658\nEpoch 46/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5939 - loss: 1.0990 - val_accuracy: 0.6128 - val_loss: 1.0625\nEpoch 47/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5873 - loss: 1.1116 - val_accuracy: 0.6101 - val_loss: 1.0645\nEpoch 48/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5917 - loss: 1.0954 - val_accuracy: 0.6135 - val_loss: 1.0603\nEpoch 49/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5947 - loss: 1.1003 - val_accuracy: 0.6073 - val_loss: 1.0627\nEpoch 50/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5979 - loss: 1.0891 - val_accuracy: 0.6121 - val_loss: 1.0625\nEpoch 51/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5987 - loss: 1.0767 - val_accuracy: 0.6145 - val_loss: 1.0591\nEpoch 52/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5988 - loss: 1.0938 - val_accuracy: 0.6114 - val_loss: 1.0650\nEpoch 53/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5948 - loss: 1.0922 - val_accuracy: 0.6022 - val_loss: 1.0775\nEpoch 54/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5953 - loss: 1.0905 - val_accuracy: 0.6128 - val_loss: 1.0610\nEpoch 55/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5999 - loss: 1.0819 - val_accuracy: 0.6087 - val_loss: 1.0640\nEpoch 56/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6016 - loss: 1.0922 - val_accuracy: 0.6148 - val_loss: 1.0588\nEpoch 57/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6017 - loss: 1.0728 - val_accuracy: 0.6131 - val_loss: 1.0608\nEpoch 58/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6027 - loss: 1.0866 - val_accuracy: 0.6145 - val_loss: 1.0634\nEpoch 59/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5963 - loss: 1.0957 - val_accuracy: 0.6012 - val_loss: 1.0726\nEpoch 60/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6002 - loss: 1.0857 - val_accuracy: 0.6060 - val_loss: 1.0658\nEpoch 61/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6022 - loss: 1.0755 - val_accuracy: 0.6080 - val_loss: 1.0612\nEpoch 62/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6020 - loss: 1.0827 - val_accuracy: 0.6090 - val_loss: 1.0604\nEpoch 63/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6006 - loss: 1.0819 - val_accuracy: 0.6073 - val_loss: 1.0597\nEpoch 64/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5934 - loss: 1.0903 - val_accuracy: 0.6056 - val_loss: 1.0589\nEpoch 65/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6033 - loss: 1.0555 - val_accuracy: 0.6036 - val_loss: 1.0761\nEpoch 66/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5994 - loss: 1.0906 - val_accuracy: 0.6016 - val_loss: 1.0603\nEpoch 67/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6102 - loss: 1.0822 - val_accuracy: 0.6043 - val_loss: 1.0636\nEpoch 68/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6078 - loss: 1.0748 - val_accuracy: 0.6141 - val_loss: 1.0594\nEpoch 69/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5926 - loss: 1.0910 - val_accuracy: 0.6046 - val_loss: 1.0762\nEpoch 70/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5914 - loss: 1.1019 - val_accuracy: 0.6118 - val_loss: 1.0615\nEpoch 71/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5977 - loss: 1.0815 - val_accuracy: 0.6107 - val_loss: 1.0607\nEpoch 71: early stopping\nRestoring model weights from the end of the best epoch: 56.\n\nTemps d'entraînement: 65.44 secondes\n\nÉvaluation du modèle sur l'ensemble de test...\nPrécision (accuracy): 0.6113\nTemps de prédiction: 1.00 secondes\n\nRapport de classification:\n                              precision    recall  f1-score   support\n\n                  Background       0.71      0.88      0.78      3233\n                     Alfalfa       0.00      0.00      0.00        14\n                 Corn-notill       0.50      0.32      0.39       428\n                Corn-mintill       0.29      0.06      0.11       249\n                        Corn       0.49      0.25      0.33        71\n               Grass-pasture       0.61      0.19      0.29       145\n                 Grass-trees       0.34      0.07      0.11       219\n         Grass-pasture-mowed       0.00      0.00      0.00         8\n               Hay-windrowed       0.69      0.73      0.71       143\n                        Oats       0.00      0.00      0.00         6\n              Soybean-notill       0.21      0.07      0.10       292\n             Soybean-mintill       0.42      0.78      0.55       737\n               Soybean-clean       0.44      0.12      0.19       178\n                       Wheat       0.61      0.44      0.51        61\n                       Woods       0.44      0.09      0.16       380\nBuildings-Grass-Trees-Drives       0.00      0.00      0.00       116\n          Stone-Steel-Towers       0.67      0.36      0.47        28\n\n                    accuracy                           0.61      6308\n                   macro avg       0.38      0.26      0.28      6308\n                weighted avg       0.56      0.61      0.55      6308\n\nImpossible de créer la visualisation du F1-score par classe - données insuffisantes\n\nModèle sauvegardé sous 'resultats_equal5/model_equal5.h5'\n\n========== MODÈLE EQUAL 10 BANDES ==========\nNombre de classes (avec background): 17\nDimensions des données: (21025, 10)\nEnsemble d'entraînement: (14717, 10), (14717, 17)\nEnsemble de test: (6308, 10), (6308, 17)\n\nCréation et entraînement du modèle equal10...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_5\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_5\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ input_layer_5 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_25 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │           \u001b[38;5;34m5,632\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_20               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │           \u001b[38;5;34m2,048\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_20 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_26 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │          \u001b[38;5;34m32,832\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_21               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │             \u001b[38;5;34m256\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_21 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_27 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │           \u001b[38;5;34m4,160\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_22               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │             \u001b[38;5;34m256\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_22 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_28 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │           \u001b[38;5;34m2,080\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_23               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │             \u001b[38;5;34m128\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_23 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_29 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m)                  │             \u001b[38;5;34m561\u001b[0m │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ input_layer_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">5,632</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_20               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">32,832</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_21               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_22               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_23               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">561</span> │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m47,953\u001b[0m (187.32 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">47,953</span> (187.32 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m46,609\u001b[0m (182.07 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">46,609</span> (182.07 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,344\u001b[0m (5.25 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,344</span> (5.25 KB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.4636 - loss: 1.8509 - val_accuracy: 0.5999 - val_loss: 1.2128\nEpoch 2/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5841 - loss: 1.2310 - val_accuracy: 0.6090 - val_loss: 1.1113\nEpoch 3/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5903 - loss: 1.1582 - val_accuracy: 0.6097 - val_loss: 1.0685\nEpoch 4/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5891 - loss: 1.1326 - val_accuracy: 0.6179 - val_loss: 1.0258\nEpoch 5/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6081 - loss: 1.0840 - val_accuracy: 0.6264 - val_loss: 0.9826\nEpoch 6/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6142 - loss: 1.0498 - val_accuracy: 0.6393 - val_loss: 1.0103\nEpoch 7/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6166 - loss: 1.0416 - val_accuracy: 0.6552 - val_loss: 0.9372\nEpoch 8/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6215 - loss: 1.0236 - val_accuracy: 0.6610 - val_loss: 0.9213\nEpoch 9/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6259 - loss: 1.0098 - val_accuracy: 0.6542 - val_loss: 0.9125\nEpoch 10/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6322 - loss: 0.9864 - val_accuracy: 0.6702 - val_loss: 0.9072\nEpoch 11/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6382 - loss: 0.9794 - val_accuracy: 0.6556 - val_loss: 0.9114\nEpoch 12/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6443 - loss: 0.9562 - val_accuracy: 0.6641 - val_loss: 0.9049\nEpoch 13/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6371 - loss: 0.9633 - val_accuracy: 0.6773 - val_loss: 0.8826\nEpoch 14/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6305 - loss: 0.9666 - val_accuracy: 0.6790 - val_loss: 0.8590\nEpoch 15/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6398 - loss: 0.9402 - val_accuracy: 0.6698 - val_loss: 0.8813\nEpoch 16/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6577 - loss: 0.9375 - val_accuracy: 0.6695 - val_loss: 0.8730\nEpoch 17/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6598 - loss: 0.9061 - val_accuracy: 0.6773 - val_loss: 0.8461\nEpoch 18/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6436 - loss: 0.9191 - val_accuracy: 0.6712 - val_loss: 0.8728\nEpoch 19/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6523 - loss: 0.9235 - val_accuracy: 0.6919 - val_loss: 0.8432\nEpoch 20/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6638 - loss: 0.9125 - val_accuracy: 0.6743 - val_loss: 0.8646\nEpoch 21/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6544 - loss: 0.8942 - val_accuracy: 0.6783 - val_loss: 0.8481\nEpoch 22/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6539 - loss: 0.9020 - val_accuracy: 0.6797 - val_loss: 0.8568\nEpoch 23/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6605 - loss: 0.8932 - val_accuracy: 0.6797 - val_loss: 0.8485\nEpoch 24/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6625 - loss: 0.8828 - val_accuracy: 0.6817 - val_loss: 0.8489\nEpoch 25/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6509 - loss: 0.9052 - val_accuracy: 0.6872 - val_loss: 0.8340\nEpoch 26/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6546 - loss: 0.9058 - val_accuracy: 0.6838 - val_loss: 0.8467\nEpoch 27/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6564 - loss: 0.8926 - val_accuracy: 0.6851 - val_loss: 0.8275\nEpoch 28/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6692 - loss: 0.8799 - val_accuracy: 0.6821 - val_loss: 0.8462\nEpoch 29/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6699 - loss: 0.8721 - val_accuracy: 0.6923 - val_loss: 0.8310\nEpoch 30/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6621 - loss: 0.8982 - val_accuracy: 0.6875 - val_loss: 0.8243\nEpoch 31/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6577 - loss: 0.8864 - val_accuracy: 0.6868 - val_loss: 0.8329\nEpoch 32/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6610 - loss: 0.8855 - val_accuracy: 0.6797 - val_loss: 0.8388\nEpoch 33/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6598 - loss: 0.8823 - val_accuracy: 0.6865 - val_loss: 0.8370\nEpoch 34/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6684 - loss: 0.8795 - val_accuracy: 0.6841 - val_loss: 0.8347\nEpoch 35/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6695 - loss: 0.8719 - val_accuracy: 0.6899 - val_loss: 0.8153\nEpoch 36/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6696 - loss: 0.8697 - val_accuracy: 0.6895 - val_loss: 0.8273\nEpoch 37/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6671 - loss: 0.8685 - val_accuracy: 0.6980 - val_loss: 0.8149\nEpoch 38/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6720 - loss: 0.8709 - val_accuracy: 0.6861 - val_loss: 0.8206\nEpoch 39/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6661 - loss: 0.8694 - val_accuracy: 0.6814 - val_loss: 0.8330\nEpoch 40/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6825 - loss: 0.8573 - val_accuracy: 0.6902 - val_loss: 0.8378\nEpoch 41/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6759 - loss: 0.8533 - val_accuracy: 0.6902 - val_loss: 0.8114\nEpoch 42/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6734 - loss: 0.8471 - val_accuracy: 0.6984 - val_loss: 0.7995\nEpoch 43/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6751 - loss: 0.8512 - val_accuracy: 0.6875 - val_loss: 0.8124\nEpoch 44/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6773 - loss: 0.8640 - val_accuracy: 0.7007 - val_loss: 0.7988\nEpoch 45/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6801 - loss: 0.8511 - val_accuracy: 0.6919 - val_loss: 0.8005\nEpoch 46/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6723 - loss: 0.8468 - val_accuracy: 0.6923 - val_loss: 0.8099\nEpoch 47/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6729 - loss: 0.8567 - val_accuracy: 0.6902 - val_loss: 0.8231\nEpoch 48/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6822 - loss: 0.8367 - val_accuracy: 0.6909 - val_loss: 0.8023\nEpoch 49/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6747 - loss: 0.8516 - val_accuracy: 0.6987 - val_loss: 0.8045\nEpoch 50/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6777 - loss: 0.8471 - val_accuracy: 0.6977 - val_loss: 0.8076\nEpoch 51/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6882 - loss: 0.8394 - val_accuracy: 0.7055 - val_loss: 0.7897\nEpoch 52/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6846 - loss: 0.8359 - val_accuracy: 0.6943 - val_loss: 0.7878\nEpoch 53/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6781 - loss: 0.8332 - val_accuracy: 0.6957 - val_loss: 0.8143\nEpoch 54/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6803 - loss: 0.8439 - val_accuracy: 0.7001 - val_loss: 0.7925\nEpoch 55/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6871 - loss: 0.8327 - val_accuracy: 0.7001 - val_loss: 0.7964\nEpoch 56/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6817 - loss: 0.8219 - val_accuracy: 0.6987 - val_loss: 0.7934\nEpoch 57/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6741 - loss: 0.8507 - val_accuracy: 0.6960 - val_loss: 0.8188\nEpoch 58/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6834 - loss: 0.8557 - val_accuracy: 0.6929 - val_loss: 0.7991\nEpoch 59/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6844 - loss: 0.8366 - val_accuracy: 0.6980 - val_loss: 0.7942\nEpoch 60/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6725 - loss: 0.8481 - val_accuracy: 0.6967 - val_loss: 0.7940\nEpoch 61/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6875 - loss: 0.8159 - val_accuracy: 0.6933 - val_loss: 0.7960\nEpoch 62/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6802 - loss: 0.8224 - val_accuracy: 0.7041 - val_loss: 0.7987\nEpoch 63/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6736 - loss: 0.8371 - val_accuracy: 0.6980 - val_loss: 0.8041\nEpoch 64/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6871 - loss: 0.8273 - val_accuracy: 0.7004 - val_loss: 0.8077\nEpoch 65/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6731 - loss: 0.8271 - val_accuracy: 0.7052 - val_loss: 0.7963\nEpoch 66/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6800 - loss: 0.8264 - val_accuracy: 0.7018 - val_loss: 0.7746\nEpoch 67/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6817 - loss: 0.8344 - val_accuracy: 0.7038 - val_loss: 0.7916\nEpoch 68/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6835 - loss: 0.8211 - val_accuracy: 0.6929 - val_loss: 0.8016\nEpoch 69/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6855 - loss: 0.8202 - val_accuracy: 0.7055 - val_loss: 0.7837\nEpoch 70/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6799 - loss: 0.8345 - val_accuracy: 0.7028 - val_loss: 0.7956\nEpoch 71/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6881 - loss: 0.8180 - val_accuracy: 0.7069 - val_loss: 0.7830\nEpoch 72/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6825 - loss: 0.8263 - val_accuracy: 0.7001 - val_loss: 0.7873\nEpoch 73/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6893 - loss: 0.8044 - val_accuracy: 0.7069 - val_loss: 0.7695\nEpoch 74/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6822 - loss: 0.8255 - val_accuracy: 0.7038 - val_loss: 0.7820\nEpoch 75/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6803 - loss: 0.8191 - val_accuracy: 0.6861 - val_loss: 0.8106\nEpoch 76/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6736 - loss: 0.8281 - val_accuracy: 0.7072 - val_loss: 0.7812\nEpoch 77/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6897 - loss: 0.8124 - val_accuracy: 0.7021 - val_loss: 0.7775\nEpoch 78/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6764 - loss: 0.8267 - val_accuracy: 0.7062 - val_loss: 0.7741\nEpoch 79/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6866 - loss: 0.8161 - val_accuracy: 0.7024 - val_loss: 0.7814\nEpoch 80/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6870 - loss: 0.8146 - val_accuracy: 0.7045 - val_loss: 0.7694\nEpoch 81/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6758 - loss: 0.8387 - val_accuracy: 0.7018 - val_loss: 0.7818\nEpoch 82/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6912 - loss: 0.8052 - val_accuracy: 0.7001 - val_loss: 0.7905\nEpoch 83/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6868 - loss: 0.8247 - val_accuracy: 0.7181 - val_loss: 0.7701\nEpoch 84/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6804 - loss: 0.8238 - val_accuracy: 0.7028 - val_loss: 0.7878\nEpoch 85/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6878 - loss: 0.8229 - val_accuracy: 0.7072 - val_loss: 0.7775\nEpoch 86/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6985 - loss: 0.8089 - val_accuracy: 0.7038 - val_loss: 0.7871\nEpoch 87/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6861 - loss: 0.8194 - val_accuracy: 0.7086 - val_loss: 0.7688\nEpoch 88/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6829 - loss: 0.8213 - val_accuracy: 0.7092 - val_loss: 0.7733\nEpoch 89/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6944 - loss: 0.7937 - val_accuracy: 0.7096 - val_loss: 0.7740\nEpoch 90/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6848 - loss: 0.8151 - val_accuracy: 0.6990 - val_loss: 0.7825\nEpoch 91/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6826 - loss: 0.8261 - val_accuracy: 0.7048 - val_loss: 0.7742\nEpoch 92/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6870 - loss: 0.8210 - val_accuracy: 0.6974 - val_loss: 0.7835\nEpoch 93/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6869 - loss: 0.8081 - val_accuracy: 0.7069 - val_loss: 0.7779\nEpoch 94/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6938 - loss: 0.7944 - val_accuracy: 0.7072 - val_loss: 0.7704\nEpoch 95/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6938 - loss: 0.7948 - val_accuracy: 0.7014 - val_loss: 0.7794\nEpoch 96/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6917 - loss: 0.7953 - val_accuracy: 0.7072 - val_loss: 0.7831\nEpoch 97/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6834 - loss: 0.8159 - val_accuracy: 0.7075 - val_loss: 0.7694\nEpoch 98/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6889 - loss: 0.8112 - val_accuracy: 0.7099 - val_loss: 0.7676\nEpoch 99/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6856 - loss: 0.8279 - val_accuracy: 0.7024 - val_loss: 0.7912\nEpoch 100/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6901 - loss: 0.7931 - val_accuracy: 0.6997 - val_loss: 0.7759\nRestoring model weights from the end of the best epoch: 98.\n\nTemps d'entraînement: 87.86 secondes\n\nÉvaluation du modèle sur l'ensemble de test...\nPrécision (accuracy): 0.7213\nTemps de prédiction: 1.06 secondes\n\nRapport de classification:\n                              precision    recall  f1-score   support\n\n                  Background       0.78      0.85      0.82      3233\n                     Alfalfa       0.33      0.07      0.12        14\n                 Corn-notill       0.66      0.62      0.64       428\n                Corn-mintill       0.68      0.53      0.60       249\n                        Corn       0.62      0.39      0.48        71\n               Grass-pasture       0.83      0.50      0.62       145\n                 Grass-trees       0.72      0.62      0.67       219\n         Grass-pasture-mowed       0.00      0.00      0.00         8\n               Hay-windrowed       0.71      0.99      0.82       143\n                        Oats       0.00      0.00      0.00         6\n              Soybean-notill       0.55      0.44      0.49       292\n             Soybean-mintill       0.59      0.78      0.68       737\n               Soybean-clean       0.67      0.75      0.71       178\n                       Wheat       0.75      0.90      0.82        61\n                       Woods       0.63      0.27      0.38       380\nBuildings-Grass-Trees-Drives       0.00      0.00      0.00       116\n          Stone-Steel-Towers       0.74      0.61      0.67        28\n\n                    accuracy                           0.72      6308\n                   macro avg       0.55      0.49      0.50      6308\n                weighted avg       0.70      0.72      0.70      6308\n\nImpossible de créer la visualisation du F1-score par classe - données insuffisantes\n\nModèle sauvegardé sous 'resultats_equal10/model_equal10.h5'\n\n========== MODÈLE EQUAL 15 BANDES ==========\nNombre de classes (avec background): 17\nDimensions des données: (21025, 15)\nEnsemble d'entraînement: (14717, 15), (14717, 17)\nEnsemble de test: (6308, 15), (6308, 17)\n\nCréation et entraînement du modèle equal15...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_6\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_6\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ input_layer_6 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_30 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │           \u001b[38;5;34m8,192\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_24               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │           \u001b[38;5;34m2,048\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_24 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_31 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │          \u001b[38;5;34m32,832\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_25               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │             \u001b[38;5;34m256\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_25 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_32 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │           \u001b[38;5;34m4,160\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_26               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │             \u001b[38;5;34m256\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_26 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_33 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │           \u001b[38;5;34m2,080\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_27               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │             \u001b[38;5;34m128\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_27 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_34 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m)                  │             \u001b[38;5;34m561\u001b[0m │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ input_layer_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,192</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_24               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">32,832</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_25               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_26               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_27               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_34 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">561</span> │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m50,513\u001b[0m (197.32 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">50,513</span> (197.32 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m49,169\u001b[0m (192.07 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">49,169</span> (192.07 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,344\u001b[0m (5.25 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,344</span> (5.25 KB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.4428 - loss: 1.9598 - val_accuracy: 0.6118 - val_loss: 1.1799\nEpoch 2/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5927 - loss: 1.2109 - val_accuracy: 0.6257 - val_loss: 1.0589\nEpoch 3/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6063 - loss: 1.1104 - val_accuracy: 0.6427 - val_loss: 0.9880\nEpoch 4/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6095 - loss: 1.0690 - val_accuracy: 0.6495 - val_loss: 0.9601\nEpoch 5/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6228 - loss: 1.0197 - val_accuracy: 0.6651 - val_loss: 0.9168\nEpoch 6/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6314 - loss: 0.9950 - val_accuracy: 0.6399 - val_loss: 0.9651\nEpoch 7/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6371 - loss: 0.9822 - val_accuracy: 0.6855 - val_loss: 0.8603\nEpoch 8/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6538 - loss: 0.9326 - val_accuracy: 0.6776 - val_loss: 0.8364\nEpoch 9/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6682 - loss: 0.9049 - val_accuracy: 0.7048 - val_loss: 0.8190\nEpoch 10/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6577 - loss: 0.9206 - val_accuracy: 0.6895 - val_loss: 0.8521\nEpoch 11/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6604 - loss: 0.9021 - val_accuracy: 0.7024 - val_loss: 0.7944\nEpoch 12/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6695 - loss: 0.8755 - val_accuracy: 0.6984 - val_loss: 0.7957\nEpoch 13/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6827 - loss: 0.8691 - val_accuracy: 0.7150 - val_loss: 0.7673\nEpoch 14/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6759 - loss: 0.8762 - val_accuracy: 0.7052 - val_loss: 0.7547\nEpoch 15/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6778 - loss: 0.8606 - val_accuracy: 0.7255 - val_loss: 0.7501\nEpoch 16/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6825 - loss: 0.8381 - val_accuracy: 0.7143 - val_loss: 0.7505\nEpoch 17/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6901 - loss: 0.8310 - val_accuracy: 0.7140 - val_loss: 0.7600\nEpoch 18/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6838 - loss: 0.8229 - val_accuracy: 0.7147 - val_loss: 0.7476\nEpoch 19/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6876 - loss: 0.8199 - val_accuracy: 0.6936 - val_loss: 0.7836\nEpoch 20/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6862 - loss: 0.8285 - val_accuracy: 0.7204 - val_loss: 0.7386\nEpoch 21/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6892 - loss: 0.8155 - val_accuracy: 0.7191 - val_loss: 0.7402\nEpoch 22/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6853 - loss: 0.8191 - val_accuracy: 0.7266 - val_loss: 0.7252\nEpoch 23/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6878 - loss: 0.8125 - val_accuracy: 0.7221 - val_loss: 0.7473\nEpoch 24/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6880 - loss: 0.8111 - val_accuracy: 0.7327 - val_loss: 0.7221\nEpoch 25/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6922 - loss: 0.8048 - val_accuracy: 0.7184 - val_loss: 0.7254\nEpoch 26/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6959 - loss: 0.8040 - val_accuracy: 0.7204 - val_loss: 0.7308\nEpoch 27/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6957 - loss: 0.7902 - val_accuracy: 0.7354 - val_loss: 0.7045\nEpoch 28/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6984 - loss: 0.7962 - val_accuracy: 0.7228 - val_loss: 0.7186\nEpoch 29/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6946 - loss: 0.7949 - val_accuracy: 0.7211 - val_loss: 0.7317\nEpoch 30/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7064 - loss: 0.7654 - val_accuracy: 0.7269 - val_loss: 0.7327\nEpoch 31/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6955 - loss: 0.7901 - val_accuracy: 0.7293 - val_loss: 0.7279\nEpoch 32/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7008 - loss: 0.7771 - val_accuracy: 0.7371 - val_loss: 0.6874\nEpoch 33/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6981 - loss: 0.7775 - val_accuracy: 0.7272 - val_loss: 0.7144\nEpoch 34/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7062 - loss: 0.7761 - val_accuracy: 0.7249 - val_loss: 0.7256\nEpoch 35/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6973 - loss: 0.7745 - val_accuracy: 0.7306 - val_loss: 0.7104\nEpoch 36/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7151 - loss: 0.7521 - val_accuracy: 0.7330 - val_loss: 0.6933\nEpoch 37/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7052 - loss: 0.7677 - val_accuracy: 0.7289 - val_loss: 0.7183\nEpoch 38/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7015 - loss: 0.7876 - val_accuracy: 0.7215 - val_loss: 0.7231\nEpoch 39/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6970 - loss: 0.7758 - val_accuracy: 0.7374 - val_loss: 0.6994\nEpoch 40/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7097 - loss: 0.7559 - val_accuracy: 0.7385 - val_loss: 0.6897\nEpoch 41/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7078 - loss: 0.7569 - val_accuracy: 0.7300 - val_loss: 0.7034\nEpoch 42/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7110 - loss: 0.7479 - val_accuracy: 0.7238 - val_loss: 0.7168\nEpoch 43/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7156 - loss: 0.7506 - val_accuracy: 0.7306 - val_loss: 0.7161\nEpoch 44/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7105 - loss: 0.7601 - val_accuracy: 0.7432 - val_loss: 0.7040\nEpoch 45/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7081 - loss: 0.7591 - val_accuracy: 0.7452 - val_loss: 0.6664\nEpoch 46/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7212 - loss: 0.7407 - val_accuracy: 0.7439 - val_loss: 0.6807\nEpoch 47/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6989 - loss: 0.7669 - val_accuracy: 0.7344 - val_loss: 0.6963\nEpoch 48/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7161 - loss: 0.7537 - val_accuracy: 0.7432 - val_loss: 0.7031\nEpoch 49/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7189 - loss: 0.7443 - val_accuracy: 0.7442 - val_loss: 0.6852\nEpoch 50/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7092 - loss: 0.7536 - val_accuracy: 0.7469 - val_loss: 0.6741\nEpoch 51/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7224 - loss: 0.7284 - val_accuracy: 0.7415 - val_loss: 0.6709\nEpoch 52/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7208 - loss: 0.7326 - val_accuracy: 0.7147 - val_loss: 0.7629\nEpoch 53/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7213 - loss: 0.7355 - val_accuracy: 0.7514 - val_loss: 0.6731\nEpoch 54/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7128 - loss: 0.7470 - val_accuracy: 0.7371 - val_loss: 0.7077\nEpoch 55/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7137 - loss: 0.7412 - val_accuracy: 0.7565 - val_loss: 0.6617\nEpoch 56/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7253 - loss: 0.7339 - val_accuracy: 0.7418 - val_loss: 0.7072\nEpoch 57/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7177 - loss: 0.7397 - val_accuracy: 0.7249 - val_loss: 0.7148\nEpoch 58/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7162 - loss: 0.7343 - val_accuracy: 0.7385 - val_loss: 0.6773\nEpoch 59/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7324 - loss: 0.7189 - val_accuracy: 0.7432 - val_loss: 0.6732\nEpoch 60/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7154 - loss: 0.7364 - val_accuracy: 0.7395 - val_loss: 0.6886\nEpoch 61/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7199 - loss: 0.7411 - val_accuracy: 0.7378 - val_loss: 0.6909\nEpoch 62/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7186 - loss: 0.7340 - val_accuracy: 0.7408 - val_loss: 0.6841\nEpoch 63/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7218 - loss: 0.7325 - val_accuracy: 0.7435 - val_loss: 0.6802\nEpoch 64/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7167 - loss: 0.7302 - val_accuracy: 0.7385 - val_loss: 0.6699\nEpoch 65/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7179 - loss: 0.7298 - val_accuracy: 0.7442 - val_loss: 0.6704\nEpoch 66/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7257 - loss: 0.7276 - val_accuracy: 0.7432 - val_loss: 0.6717\nEpoch 67/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7144 - loss: 0.7395 - val_accuracy: 0.7473 - val_loss: 0.6685\nEpoch 68/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7250 - loss: 0.7218 - val_accuracy: 0.7425 - val_loss: 0.6829\nEpoch 69/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7249 - loss: 0.7246 - val_accuracy: 0.7452 - val_loss: 0.6763\nEpoch 70/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7220 - loss: 0.7314 - val_accuracy: 0.7558 - val_loss: 0.6475\nEpoch 71/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7193 - loss: 0.7209 - val_accuracy: 0.7439 - val_loss: 0.6655\nEpoch 72/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7313 - loss: 0.7013 - val_accuracy: 0.7446 - val_loss: 0.6805\nEpoch 73/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7288 - loss: 0.6911 - val_accuracy: 0.7636 - val_loss: 0.6515\nEpoch 74/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7297 - loss: 0.7088 - val_accuracy: 0.7452 - val_loss: 0.6836\nEpoch 75/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7280 - loss: 0.7022 - val_accuracy: 0.7422 - val_loss: 0.6793\nEpoch 76/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7070 - loss: 0.7392 - val_accuracy: 0.7531 - val_loss: 0.6577\nEpoch 77/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7273 - loss: 0.7131 - val_accuracy: 0.7466 - val_loss: 0.6716\nEpoch 78/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7291 - loss: 0.7086 - val_accuracy: 0.7554 - val_loss: 0.6667\nEpoch 79/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7237 - loss: 0.7117 - val_accuracy: 0.7520 - val_loss: 0.6714\nEpoch 80/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7279 - loss: 0.7097 - val_accuracy: 0.7585 - val_loss: 0.6476\nEpoch 81/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7356 - loss: 0.6948 - val_accuracy: 0.7435 - val_loss: 0.6736\nEpoch 82/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7266 - loss: 0.7091 - val_accuracy: 0.7548 - val_loss: 0.6662\nEpoch 83/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7266 - loss: 0.7127 - val_accuracy: 0.7497 - val_loss: 0.6720\nEpoch 84/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7322 - loss: 0.7000 - val_accuracy: 0.7483 - val_loss: 0.6593\nEpoch 85/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7219 - loss: 0.7183 - val_accuracy: 0.7497 - val_loss: 0.6669\nEpoch 85: early stopping\nRestoring model weights from the end of the best epoch: 70.\n\nTemps d'entraînement: 75.70 secondes\n\nÉvaluation du modèle sur l'ensemble de test...\nPrécision (accuracy): 0.7643\nTemps de prédiction: 1.01 secondes\n\nRapport de classification:\n                              precision    recall  f1-score   support\n\n                  Background       0.80      0.87      0.83      3233\n                     Alfalfa       0.50      0.14      0.22        14\n                 Corn-notill       0.69      0.70      0.69       428\n                Corn-mintill       0.74      0.67      0.70       249\n                        Corn       0.71      0.59      0.65        71\n               Grass-pasture       0.85      0.64      0.73       145\n                 Grass-trees       0.79      0.73      0.76       219\n         Grass-pasture-mowed       0.50      0.88      0.64         8\n               Hay-windrowed       0.80      0.97      0.88       143\n                        Oats       0.00      0.00      0.00         6\n              Soybean-notill       0.68      0.82      0.74       292\n             Soybean-mintill       0.70      0.78      0.74       737\n               Soybean-clean       0.76      0.72      0.74       178\n                       Wheat       0.79      0.92      0.85        61\n                       Woods       0.62      0.21      0.31       380\nBuildings-Grass-Trees-Drives       0.00      0.00      0.00       116\n          Stone-Steel-Towers       0.76      0.89      0.82        28\n\n                    accuracy                           0.76      6308\n                   macro avg       0.63      0.62      0.61      6308\n                weighted avg       0.74      0.76      0.75      6308\n\nImpossible de créer la visualisation du F1-score par classe - données insuffisantes\n\nModèle sauvegardé sous 'resultats_equal15/model_equal15.h5'\n\n========== MODÈLE EQUAL 20 BANDES ==========\nNombre de classes (avec background): 17\nDimensions des données: (21025, 20)\nEnsemble d'entraînement: (14717, 20), (14717, 17)\nEnsemble de test: (6308, 20), (6308, 17)\n\nCréation et entraînement du modèle equal20...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_7\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_7\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ input_layer_7 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_35 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │          \u001b[38;5;34m10,752\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_28               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │           \u001b[38;5;34m2,048\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_28 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_36 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │          \u001b[38;5;34m32,832\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_29               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │             \u001b[38;5;34m256\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_29 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_37 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │           \u001b[38;5;34m4,160\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_30               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │             \u001b[38;5;34m256\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_30 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_38 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │           \u001b[38;5;34m2,080\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_31               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │             \u001b[38;5;34m128\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_31 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_39 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m)                  │             \u001b[38;5;34m561\u001b[0m │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ input_layer_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_35 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">10,752</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_28               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_36 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">32,832</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_29               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_37 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_30               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_38 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_31               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_39 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">561</span> │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m53,073\u001b[0m (207.32 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">53,073</span> (207.32 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m51,729\u001b[0m (202.07 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">51,729</span> (202.07 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,344\u001b[0m (5.25 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,344</span> (5.25 KB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - accuracy: 0.4776 - loss: 1.7990 - val_accuracy: 0.6145 - val_loss: 1.1321\nEpoch 2/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5957 - loss: 1.1574 - val_accuracy: 0.6345 - val_loss: 1.0092\nEpoch 3/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6180 - loss: 1.0797 - val_accuracy: 0.6624 - val_loss: 0.9539\nEpoch 4/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6314 - loss: 1.0119 - val_accuracy: 0.6671 - val_loss: 0.8862\nEpoch 5/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6494 - loss: 0.9557 - val_accuracy: 0.6749 - val_loss: 0.8774\nEpoch 6/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6567 - loss: 0.9465 - val_accuracy: 0.6821 - val_loss: 0.8280\nEpoch 7/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6646 - loss: 0.9026 - val_accuracy: 0.6712 - val_loss: 0.8479\nEpoch 8/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6751 - loss: 0.8727 - val_accuracy: 0.6848 - val_loss: 0.8275\nEpoch 9/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6737 - loss: 0.8627 - val_accuracy: 0.7082 - val_loss: 0.7822\nEpoch 10/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6822 - loss: 0.8550 - val_accuracy: 0.7174 - val_loss: 0.7574\nEpoch 11/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6792 - loss: 0.8389 - val_accuracy: 0.7099 - val_loss: 0.7557\nEpoch 12/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6869 - loss: 0.8273 - val_accuracy: 0.7201 - val_loss: 0.7477\nEpoch 13/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6921 - loss: 0.8265 - val_accuracy: 0.7045 - val_loss: 0.7547\nEpoch 14/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6927 - loss: 0.8235 - val_accuracy: 0.7327 - val_loss: 0.7231\nEpoch 15/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6900 - loss: 0.7969 - val_accuracy: 0.7286 - val_loss: 0.7299\nEpoch 16/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6965 - loss: 0.8029 - val_accuracy: 0.7283 - val_loss: 0.7227\nEpoch 17/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6928 - loss: 0.8036 - val_accuracy: 0.7255 - val_loss: 0.7141\nEpoch 18/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7059 - loss: 0.7842 - val_accuracy: 0.7300 - val_loss: 0.7156\nEpoch 19/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7083 - loss: 0.7728 - val_accuracy: 0.7286 - val_loss: 0.7165\nEpoch 20/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7049 - loss: 0.7739 - val_accuracy: 0.7310 - val_loss: 0.6970\nEpoch 21/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6973 - loss: 0.7910 - val_accuracy: 0.7381 - val_loss: 0.6983\nEpoch 22/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7059 - loss: 0.7553 - val_accuracy: 0.7259 - val_loss: 0.7302\nEpoch 23/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7081 - loss: 0.7742 - val_accuracy: 0.7435 - val_loss: 0.6910\nEpoch 24/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7024 - loss: 0.7792 - val_accuracy: 0.7452 - val_loss: 0.6693\nEpoch 25/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7142 - loss: 0.7456 - val_accuracy: 0.7476 - val_loss: 0.6787\nEpoch 26/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7087 - loss: 0.7585 - val_accuracy: 0.7364 - val_loss: 0.6774\nEpoch 27/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7159 - loss: 0.7366 - val_accuracy: 0.7334 - val_loss: 0.7014\nEpoch 28/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7197 - loss: 0.7332 - val_accuracy: 0.7439 - val_loss: 0.6727\nEpoch 29/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7135 - loss: 0.7451 - val_accuracy: 0.7459 - val_loss: 0.6839\nEpoch 30/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7125 - loss: 0.7616 - val_accuracy: 0.7242 - val_loss: 0.7119\nEpoch 31/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7198 - loss: 0.7257 - val_accuracy: 0.7286 - val_loss: 0.6956\nEpoch 32/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7188 - loss: 0.7377 - val_accuracy: 0.7429 - val_loss: 0.6760\nEpoch 33/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7265 - loss: 0.7280 - val_accuracy: 0.7452 - val_loss: 0.6708\nEpoch 34/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7182 - loss: 0.7439 - val_accuracy: 0.7388 - val_loss: 0.6632\nEpoch 35/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7290 - loss: 0.7164 - val_accuracy: 0.7466 - val_loss: 0.6637\nEpoch 36/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7168 - loss: 0.7439 - val_accuracy: 0.7520 - val_loss: 0.6541\nEpoch 37/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7304 - loss: 0.7138 - val_accuracy: 0.7459 - val_loss: 0.6551\nEpoch 38/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7183 - loss: 0.7337 - val_accuracy: 0.7405 - val_loss: 0.6706\nEpoch 39/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7303 - loss: 0.7172 - val_accuracy: 0.7517 - val_loss: 0.6450\nEpoch 40/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7214 - loss: 0.7193 - val_accuracy: 0.7473 - val_loss: 0.6617\nEpoch 41/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7260 - loss: 0.7085 - val_accuracy: 0.7473 - val_loss: 0.6476\nEpoch 42/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7226 - loss: 0.7220 - val_accuracy: 0.7401 - val_loss: 0.6626\nEpoch 43/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7299 - loss: 0.7111 - val_accuracy: 0.7588 - val_loss: 0.6358\nEpoch 44/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7244 - loss: 0.7103 - val_accuracy: 0.7435 - val_loss: 0.6442\nEpoch 45/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7247 - loss: 0.7213 - val_accuracy: 0.7520 - val_loss: 0.6566\nEpoch 46/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7239 - loss: 0.7108 - val_accuracy: 0.7551 - val_loss: 0.6295\nEpoch 47/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7278 - loss: 0.7116 - val_accuracy: 0.7592 - val_loss: 0.6349\nEpoch 48/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7309 - loss: 0.7015 - val_accuracy: 0.7456 - val_loss: 0.6496\nEpoch 49/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7277 - loss: 0.7099 - val_accuracy: 0.7503 - val_loss: 0.6447\nEpoch 50/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7362 - loss: 0.6872 - val_accuracy: 0.7469 - val_loss: 0.6513\nEpoch 51/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7315 - loss: 0.6947 - val_accuracy: 0.7398 - val_loss: 0.6669\nEpoch 52/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7346 - loss: 0.6997 - val_accuracy: 0.7595 - val_loss: 0.6315\nEpoch 53/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7298 - loss: 0.6988 - val_accuracy: 0.7395 - val_loss: 0.6719\nEpoch 54/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7388 - loss: 0.6755 - val_accuracy: 0.7520 - val_loss: 0.6459\nEpoch 55/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7377 - loss: 0.6928 - val_accuracy: 0.7588 - val_loss: 0.6379\nEpoch 56/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7348 - loss: 0.6965 - val_accuracy: 0.7612 - val_loss: 0.6332\nEpoch 57/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7389 - loss: 0.6785 - val_accuracy: 0.7510 - val_loss: 0.6333\nEpoch 58/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7399 - loss: 0.6703 - val_accuracy: 0.7361 - val_loss: 0.6832\nEpoch 59/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7335 - loss: 0.6815 - val_accuracy: 0.7544 - val_loss: 0.6343\nEpoch 60/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7370 - loss: 0.6773 - val_accuracy: 0.7643 - val_loss: 0.6180\nEpoch 61/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7415 - loss: 0.6787 - val_accuracy: 0.7493 - val_loss: 0.6557\nEpoch 62/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7465 - loss: 0.6699 - val_accuracy: 0.7656 - val_loss: 0.6093\nEpoch 63/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7450 - loss: 0.6669 - val_accuracy: 0.7537 - val_loss: 0.6513\nEpoch 64/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7448 - loss: 0.6683 - val_accuracy: 0.7531 - val_loss: 0.6245\nEpoch 65/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7323 - loss: 0.6721 - val_accuracy: 0.7385 - val_loss: 0.6855\nEpoch 66/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7410 - loss: 0.6677 - val_accuracy: 0.7582 - val_loss: 0.6363\nEpoch 67/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7389 - loss: 0.6671 - val_accuracy: 0.7578 - val_loss: 0.6202\nEpoch 68/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7323 - loss: 0.6793 - val_accuracy: 0.7571 - val_loss: 0.6254\nEpoch 69/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7417 - loss: 0.6699 - val_accuracy: 0.7711 - val_loss: 0.6155\nEpoch 70/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7483 - loss: 0.6491 - val_accuracy: 0.7639 - val_loss: 0.6283\nEpoch 71/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7486 - loss: 0.6624 - val_accuracy: 0.7514 - val_loss: 0.6411\nEpoch 72/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7501 - loss: 0.6573 - val_accuracy: 0.7643 - val_loss: 0.6211\nEpoch 73/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7468 - loss: 0.6655 - val_accuracy: 0.7568 - val_loss: 0.6161\nEpoch 74/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7480 - loss: 0.6635 - val_accuracy: 0.7649 - val_loss: 0.6187\nEpoch 75/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7500 - loss: 0.6593 - val_accuracy: 0.7636 - val_loss: 0.6152\nEpoch 76/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7437 - loss: 0.6609 - val_accuracy: 0.7694 - val_loss: 0.6109\nEpoch 77/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7339 - loss: 0.6839 - val_accuracy: 0.7582 - val_loss: 0.6310\nEpoch 77: early stopping\nRestoring model weights from the end of the best epoch: 62.\n\nTemps d'entraînement: 69.28 secondes\n\nÉvaluation du modèle sur l'ensemble de test...\nPrécision (accuracy): 0.7738\nTemps de prédiction: 1.00 secondes\n\nRapport de classification:\n                              precision    recall  f1-score   support\n\n                  Background       0.81      0.85      0.83      3233\n                     Alfalfa       0.89      0.57      0.70        14\n                 Corn-notill       0.69      0.79      0.74       428\n                Corn-mintill       0.74      0.73      0.74       249\n                        Corn       0.64      0.85      0.73        71\n               Grass-pasture       0.88      0.63      0.73       145\n                 Grass-trees       0.74      0.81      0.77       219\n         Grass-pasture-mowed       0.78      0.88      0.82         8\n               Hay-windrowed       0.81      0.99      0.89       143\n                        Oats       0.00      0.00      0.00         6\n              Soybean-notill       0.66      0.85      0.74       292\n             Soybean-mintill       0.76      0.75      0.75       737\n               Soybean-clean       0.79      0.81      0.80       178\n                       Wheat       0.89      0.89      0.89        61\n                       Woods       0.65      0.24      0.36       380\nBuildings-Grass-Trees-Drives       0.00      0.00      0.00       116\n          Stone-Steel-Towers       0.59      0.93      0.72        28\n\n                    accuracy                           0.77      6308\n                   macro avg       0.66      0.68      0.66      6308\n                weighted avg       0.76      0.77      0.76      6308\n\nImpossible de créer la visualisation du F1-score par classe - données insuffisantes\n\nModèle sauvegardé sous 'resultats_equal20/model_equal20.h5'\n\nTableau de comparaison sauvegardé dans 'resultats_comparaison_equal/comparaison_modeles_equal.csv'\n\nAnalyse comparative terminée!\n\nRécapitulatif des précisions:\n5 segments (5 bandes): 0.6113\n10 segments (10 bandes): 0.7213\n15 segments (15 bandes): 0.7643\n20 segments (20 bandes): 0.7738\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}