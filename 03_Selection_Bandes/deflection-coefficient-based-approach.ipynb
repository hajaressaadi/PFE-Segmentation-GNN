{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11747194,"sourceType":"datasetVersion","datasetId":7374450}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Sélection des bandes discriminantes pour les images hyperspectrales\n# Méthode : coefficient de déflexion VS ES coefficient de déflexion","metadata":{}},{"cell_type":"markdown","source":"**Ce notebook implémente une approche de classification multiclasse basée sur le coefficient de déflexion pour la sélection de bandes spectrales. Nous calculons le coefficient de déflexion DC = (μi - μj)²/(σi · σj) pour chaque paire de classes sur toutes les bandes, puis utilisons le critère \"worst-case\" en sélectionnant pour chaque bande la valeur minimale du coefficient de déflexion parmi toutes les paires de classes. Deux stratégies sont évaluées : la première sélectionne directement les meilleures bandes selon leur coefficient de déflexion global (top 5, 10, 15, 20), tandis que la seconde applique une approche de segmentation spectrale en divisant l'espace spectral en segments égaux et en choisissant la meilleure bande de chaque segment selon ce critère. Les modèles MLP résultants sont comparés pour déterminer l'efficacité du coefficient de déflexion comme métrique de sélection de bandes pour la classification hyperspectrale multiclasse.**","metadata":{}},{"cell_type":"markdown","source":"### Dataset : Indian Pines ","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom scipy.io import loadmat","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T12:14:23.082867Z","iopub.execute_input":"2025-05-09T12:14:23.083153Z","iopub.status.idle":"2025-05-09T12:14:23.086972Z","shell.execute_reply.started":"2025-05-09T12:14:23.083133Z","shell.execute_reply":"2025-05-09T12:14:23.086415Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# Définir le chemin du dataset\ndataset_path = \"/kaggle/input/dataset-deflection-coefficient\"  # Chemin vers le dossier du dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T12:14:25.216027Z","iopub.execute_input":"2025-05-09T12:14:25.216503Z","iopub.status.idle":"2025-05-09T12:14:25.219717Z","shell.execute_reply.started":"2025-05-09T12:14:25.216482Z","shell.execute_reply":"2025-05-09T12:14:25.219038Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"###   Étape 1: Charger les données hyperspectrales et les vérités terrain","metadata":{}},{"cell_type":"code","source":"def charger_donnees(dataset_path):\n    \"\"\"\n    Charge les images hyperspectrales et les vérités terrain.\n    \n    Args:\n        dataset_path: Chemin vers le dossier contenant les fichiers .mat\n    \n    Returns:\n        donnees_hyperspectrales: Données spectrales (n_rows, n_cols, n_bands)\n        verite_terrain: Vérités terrain (n_rows, n_cols)\n    \"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"ÉTAPE 1: CHARGEMENT DES DONNÉES HYPERSPECTRALES\")\n    print(\"=\"*80)\n    \n    # Chargement des données corrigées\n    chemin_image = os.path.join(dataset_path, \"Indian_pines_corrected.mat\")\n    donnees_mat = loadmat(chemin_image)\n    \n    # Utiliser le nom de variable exact pour l'image hyperspectrale\n    donnees_hyperspectrales = donnees_mat['indian_pines_corrected']  # Variable: 'indian_pines_corrected'\n    print(f\"Dimensions de l'image hyperspectrale: {donnees_hyperspectrales.shape}\")\n    \n    # Charger la vérité terrain\n    chemin_gt = os.path.join(dataset_path, \"Indian_pines_gt.mat\")\n    gt_mat = loadmat(chemin_gt)\n    \n    # Utiliser le nom de variable exact pour la vérité terrain\n    verite_terrain = gt_mat['indian_pines_gt']  # Variable: 'indian_pines_gt'\n    print(f\"Dimensions de la vérité terrain: {verite_terrain.shape}\")\n    \n    # Afficher des informations sur les données chargées\n    print(f\"Nombre de classes uniques dans la vérité terrain: {len(np.unique(verite_terrain))}\")\n    print(f\"Classes uniques: {np.unique(verite_terrain)}\")\n    \n    return donnees_hyperspectrales, verite_terrain","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T12:14:28.917291Z","iopub.execute_input":"2025-05-09T12:14:28.917549Z","iopub.status.idle":"2025-05-09T12:14:28.922715Z","shell.execute_reply.started":"2025-05-09T12:14:28.917531Z","shell.execute_reply":"2025-05-09T12:14:28.922083Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"### Étape 2: Préparation des données","metadata":{}},{"cell_type":"code","source":"def preparer_donnees(donnees_hyperspectrales, verite_terrain):\n    \"\"\"\n    Prépare les données pour l'analyse de séparabilité.\n    \n    Args:\n        donnees_hyperspectrales: Données spectrales (n_rows, n_cols, n_bands)\n        verite_terrain: Vérités terrain (n_rows, n_cols)\n    \n    Returns:\n        pixels: Données spectrales (n_pixels, n_bands)\n        classes: Étiquettes de classe pour chaque pixel (n_pixels)\n        classes_uniques: Liste des classes uniques\n        class_names: Noms des classes\n    \"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"ÉTAPE 2: PRÉPARATION DES DONNÉES POUR L'ANALYSE\")\n    print(\"=\"*80)\n    \n    # Définir les noms des classes pour Indian Pines\n    # Les 16 classes + background (classe 0) d'après la documentation\n    class_names = [\n        \"Background\",             # 0\n        \"Alfalfa\",                # 1\n        \"Corn-notill\",            # 2\n        \"Corn-mintill\",           # 3\n        \"Corn\",                   # 4\n        \"Grass-pasture\",          # 5\n        \"Grass-trees\",            # 6\n        \"Grass-pasture-mowed\",    # 7\n        \"Hay-windrowed\",          # 8\n        \"Oats\",                   # 9\n        \"Soybean-notill\",         # 10\n        \"Soybean-mintill\",        # 11\n        \"Soybean-clean\",          # 12\n        \"Wheat\",                  # 13\n        \"Woods\",                  # 14\n        \"Buildings-Grass-Trees-Drives\", # 15\n        \"Stone-Steel-Towers\"      # 16\n    ]\n    \n    # Obtenir les dimensions des données\n    height, width, n_bands = donnees_hyperspectrales.shape\n    \n    # Réorganiser les données pour l'analyse\n    pixels = donnees_hyperspectrales.reshape(height * width, n_bands)\n    classes = verite_terrain.reshape(height * width)\n    # Extraire les classes uniques (y compris le background - classe 0)\n    classes_uniques = np.unique(classes)\n    \n    print(f\"Données préparées: {pixels.shape[0]} pixels avec {pixels.shape[1]} bandes\")\n    print(f\"Nombre de classes (avec background): {len(classes_uniques)}\")\n    \n    # Compter le nombre de pixels par classe\n    for classe in classes_uniques:\n        n_pixels = np.sum(classes == classe)\n        nom_classe = class_names[classe] if classe < len(class_names) else f\"Classe {classe}\"\n        print(f\"Classe {classe} ({nom_classe}): {n_pixels} pixels\")\n    \n    return pixels, classes, classes_uniques, class_names","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T12:14:31.975354Z","iopub.execute_input":"2025-05-09T12:14:31.975611Z","iopub.status.idle":"2025-05-09T12:14:31.981700Z","shell.execute_reply.started":"2025-05-09T12:14:31.975592Z","shell.execute_reply":"2025-05-09T12:14:31.981051Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"### Étape 3: Calcul des coefficients de déflexion","metadata":{}},{"cell_type":"code","source":"def calculer_coefficient_deflexion(pixels, classes, classes_uniques, class_names):\n    \"\"\"\n    Calcule le coefficient de déflexion pour chaque paire de classes et chaque bande.\n    \n    Args:\n        pixels: Données spectrales (n_pixels x n_bandes)\n        classes: Étiquettes de classe pour chaque pixel\n        classes_uniques: Liste des classes uniques\n        class_names: Liste des noms de classes\n    \n    Returns:\n        DataFrame contenant les coefficients de déflexion pour chaque paire de classes et chaque bande\n    \"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"ÉTAPE 3: CALCUL DES COEFFICIENTS DE DÉFLEXION\")\n    print(\"=\"*80)\n    \n    # Créer une liste pour stocker les résultats\n    resultats_list = []\n    \n    # Nombre total d'itérations pour la barre de progression\n    total_iterations = len(classes_uniques) * (len(classes_uniques) - 1) // 2 * pixels.shape[1]\n    \n    # Utiliser tqdm pour afficher une barre de progression\n    with tqdm(total=total_iterations, desc=\"Calcul des coefficients de déflexion\") as pbar:\n        # Pour chaque paire de classes\n        for i, classe_A in enumerate(classes_uniques):\n            for classe_B in classes_uniques[i+1:]:  # Ne considérer que les paires uniques\n                # Obtenir les noms des classes\n                nom_classe_A = class_names[classe_A] if classe_A < len(class_names) else f\"Classe {classe_A}\"\n                nom_classe_B = class_names[classe_B] if classe_B < len(class_names) else f\"Classe {classe_B}\"\n                \n                # Créer les masques pour les deux classes\n                mask_A = classes == classe_A\n                mask_B = classes == classe_B\n                \n                # Pour chaque bande spectrale\n                for bande in range(pixels.shape[1]):\n                    # Extraire les valeurs de la bande pour les deux classes\n                    valeurs_A = pixels[mask_A, bande]\n                    valeurs_B = pixels[mask_B, bande]\n                    \n                    # Vérifier que les deux classes ont des pixels\n                    if len(valeurs_A) > 0 and len(valeurs_B) > 0:\n                        # Calculer les statistiques pour chaque classe\n                        mu_A = np.mean(valeurs_A)\n                        mu_B = np.mean(valeurs_B)\n                        sigma_A = np.std(valeurs_A)\n                        sigma_B = np.std(valeurs_B)\n                        \n                        # Éviter la division par zéro\n                        if sigma_A > 0 and sigma_B > 0:\n                            # Calculer le coefficient de déflexion\n                            # DC = (μi − μj)²/(σi · σj)\n                            dc = ((mu_A - mu_B) ** 2) / (sigma_A * sigma_B)\n                        else:\n                            # Si un des écarts-types est zéro, considérer une valeur très grande\n                            # car les classes sont parfaitement séparables (pas de variation dans une classe)\n                            dc = float('inf')\n                        \n                        # Stocker les résultats dans la liste\n                        resultats_list.append({\n                            'ClasseA': int(classe_A),\n                            'ClasseB': int(classe_B),\n                            'NomClasseA': nom_classe_A,\n                            'NomClasseB': nom_classe_B,\n                            'Bande': int(bande),\n                            'CoefficientDeflexion': float(dc)\n                        })\n                    \n                    # Mettre à jour la barre de progression\n                    pbar.update(1)\n    \n    # Créer un DataFrame à partir de la liste de résultats\n    resultats_dc = pd.DataFrame(resultats_list)\n    \n    return resultats_dc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T12:14:34.586393Z","iopub.execute_input":"2025-05-09T12:14:34.586949Z","iopub.status.idle":"2025-05-09T12:14:34.594861Z","shell.execute_reply.started":"2025-05-09T12:14:34.586928Z","shell.execute_reply":"2025-05-09T12:14:34.594145Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"### Étape 4: Analyser les coefficients de déflexion et trier les bandes selon leur worst-case \n","metadata":{}},{"cell_type":"code","source":"def analyser_coefficients_deflexion(resultats_dc, top_n=20):\n    \"\"\"\n    Analyse les coefficients de déflexion et sélectionne les meilleures bandes.\n    \n    Args:\n        resultats_dc: DataFrame contenant les coefficients de déflexion\n        top_n: Nombre de meilleures bandes à sélectionner\n    \n    Returns:\n        DataFrame contenant les bandes triées selon leur coefficient représentatif\n    \"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"ÉTAPE 4: ANALYSE DES COEFFICIENTS DE DÉFLEXION\")\n    print(\"=\"*80)\n    \n    # Pour chaque bande, trouver le coefficient de déflexion minimum (worst case)\n    min_dc_par_bande = (resultats_dc\n                        .groupby('Bande')\n                        .agg({\n                            'CoefficientDeflexion': 'min'  # Prendre le coefficient minimum\n                        })\n                        .reset_index())\n    \n    # Trier les bandes par leur coefficient de déflexion minimum (ordre décroissant)\n    min_dc_par_bande = min_dc_par_bande.sort_values('CoefficientDeflexion', ascending=False)\n    \n    # Afficher les top_n meilleures bandes\n    print(f\"\\nTop {top_n} des bandes selon leur coefficient de déflexion minimum:\")\n    print(min_dc_par_bande.head(top_n))\n    \n    # Sélectionner les top_n bandes\n    top_bandes = min_dc_par_bande.head(top_n)['Bande'].tolist()\n    \n    return min_dc_par_bande, top_bandes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T12:14:38.528340Z","iopub.execute_input":"2025-05-09T12:14:38.528592Z","iopub.status.idle":"2025-05-09T12:14:38.533585Z","shell.execute_reply.started":"2025-05-09T12:14:38.528573Z","shell.execute_reply":"2025-05-09T12:14:38.532991Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"### Étape 5: Affichage et sauvegarde des rèsultats","metadata":{}},{"cell_type":"code","source":"def afficher_resultats(resultats_dc, min_dc_par_bande, top_bandes, top_n=20):\n    \"\"\"\n    Affiche et sauvegarde les résultats.\n    \n    Args:\n        resultats_dc: DataFrame contenant tous les coefficients de déflexion\n        min_dc_par_bande: DataFrame contenant les bandes triées\n        top_bandes: Liste des top bandes sélectionnées\n        top_n: Nombre de meilleures bandes à afficher\n    \"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"ÉTAPE 5: AFFICHAGE ET SAUVEGARDE DES RÉSULTATS\")\n    print(\"=\"*80)\n    \n    # Sauvegarder tous les coefficients de déflexion\n    resultats_dc.to_csv('coefficients_deflexion_complets.csv', index=False)\n    print(\"\\nTous les coefficients de déflexion ont été sauvegardés dans 'coefficients_deflexion_complets.csv'\")\n    \n    # Sauvegarder le tri décroissant des coefficients minimum de chaque bande\n    min_dc_par_bande.to_csv('coefficients_deflexion_par_bande_tri.csv', index=False)\n    print(\"Tri décroissant des coefficients minimum par bande sauvegardé dans 'coefficients_deflexion_par_bande_tri.csv'\")\n    \n    # Sauvegarder les top bandes sélectionnées\n    pd.DataFrame({\n        'Bande': top_bandes,\n        'CoefficientDeflexion': [min_dc_par_bande[min_dc_par_bande['Bande'] == bande]['CoefficientDeflexion'].values[0] for bande in top_bandes]\n    }).to_csv('top_bandes_coefficient_deflexion.csv', index=False)\n    print(\"Top bandes sélectionnées sauvegardées dans 'top_bandes_coefficient_deflexion.csv'\")\n    \n    # Afficher les résultats\n    print(f\"\\nLes {top_n} meilleures bandes sélectionnées sont:\")\n    for i, bande in enumerate(top_bandes, 1):\n        coefficient = min_dc_par_bande[min_dc_par_bande['Bande'] == bande]['CoefficientDeflexion'].values[0]\n        print(f\"{i}. Bande {bande}: Coefficient de déflexion minimum = {coefficient:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T12:14:41.679791Z","iopub.execute_input":"2025-05-09T12:14:41.680050Z","iopub.status.idle":"2025-05-09T12:14:41.685877Z","shell.execute_reply.started":"2025-05-09T12:14:41.680030Z","shell.execute_reply":"2025-05-09T12:14:41.685295Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"### Étape 6: Visualisation des résultats","metadata":{}},{"cell_type":"code","source":"def visualiser_resultats(min_dc_par_bande, top_bandes):\n    \"\"\"\n    Visualise les résultats sous forme de graphiques.\n    \n    Args:\n        min_dc_par_bande: DataFrame contenant les bandes triées\n        top_bandes: Liste des top bandes sélectionnées\n    \"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"ÉTAPE 6: VISUALISATION DES RÉSULTATS\")\n    print(\"=\"*80)\n    \n    # Visualiser les coefficients de déflexion minimum pour toutes les bandes\n    plt.figure(figsize=(14, 8))\n    \n    # Créer un bar plot pour toutes les bandes\n    bars = plt.bar(min_dc_par_bande['Bande'], min_dc_par_bande['CoefficientDeflexion'], color='lightblue', alpha=0.7)\n    \n    # Mettre en évidence les top_n bandes\n    for bande in top_bandes:\n        index = min_dc_par_bande[min_dc_par_bande['Bande'] == bande].index[0]\n        bars[index].set_color('red')\n        bars[index].set_alpha(1.0)\n    \n    plt.xlabel('Bande spectrale', fontsize=12)\n    plt.ylabel('Coefficient de déflexion minimum', fontsize=12)\n    plt.title('Coefficient de déflexion minimum par bande spectrale', fontsize=14)\n    plt.xticks(rotation=90)\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    \n    # Ajouter un texte pour indiquer les top bandes\n    top_bands_text = ', '.join([str(band) for band in top_bandes[:10]])  # Premiers 10 pour éviter de surcharger\n    plt.figtext(0.5, 0.01, f\"Top 10 bandes: {top_bands_text}...\", ha='center', fontsize=10)\n    \n    # Sauvegarder le graphique\n    plt.savefig('coefficients_deflexion_par_bande.png', dpi=300, bbox_inches='tight')\n    plt.show()\n    \n    print(\"Graphique sauvegardé dans 'coefficients_deflexion_par_bande.png'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T12:14:44.310965Z","iopub.execute_input":"2025-05-09T12:14:44.311216Z","iopub.status.idle":"2025-05-09T12:14:44.317952Z","shell.execute_reply.started":"2025-05-09T12:14:44.311191Z","shell.execute_reply":"2025-05-09T12:14:44.317250Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"### Étape 7: Fonction principale","metadata":{}},{"cell_type":"code","source":"def main():\n    print(\"\\n\" + \"#\"*100)\n    print(\"#\" + \" \"*38 + \"ANALYSE HYPERSPECTRALE\" + \" \"*38 + \"#\")\n    print(\"#\" + \" \"*26 + \"SELECTION DE BANDES PAR COEFFICIENT DE DÉFLEXION\" + \" \"*26 + \"#\")\n    print(\"#\"*100 + \"\\n\")\n    \n    # Étape 1: Charger les données\n    donnees_hyperspectrales, verite_terrain = charger_donnees(dataset_path)\n    \n    # Étape 2: Préparer les données\n    pixels, classes, classes_uniques, class_names = preparer_donnees(donnees_hyperspectrales, verite_terrain)\n    \n    # Étape 3: Calculer les coefficients de déflexion\n    resultats_dc = calculer_coefficient_deflexion(pixels, classes, classes_uniques, class_names)\n    \n    # Étape 4: Analyser les coefficients de déflexion\n    min_dc_par_bande, top_bandes = analyser_coefficients_deflexion(resultats_dc, top_n=20)\n    \n    # Étape 5: Afficher et sauvegarder les résultats\n    afficher_resultats(resultats_dc, min_dc_par_bande, top_bandes, top_n=20)\n    \n    # Étape 6: Visualiser les résultats\n    visualiser_resultats(min_dc_par_bande, top_bandes)\n    \n    print(\"\\n\" + \"#\"*100)\n    print(\"#\" + \" \"*38 + \"FIN DE L'ANALYSE\" + \" \"*38 + \"#\")\n    print(\"#\"*100 + \"\\n\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T12:14:47.835897Z","iopub.execute_input":"2025-05-09T12:14:47.836187Z","iopub.status.idle":"2025-05-09T12:14:54.229892Z","shell.execute_reply.started":"2025-05-09T12:14:47.836168Z","shell.execute_reply":"2025-05-09T12:14:54.229143Z"}},"outputs":[{"name":"stdout","text":"\n####################################################################################################\n#                                      ANALYSE HYPERSPECTRALE                                      #\n#                          SELECTION DE BANDES PAR COEFFICIENT DE DÉFLEXION                          #\n####################################################################################################\n\n\n================================================================================\nÉTAPE 1: CHARGEMENT DES DONNÉES HYPERSPECTRALES\n================================================================================\nDimensions de l'image hyperspectrale: (145, 145, 200)\nDimensions de la vérité terrain: (145, 145)\nNombre de classes uniques dans la vérité terrain: 17\nClasses uniques: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16]\n\n================================================================================\nÉTAPE 2: PRÉPARATION DES DONNÉES POUR L'ANALYSE\n================================================================================\nDonnées préparées: 21025 pixels avec 200 bandes\nNombre de classes (avec background): 17\nClasse 0 (Background): 10776 pixels\nClasse 1 (Alfalfa): 46 pixels\nClasse 2 (Corn-notill): 1428 pixels\nClasse 3 (Corn-mintill): 830 pixels\nClasse 4 (Corn): 237 pixels\nClasse 5 (Grass-pasture): 483 pixels\nClasse 6 (Grass-trees): 730 pixels\nClasse 7 (Grass-pasture-mowed): 28 pixels\nClasse 8 (Hay-windrowed): 478 pixels\nClasse 9 (Oats): 20 pixels\nClasse 10 (Soybean-notill): 972 pixels\nClasse 11 (Soybean-mintill): 2455 pixels\nClasse 12 (Soybean-clean): 593 pixels\nClasse 13 (Wheat): 205 pixels\nClasse 14 (Woods): 1265 pixels\nClasse 15 (Buildings-Grass-Trees-Drives): 386 pixels\nClasse 16 (Stone-Steel-Towers): 93 pixels\n\n================================================================================\nÉTAPE 3: CALCUL DES COEFFICIENTS DE DÉFLEXION\n================================================================================\n","output_type":"stream"},{"name":"stderr","text":"Calcul des coefficients de déflexion: 100%|██████████| 27200/27200 [00:04<00:00, 5621.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nÉTAPE 4: ANALYSE DES COEFFICIENTS DE DÉFLEXION\n================================================================================\n\nTop 20 des bandes selon leur coefficient de déflexion minimum:\n     Bande  CoefficientDeflexion\n157    157              0.008609\n158    158              0.007508\n160    160              0.005267\n159    159              0.004439\n186    186              0.004324\n162    162              0.004191\n25      25              0.004012\n152    152              0.003647\n156    156              0.003528\n185    185              0.003424\n22      22              0.003285\n176    176              0.003262\n26      26              0.003256\n178    178              0.003059\n161    161              0.003031\n51      51              0.002990\n23      23              0.002845\n28      28              0.002737\n73      73              0.002712\n98      98              0.002542\n\n================================================================================\nÉTAPE 5: AFFICHAGE ET SAUVEGARDE DES RÉSULTATS\n================================================================================\n\nTous les coefficients de déflexion ont été sauvegardés dans 'coefficients_deflexion_complets.csv'\nTri décroissant des coefficients minimum par bande sauvegardé dans 'coefficients_deflexion_par_bande_tri.csv'\nTop bandes sélectionnées sauvegardées dans 'top_bandes_coefficient_deflexion.csv'\n\nLes 20 meilleures bandes sélectionnées sont:\n1. Bande 157: Coefficient de déflexion minimum = 0.0086\n2. Bande 158: Coefficient de déflexion minimum = 0.0075\n3. Bande 160: Coefficient de déflexion minimum = 0.0053\n4. Bande 159: Coefficient de déflexion minimum = 0.0044\n5. Bande 186: Coefficient de déflexion minimum = 0.0043\n6. Bande 162: Coefficient de déflexion minimum = 0.0042\n7. Bande 25: Coefficient de déflexion minimum = 0.0040\n8. Bande 152: Coefficient de déflexion minimum = 0.0036\n9. Bande 156: Coefficient de déflexion minimum = 0.0035\n10. Bande 185: Coefficient de déflexion minimum = 0.0034\n11. Bande 22: Coefficient de déflexion minimum = 0.0033\n12. Bande 176: Coefficient de déflexion minimum = 0.0033\n13. Bande 26: Coefficient de déflexion minimum = 0.0033\n14. Bande 178: Coefficient de déflexion minimum = 0.0031\n15. Bande 161: Coefficient de déflexion minimum = 0.0030\n16. Bande 51: Coefficient de déflexion minimum = 0.0030\n17. Bande 23: Coefficient de déflexion minimum = 0.0028\n18. Bande 28: Coefficient de déflexion minimum = 0.0027\n19. Bande 73: Coefficient de déflexion minimum = 0.0027\n20. Bande 98: Coefficient de déflexion minimum = 0.0025\n\n================================================================================\nÉTAPE 6: VISUALISATION DES RÉSULTATS\n================================================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1400x800 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAABW0AAAMgCAYAAABYgUA7AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAACo/0lEQVR4nOzdeZxVdf0/8NcFZBFZAwREcSO0EE1NFBdcSFA0MTXFUtTStNwytTR3LVPTFJfMDTUj17JMswyXFskNzd1cUFJjEwWXQGXO7w9/M1+HGWC4XOTKPJ+Pxzzgfs7nnPs+937uvTOv+cznlIqiKAIAAAAAQFVosawLAAAAAADg/whtAQAAAACqiNAWAAAAAKCKCG0BAAAAAKqI0BYAAAAAoIoIbQEAAAAAqojQFgAAAACgightAQAAAACqiNAWAAAAAKCKCG0BAD6lbrnllpRKpfzwhz9c1qUAAAAVJLQFgOXU888/n1133TW9evVKixYt0rlz50VuO+WUU1IqlXLvvfeWdZ/33ntvSqVSTjnllCWuf1nbb7/9UiqV8vLLL3/i910qlbL11lsvtM/zzz+fAw44IKNHj86PfvSjRvs8/PDD+dKXvpTu3bunVCplgw02SLJszq0p5/Rp9PLLL6dUKmW//fZbouNcffXVKZVKufrqqytSF0umUs/rJ2VJ37tZcsvrexwAy06rZV0AACwvHnnkkVxyySX561//mtdffz01NTXp3bt3Bg8enH333Tdf+tKXPrFa5s2bl5EjR+aFF17IPvvskz59+qRt27aL3PZptd9+++Waa67JpEmTsvrqqy/rcpa6//3vf9l9990zePDgXHHFFY32mT17dkaMGJE5c+Zkn332Sbdu3dKzZ89PuFKAZWPrrbfOfffdl6IolnUpAFAWoS0ALKGampocffTR+dnPfpZWrVpl2223zZe//OWssMIKeemll3L77bfnuuuuy2mnnZYTTzzxE6lp0qRJefrpp3PggQfmsssua/K2Qw89NHvttVdWW221su53k002yTPPPJNu3bqVXTuL9vjjj2ePPfbIkUcemVatGv927sEHH8y0adPyox/9KMcff/wnXGFDzzzzTFZcccVlXUbFrbLKKnnmmWfSqVOnJTrOrrvumk033TS9evWqUGUAAHyaCW0BYAmdcMIJ+dnPfpYNNtggN998c9Zaa6162//3v//loosuyhtvvPGJ1fT6668nSXr37r1Y27p167ZEgeuKK66YddZZp+z9aZpBgwZl0KBBC+2zsOd5WVhex8UKK6xQkXPr1KnTEge/AAAsP6xpCwBL4IUXXsjZZ5+dz3zmM7nzzjsbBLZJ0q5duxxzzDE59dRT67XPmDEjRx55ZNZYY420adMmPXr0yFe/+tU8+eSTjd7X+++/n/POOy8bbrhh2rdvnw4dOmTLLbfM73//+3r9Vl999QwZMiRJcuqpp6ZUKtWtM7uwbcnC10X817/+la997Wvp06dP2rRpk169emX48OG57bbb6vosbE3badOm5bvf/W7WXnvttGnTJt26dctuu+3W6PmuvvrqWX311fPOO+/kiCOOSO/evdOmTZsMHDgwN998c4O+11xzTZJkjTXWqDunpq4t+NRTT2WnnXZKhw4d0qlTp+y4444LfA5q/e53v8t2222XLl26pG3bthkwYEB++tOfZt68eU26z1pXXHFFBgwYkLZt22bVVVfNsccemzlz5iyw/9tvv52TTz45n//859OuXbt07tw5w4YNy9///vd6/UqlUkaPHp0k2X///esek6asl/rXv/41O++8c7p165Y2bdqkX79+OeGEE/Lee+/V9XnhhRfSoUOH9OnTp8EvIxa0bUHPyeK8DmrX4p00aVLGjBmTddZZJ23atEnfvn1z6qmnpqamZpHnl9Rfr/SZZ57JTjvtlM6dO6dLly4ZNWpUZsyYkSSZMGFCtttuu3Ts2DFdunTJN7/5zbz77rsLPNbHbb311imVSvnggw/qXntt2rTJZz/72VxyySUNalrQmra1j9trr72WvffeO926dUuHDh0yYsSIvPTSS0k+msU8cuTIdO3aNR06dMjuu++eqVOn1jvOwl6bCzqH2tfhrFmzcsghh6RXr15p3759ttpqq0ycODHJR78c+PrXv54ePXqkXbt22X777fP8888v6ilo8DjNmTMnP/jBD7Laaqulbdu2WXfddXPhhRc2+NP2WbNm5ayzzsqQIUPSu3fvtG7dOr17986+++6bF198scHxP/6edvXVV2fDDTfMiiuuuFhrjz711FMZMWJEOnfunJVWWinbb799HnnkkQb9HnnkkRx66KEZMGBAOnXqlHbt2mW99dbLT37yk3zwwQcN+i/O+1yt//znPxk1alS6du2alVZaKUOGDMlf//rXhdbflNf0okycODG77757VltttbRp0ybdu3fPF7/4xQbradee01tvvZVvfetb6dmzZ9q2bZsvfOEL+fWvf93osYuiyFVXXZXNN988HTt2zIorrpiNN944V1111QL7jx07NltuuWU6d+6cFVdcMf369cu3vvWtTJ48OclHr5v77ruv7v+1X7VjfP73gF133TWf+cxn6q31/dvf/jajRo3K2muvnRVXXDGdOnXKlltumVtuuaXJj1vS9M9uAJifmbYAsASuvvrqzJs3L9/61rey8sorL7RvmzZt6v4/ffr0bLbZZnnxxRez9dZbZ6+99sqkSZNy88035/bbb8+f/vSnbLHFFnX9586dm+HDh+fee+/NBhtskG984xv54IMPcvvtt2eXXXbJhRdemEMPPTRJcuSRR+axxx7LNddckyFDhtSFE1tvvXU6d+68wG0Lc8stt2TvvfdOURTZeeed079//0ybNi0PPPBArrzyyuy8884L3b/2PF999dVsv/32GTlyZKZNm5Zbbrklf/rTnzJ+/PgGM0c/+OCDbL/99nnzzTez22675b333sv111+fr371q7nzzjuz/fbb153v1VdfnX/961854ogj6i6q1pS1bZ988slsvvnmeeedd/KVr3wl/fr1y4MPPpjNN98866+/fqP7HHfccfnJT36SVVZZJV/5ylfSqVOn/O1vf8sxxxyTBx54IDfddNMi7zdJTj/99Jx00klZeeWVc+CBB2aFFVbIDTfckGeeeabR/jNnzsxWW22Vp556KptvvnkOPvjgzJ49O7/73e+yzTbb5KabbsrIkSOTJCeffHIee+yx/O53v8suu+xSdwGy2n8X5Oc//3m+853vpHPnztl5553To0ePPPzww/nRj36Ue+65J/fcc09at26dtddeOxdeeGH233//fPOb38xvf/vbJB89Z6NGjcp7772X3//+9/nMZz6z0Ptb3NdBrWOOOSb33XdfdtpppwwbNiy33nprTjnllLz//vsLvChbYyZNmpTBgwdn4403zje/+c08/PDDuf766/Of//wnP/nJT7L99tvnS1/6Ug466KDce++9ufLKK1NTU7PAMKkxo0aNyoMPPpgddtghLVu2zI033pjvfOc7WWGFFXLggQc26Rhvvvlmtthii/Ts2TOjR4/Ov//97/zhD3/Is88+m9/97nfZcssts9FGG+WAAw7II488kltuuSUzZ87M3Xff3eQ6F+T999/Pl770pcyZMyd77rlnpk6dmhtvvDFDhw7N/fffn2HDhqVXr175+te/nhdeeCG33XZbRowYkWeeeSYtW7Zs8v189atfzaOPPprddtstyUfvOYcffnhefvnlnHvuuXX9nnnmmZx00knZZpttsuuuu6Z9+/Z59tlnM27cuNx+++2ZOHFi+vbt2+D455xzTu65557ssssu2X777Ztc20svvZTNN988G264YQ455JC88soruemmm7LVVlvl7rvvrve+dfnll+e2227LVlttlR133DHvvfde7r333hx33HF56KGHGg37mvo+lyT//e9/s9lmm+W1117LsGHDsuGGG+aZZ57Jl770pWyzzTaN1t/U1/TCPPbYYxk8eHBatmyZXXbZJX379s1bb72Vp59+Opdddll++MMf1uv//vvvZ+jQoXnnnXeyzz775N13382NN96YvffeOzNmzMhhhx1W17coinzta1/Lr3/96/Tr1y977713Wrdunbvuuivf+MY38vTTT+enP/1pXf+amprsueeeufnmm7PKKqtk1KhR6dixY15++eXceOON2WGHHbLaaqvl5JNPztVXX51XXnklJ598ct3+878HvvDCC9l0002z3nrrZb/99ssbb7xR93gcd9xxad26dbbYYov06tUr06dPz+9///vsvvvuGTNmTL3zWJDF+ewGgAYKAKBsW2+9dZGk+Mtf/rJY++2///5FkuK4446r13777bcXSYq11167mDdvXl378ccfXyQpTjzxxKKmpqauffbs2cXGG29ctG7dunjttdfq2u+5554iSXHyySc3uO+FbTv55JOLJMU999xT1zZlypSiffv2Rfv27YuJEyc22Oc///nPIo89ePDgomXLlsWdd95Zr/25554rOnToUKy33nr12vv27VskKXbZZZdi7ty5de1/+ctfiiTFsGHD6vUfPXp0kaSYNGlSg/oWZsiQIUWS4rrrrqvXftxxxxVJGhzzz3/+c939v/POO3XtNTU1xcEHH1wkKW6++eZF3u/zzz9ftGrVqlhllVWKqVOn1rXPmjWr6N+/f5GkGDJkSL199t577yJJcfnll9drnzp1arHqqqsW3bt3L/73v//VtY8dO7ZIUowdO7bB/Tf2eD311FNFq1ativXXX7+YMWNGvf5nnnlmkaT46U9/Wq99r732KpIUl1xySVEURXHMMcc0Oq6Lomj0nBb3dVBb9xprrFG8/vrrde3Tp08vOnfuXHTo0KHeeFmQSZMm1T2/559/fl17TU1NseOOOxZJis6dOxe33npr3bb333+/GDhwYNGqVatiypQpDY41evToevdRO7YGDRpUzJo1q6792WefLVq1alX079+/Xv8FPV+1dX73u9+t137IIYfU1bmgc3jkkUfq2hf2ul/QOdS+DvfYY4/igw8+qGs/66yz6u77u9/9br33pNq6brnllgb305jax6l///7FW2+9Vdf+1ltvFf379y9KpVLx0EMP1Wt/4403Ghzn7rvvLlq0aFF885vfrNde+57Wvn374vHHH29STUVRf4z84Ac/qLftzjvvLJI0eN965ZVXig8//LBeW01NTXHAAQcUSYq///3v9baV+z53xhln1Gv/xS9+UVfrx9+7y3lNN+aoo44qktR7PdSa/7i157TVVlvVO6f//Oc/Rbdu3Yo2bdoUr776al37ZZddViQp9t9//+L999+va587d26x8847F0mKhx9+uK79wgsvLJIU2223XfHee+/Vu+/33nuv3tioHVuN+fjze9JJJzXa58UXX2zQ9vbbbxfrrbde0alTp+Ldd9+tt62x97jF/ewGgI8T2gLAElhnnXWKJMWzzz7b5H3mzp1btG3btvjMZz7T4Ie+oiiKL33pS0WS4q9//WtRFEUxb968okuXLsVaa61V74e+Wr///e+LJMWFF15Y11bJ0LY2oFnQD7aLOvbEiROLJMUBBxzQ6D61gcATTzxR11b7g/9LL73UoH/fvn2Lrl271msrJ7R95ZVXiiTFwIEDG2x7++23i86dOzc45pe//OUiSfHKK6802Oett94qSqVSsdtuuy3yvk899dQiSXHuuec22PbLX/6ywQ//06dPL1q2bFlsu+22jR5vzJgxRZLitttuq2tb3ND28MMPrzfuPm7evHlF9+7di4022qhe+1tvvVWsvvrqRbt27YoxY8YUpVKp2GSTTeoFfLXmP6fFfR18vO6rrrpqgefUlGCuNrBp7DV17bXXFkmKbbbZpsF+p512WpGkuPvuuxsca0Gh7cf7zr9t9uzZdW0LC21XWmmlBo/RX//610Wew8cfpyUJbecf75MnT15kXU15vyiKBf/ipCj+77Vw6KGHNulY6623XrH66qvXa6t9T5s/9F6U2sekc+fOxdtvv91g+3bbbdcgUFyQRx55pEhSnHLKKfXaF+d9rvb10qNHj3q/nCmKj16f/fr1a/DeXc5rujG179F/+tOfFtm39pzmD6iLoihOP/30BkHxwIEDi/bt2zcIYIuiKB5//PEiSfG9732vrm3dddctWrZsWfz73/9eZC1NCW179uzZpF/0fNy5555bJCnuvffeeu3zv8eV89kNAB9neQQA+IQ9++yzmTNnTrbZZpusuOKKDbZvs802ueuuu/LYY49lyy23zHPPPZc333wzvXv3brAubvLRn5jXHndpePDBB5Ok3p/pLo5//vOfSZKpU6c2up5mbd3PPvtsBgwYUNfeuXPnrLHGGg369+nTJxMmTCirlo/717/+lSSN/vn9SiutlA022KDB2r7//Oc/0759+wX+eXy7du2a9DzU3veWW27ZYFtjbQ899FDmzZuXuXPnNvoY1q4h+uyzz2annXZa5P03pvZ5ql2uYn4rrLBCg3Pr1KlTfvWrX2WrrbbK4Ycfng4dOmTcuHFp1WrR32Iu7uvg4zbaaKMG/fv06ZMkeeuttxZ537UGDhyYUqlUr61Xr15JGl9KonZb7UXemmJRtXbo0GGRx+jXr1+Dx6i2loWdw+LUuSBdunTJaqut1ujxF1bX4t73wl4Ljz76aL32e++9N+eff34eeOCBzJgxIx9++GHdtgX9qf8mm2yyWPXU+sIXvpCVVlqp0drGjx+fRx99tO45fv/993PRRRfl+uuvz7PPPpt33nmn3pq8jT0mTX2fe+655zJnzpxsu+22adu2bb2+LVq0yOabb95gLeFyXtON+epXv5rzzz8/u+66a/bcc8986UtfylZbbZVVVlml0f6tWrXKZptt1qB9/ufzvffeyxNPPJHevXvnrLPOatC/dh3g2hrfeeedPPPMM1l77bXTr1+/RdbdFOuvv/4Cx8y0adPyk5/8JH/84x/zyiuv5H//+1+97Ysa48v6sxuATz+hLQAsgZ49e+bZZ5/Na6+9lv79+zdpn9mzZyfJAtfArQ09avvNnDkzyUcXw3nqqacWeNz5L5BUKbNmzUqSBf6Avii19d9+++25/fbbF9hv/vo7derUaL9WrVo1+YJTC1N7Xj169Gh0e2PPz8yZM/Phhx82+gN4raY8Dwu77wXdb5L84x//yD/+8Y8luu8Fqb2PxVkTNkk23HDD9O3bNy+99FJ22GGHRi/G15jFfR18XMeOHRu01QbFi3MxuIUdZ2HbGruoVDn30dRaP4k6l/V9NzYOattqXy9JctNNN2XPPffMSiutlGHDhmX11VfPiiuuWHcRt1deeaXJxy+3rgXVtvvuu+e2227LZz/72ey5557p0aNHVlhhhbz11lu54IILMnfu3AbHaer7XLnvV8niv6bnN2jQoNx777358Y9/nHHjxmXs2LFJki9+8Ys566yzGqyn261bt7Ro0fB61/M/Zm+++WaKoshrr73WpPfUJf0sasyCnt+ZM2fmi1/8YiZPnpzNN988Q4cOTefOndOyZcu69cIbez7nP0ay7D67Afj0E9oCwBLYfPPNc++992b8+PHZdtttm7RPbdAx/9Xda02ZMqVev9p/d9tttwVeUXxpqr2w12uvvdaki3vNr7b+arvgSm1YMm3atEa3N/b8dOzYMaVSKTNmzKjYfc9/0aQF3W+SfO9736t3UZ5Kqr2P2bNnN2n2Z61jjjkmL730Uj7zmc/kxhtvzOjRo7Pjjjs2+f6a+jpgydSGaB+flVrr48HjsjJ16tQGM3prx8bHg81TTjklbdu2zSOPPNJgtuX111+/wOPPPxt5cepaWHttbQ899FBuu+22DBs2LLfffnu9C53985//zAUXXFDW/dcq9/0qWfzXdGO23HLL/PGPf8z//ve/PPDAA7nttttyySWXZMSIEXnyySez5ppr1vWdMWNGampqGgS38z9mtfVttNFGefjhhxdZQ+1+r7322hKdy8ctaFxceeWVmTx5ck4//fSccMIJ9bb95Cc/ye9+97tFHntZf3YD8OnX8FegAECT7bfffmnZsmUuu+yyuj91XJDaWTnrrLNO2rZtm4ceeijvvfdeg361f5Jf++fZ6667bjp27JiHH364IjPnFlftnxX/+c9/Lmv/2qurV2JJgwWpDUgWZ5bl+uuvnyT5+9//3mDbO++8k8cee6xB+6BBg/LGG280+DPkxVV733/7298abGus7Ytf/GJKpdJSfQxrn6faP6luittvvz0XXXRRhgwZkocffjhdunTJ/vvvv8Cg6+MW93XAkunSpUuSxgOv+ZcfWBYW9lr4whe+UNf24osvZt11120Q2P73v//NSy+9VPG6Hn300bzzzjuLrO3FF19MkowYMaJeYPvxvkvis5/9bNq2bZuHH344c+bMqbetpqYm999/f4N9ynlNL0q7du2y9dZb59xzz83xxx+f//3vf7nrrrvq9fnwww8bfa+a/zHr0KFD1l133TzzzDNNWtZkpZVWyuc+97lMmjSpSe/B5Xwu1Kp9PnfZZZcG25r6fC7rz24APv2EtgCwBNZee+0ce+yxmTFjRnbYYYdMmjSpQZ85c+bkvPPOq1uLtHXr1hk1alRmzJiRM888s17fO++8M3/605+y9tprZ/PNN0/y0Z/JHnLIIXnllVdy9NFHN/rD35NPPrnAGVhLavTo0VlppZVy7rnnNhpkLmrW0yabbJJBgwbl17/+dW644YYG22tqanLfffctUY1du3ZNkvznP/9p8j6rrbZattpqqzz++OP51a9+VW/bj3/840ZDhMMPPzxJcsABB+SNN95osH3KlCl55plnFnnfe++9d1q2bJnzzjuv3vM2e/bsnHHGGQ369+zZM1/96ldz//3355xzzqm3TmatBx54oNHws6m+/e1vp1WrVjnssMMyefLkBtvfeuuteuHelClTsv/++6dLly657rrrsvrqq+eyyy7LtGnTMnr06EZr/LjFfR2wZPr3758OHTrk97//fd2fbScfzX5sbMx90k4//fR6M35nzZqVM844I6VSKaNHj65r79u3b1544YV6vxiYM2dODjnkkKUSjL311lsNlheoXSN2wIABdevZ1s6Yn/+XQE899VSD8V2ONm3a5Ktf/WqmTZuWc889t962K664Iv/+978b7LO4r+kFmTBhQoOgOPm/mbPzr7GbJMcff3zef//9utuvvvpqLrjggrRp0yZ77bVXXfvhhx+e9957LwceeGCjywRMmjQpL7/8ct3t73znO5k3b16+/e1vN1hjds6cOfXGdjmfC7UW9HyOGzcud9xxR5OOsaw/uwH49LM8AgAsoTPOOCNz5szJz372s/Tv3z/bbrttBgwYkBVWWCGTJk3KX/7yl7zxxhv1gpGzzjor9913X84444zcf//9GTRoUF5++eXcdNNNWXHFFTN27Nh6f1p66qmnZuLEiRkzZkxuv/32bLXVVunRo0dee+21PPHEE/nXv/6VCRMmLHC9wyXRo0ePXHvttdlrr72yySab5Mtf/nL69++fGTNm5IEHHsjqq6+eW2+9daHH+PWvf51tttkme+21V84///xsuOGGadeuXSZPnpwJEyZk+vTpjYYCTbXtttvmpz/9aQ466KDstttuad++ffr27Zt99tlnoftdfPHF2XzzzbPvvvvm1ltvTb9+/fLggw/moYceypZbbtlgRtXw4cNz4okn5vTTT8/aa6+d4cOHp2/fvnnjjTfywgsv5G9/+1vOOOOMrLvuugu937XXXjsnnXRSTj755AwcODBf/epX06pVq9xyyy0ZOHBgnnvuuQb7XHLJJXnuuedy7LHH5pe//GU222yzdO7cOf/5z3/y8MMP5/nnn89///vfRi/q1RQDBgzIJZdckkMOOST9+/fPjjvumLXWWitvv/12Xnrppdx3333Zb7/9cumll6Yoiuy7776ZPn16br755roLa+2+++75xje+kSuvvDLnnXdevve97y30Phf3dUD5WrduncMOOyw//vGPs+GGG2aXXXbJ22+/ndtuuy1Dhgypm1m4rHz2s5/NgAEDsttuuyVJbrnllrz66qs56qijsvHGG9f1O+yww3LYYYflC1/4Qnbfffd8+OGHueuuu1IURdZff/26i/xVypZbbpmf//zneeCBB7LpppvWjc927drliiuuqOu3ySabZJNNNsmNN96Y//73v9l0000zefLk/P73v8+IESMq8ufxP/nJTzJ+/PiccMIJ+fvf/54vfOELeeaZZ3LHHXdk++23b/DXEIvzml6Ys846K/fcc0+22mqrrLHGGmnbtm0mTpyY8ePHZ80118yuu+5ar3+vXr3y7rvvZuDAgdl5553z7rvv5sYbb8wbb7yRMWPG1FuT9lvf+lb++c9/5pprrsk//vGPDB06NL17987UqVPz7LPP5oEHHsi4cePqluY55JBDct999+XGG29Mv3798uUvfzkdO3bM5MmT86c//SlXXnllRo4cmeSjz4Wbb745u+22W3bYYYe0bds266+/fnbeeedFPtb77LNPzjrrrBx22GG555570rdv3/zrX//K+PHj85WvfCW/+c1vmvCMLdvPbgCWAwUAUBEPPfRQccABBxRrr7120a5du6JNmzbF6quvXuy9997FXXfd1aD/9OnTi8MPP7zo27dvscIKKxTdunUrdt999+KJJ55o9Pgffvhh8Ytf/KLYfPPNi44dOxZt2rQpVltttWL48OHFz3/+8+Kdd96p63vPPfcUSYqTTz65wXEWtu3kk08ukhT33HNPg22PPvpo8dWvfrVYeeWVixVWWKHo1atXscMOOxR/+MMfmnTsmTNnFieccEIxYMCAol27dsVKK61U9OvXr9h7772L3/zmN/X69u3bt+jbt2+jj8OQIUOKxr6FOfvss4t+/foVK6ywQpGkGDJkSKP7z++JJ54odtxxx2KllVYqOnToUOywww7FE088UYwePbpIUkyaNKnBPnfddVex8847F927dy9WWGGFomfPnsVmm21WnH766cXkyZObdL9FURSXX3558bnPfa5o3bp10adPn+Loo48u3nvvvQXW/9577xVnn312sdFGGxXt27cv2rVrV6yxxhrFyJEji2uvvbb44IMP6vqOHTu2SFKMHTu2wXEWdm4PPvhgsddeexW9e/euG5cbbrhh8YMf/KB45plniqIoinPOOadIUnzzm99ssP8777xTfPazny1at25dTJw4sa59Qee0OK+DhdW9sLE7v0mTJhVJitGjRzfYtrAx3NhjuqBjLWicLug8FvR8LehxK+cc5s2bV5xyyinFqquuWrRu3br47Gc/W1xwwQXFSy+91OixFvY6LKeuxtQ+Tv/73/+KY489tq62/v37F2PGjClqamrq9a+pqSkuvfTS4vOf/3zRtm3bomfPnsU3vvGNYtq0aY0+5oszLhZ0Hk8++WSx4447Fh07dizat29fDB06tHj44Ycb7DNt2rTigAMOKHr37l20bdu2WG+99YqLL764rMd3QePnlVdeKfbcc8+ic+fOxYorrlhsueWWxX333bfQ82zKa3ph7rzzzmLfffct+vfvX3To0KFYaaWVis997nPF8ccfX0yfPr3Rc5o5c2Zx0EEHFSuvvHLRpk2bYv311y/GjRu3wPu44YYbiqFDhxZdunQpVlhhhWKVVVYptt566+Lcc89tcB81NTXFFVdcUWy66aZF+/btixVXXLHo169fcfDBB9d7//3ggw+KY489tlhttdWKVq1a1XsOmjJOH3vssWL77bcvunTpUnTo0KEYMmRI8Ze//GWxX6uL89kNAB9XKopF/O0aAADAUrD11lvnvvvuW+RyGnw61M6I/fiSBgBAefy9GQAAAABAFRHaAgAAAABUEaEtAAAAAEAVsaYtAAAAAEAVMdMWAAAAAKCKCG0BAAAAAKpIq2VdwPKipqYmr7/+ejp06JBSqbSsywEAAAAAqkxRFHn77bfTu3fvtGix4Pm0QtsKef3117Pqqqsu6zIAAAAAgCr3n//8J3369FngdqFthXTo0CHJRw94x44dl3E1n4yamppMnz493bt3X+hvBqApjCcqxViikownKsVYopKMJyrFWKKSjCcqaXkeT7Nnz86qq65alyUuiNC2QmqXROjYsWOzCm3nzJmTjh07LncvID55xhOVYixRScYTlWIsUUnGE5ViLFFJxhOV1BzG06KWV10+zxoAAAAA4FNKaAsAAAAAUEWEtgAAAAAAVURoCwAAAABQRYS2AAAAAABVRGgLAAAAAFBFhLYAAAAAAFVEaAsAAAAAUEWEtgAAAAAAVURoCwAAAABQRYS2AAAAAABVRGgLAAAAAFBFhLYAAAAAAFVEaAsAAAAAUEWEtgAAAAAAVURoCwAAAABQRYS2AAAAAABVRGgLAAAAAFBFhLYAAAAAAFVEaAsAAAAAUEWEtgAAAAAAVURoCwAAAABQRYS2AAAAAABVRGgLAAAAAFBFhLYAAAAAAFVEaAsAAAAAUEWEtgAAAAAAVURoCwAAAABQRYS2AAAAAABVRGgLAAAAAFBFWi3rAgAAAADmd/+rMxu0De7TdRlUAvDJM9MWAAAAAKCKCG0BAAAAAKqI0BYAAAAAoIoIbQEAAAAAqojQFgAAAACgightAQAAAACqiNAWAAAAAKCKCG0BAAAAAKqI0BYAAAAAoIoIbQEAAAAAqojQFgAAAACgightAQAAAACqiNAWAAAAAKCKCG0BAAAAAKqI0BYAAAAAoIoIbQEAAAAAqojQFgAAAACgightAQAAAACqiNAWAAAAAKCKCG0BAAAAAKqI0BYAAAAAoIoIbQEAAAAAqojQFgAAAACgightAQAAAACqiNAWAAAAAKCKCG0BAAAAAKqI0BYAAAAAoIoIbQEAAAAAqojQFgAAAACgightAQAAAACqiNAWAAAAAKCKCG0BAAAAAKqI0BYAAAAAoIoIbQEAAAAAqojQFgAAAACgightAQAAAACqiNAWAAAAAKCKCG0BAAAAAKqI0BYAAAAAoIoIbQEAAAAAqojQFgAAAACgilRdaHvxxRdn9dVXT9u2bTNo0KA8+OCDC+1/0003ZZ111knbtm2z3nrr5Y477qi3vSiKnHTSSenVq1fatWuXoUOH5vnnn6/X59///nd22WWXdOvWLR07dswWW2yRe+65p+LnBgAAAACwKFUV2t5www056qijcvLJJ2fixIlZf/31M2zYsEybNq3R/vfff39GjRqVb3zjG3n00UczcuTIjBw5Mk8++WRdn7PPPjtjxozJpZdemgceeCDt27fPsGHDMmfOnLo+O+20Uz788MPcfffdeeSRR7L++utnp512ypQpU5b6OQMAAAAAfFxVhbbnnXdeDjzwwOy///753Oc+l0svvTQrrrhirrrqqkb7X3DBBRk+fHiOOeaYrLvuujn99NOz4YYb5qKLLkry0Szb888/PyeccEJ22WWXDBw4MNdee21ef/313HrrrUmSGTNm5Pnnn88PfvCDDBw4MP369ctPfvKTvPfee/XCXwAAAACAT0KrZV1Arffffz+PPPJIjjvuuLq2Fi1aZOjQoZkwYUKj+0yYMCFHHXVUvbZhw4bVBbKTJk3KlClTMnTo0LrtnTp1yqBBgzJhwoTstdde+cxnPpP+/fvn2muvzYYbbpg2bdrkF7/4RXr06JGNNtpogfXOnTs3c+fOrbs9e/bsJElNTU1qamoW+/w/jWpqalIURbM5X5Yu44lKMZaoJOOJSjGWqCTjiUqp9rFUFEWDtmqtleofT3y6LM/jqannVDWh7YwZMzJv3rysvPLK9dpXXnnlPPvss43uM2XKlEb71y5rUPvvwvqUSqX85S9/yciRI9OhQ4e0aNEiPXr0yJ133pkuXbossN4zzzwzp556aoP26dOn11t6YXlWU1OTWbNmpSiKtGhRVZO2+RQynqgUY4lKMp6oFGOJSjKeqJRqH0vvz3q7Qdu0FT5YBpXQFNU+nvh0WZ7H09tvN3xva0zVhLbLSlEU+c53vpMePXrkb3/7W9q1a5crrrgiO++8cx566KH06tWr0f2OO+64erN8Z8+enVVXXTXdu3dPx44dP6nyl6mampqUSqV07959uXsB8ckznqgUY4lKMp6oFGOJSjKeqJRqH0svfrBCg7YePRY8uYplq9rHE58uy/N4atu2bZP6VU1o261bt7Rs2TJTp06t1z516tT07Nmz0X169uy50P61/06dOrVe+Dp16tRssMEGSZK77747f/jDH/Lmm2/Wha2XXHJJ7rrrrlxzzTX5wQ9+0Oh9t2nTJm3atGnQ3qJFi+VuMC1MqVRqdufM0mM8USnGEpVkPFEpxhKVZDxRKdU8lkqlUoO2aqyT/1PN44lPn+V1PDX1fKrmrFu3bp2NNtoo48ePr2urqanJ+PHjs9lmmzW6z2abbVavf5Lcdddddf3XWGON9OzZs16f2bNn54EHHqjr89577yVp+IC1aNFiuVw3AwAAAACoblUz0zZJjjrqqIwePTobb7xxNtlkk5x//vl59913s//++ydJ9t1336yyyio588wzkyRHHHFEhgwZknPPPTcjRozI9ddfn4cffjiXXXZZko8S+SOPPDJnnHFG+vXrlzXWWCMnnnhievfunZEjRyb5KPjt0qVLRo8enZNOOint2rXL5ZdfnkmTJmXEiBHL5HEAAAAAAJqvqgpt99xzz0yfPj0nnXRSpkyZkg022CB33nln3YXEJk+eXG9G7ODBgzNu3LiccMIJOf7449OvX7/ceuutGTBgQF2fY489Nu+++24OOuigvPXWW9liiy1y55131q0f0a1bt9x555354Q9/mG233TYffPBBPv/5z+d3v/td1l9//U/2AQAAAAAAmr1SURTFsi5ieTB79ux06tQps2bNalYXIps2bVp69Oix3K0vwifPeKJSjCUqyXiiUowlKsl4olKqfSzd/+rMBm2D+3RdBpXQFNU+nvh0WZ7HU1MzxOXrrAEAAAAAPuWEtgAAAAAAVURoCwAAAABQRYS2AAAAAABVRGgLAAAAAFBFhLYAAAAAAFVEaAsAAAAAUEWEtgAAAAAAVURoCwAAAABQRYS2AAAAAABVRGgLAAAAAFBFhLYAAAAAAFVEaAsAAAAAUEWEtgAAAAAAVURoCwAAAABQRYS2AAAAAABVRGgLAAAAAFBFhLYAAAAAAFVEaAsAAAAAUEWEtgAAAAAAVURoCwAAAABQRYS2AAAAAABVRGgLAAAAAFBFhLYAAAAAAFVEaAsAAAAAUEWEtgAAAAAAVURoCwAAAABQRYS2AAAAAABVRGgLAAAAAFBFhLYAAAAAAFVEaAsAAAAAUEVaLesCAAAAAJrq/ldnNmgb3KfrMqgEYOkx0xYAAAAAoIoIbQEAAAAAqojQFgAAAACgightAQAAAACqiNAWAAAAAKCKCG0BAAAAAKqI0BYAAAAAoIoIbQEAAAAAqojQFgAAAACgightAQAAAACqiNAWAAAAAKCKCG0BAAAAAKqI0BYAAAAAoIoIbQEAAAAAqojQFgAAAACgightAQAAAACqiNAWAAAAAKCKCG0BAAAAAKqI0BYAAAAAoIoIbQEAAAAAqojQFgAAAACgightAQAAAACqiNAWAAAAAKCKCG0BAAAAAKqI0BYAAAAAoIoIbQEAAAAAqojQFgAAAACgightAQAAAACqiNAWAAAAAKCKCG0BAAAAAKqI0BYAAAAAoIoIbQEAAAAAqojQFgAAAACgightAQAAAACqiNAWAAAAAKCKCG0BAAAAAKqI0BYAAAAAoIoIbQEAAAAAqojQFgAAAACgightAQAAAACqiNAWAAAAAKCKCG0BAAAAAKqI0BYAAAAAoIoIbQEAAAAAqojQFgAAAACgightAQAAAACqiNAWAAAAAKCKCG0BAAAAAKqI0BYAAAAAoIoIbQEAAAAAqojQFgAAAACgightAQAAAACqiNAWAAAAAKCKCG0BAAAAAKqI0BYAAAAAoIoIbQEAAAAAqojQFgAAAACgightAQAAAACqiNAWAAAAAKCKCG0BAAAAAKqI0BYAAAAAoIoIbQEAAAAAqojQFgAAAACgightAQAAAACqiNAWAAAAAKCKCG0BAAAAAKqI0BYAAAAAoIoIbQEAAAAAqojQFgAAAACgightAQAAAACqiNAWAAAAAKCKCG0BAAAAAKqI0BYAAAAAoIoIbQEAAAAAqojQFgAAAACgightAQAAAACqiNAWAAAAAKCKCG0BAAAAAKqI0BYAAAAAoIoIbQEAAAAAqojQFgAAAACgightAQAAAACqiNAWAAAAAKCKCG0BAAAAAKqI0BYAAAAAoIoIbQEAAAAAqojQFgAAAACgirQqd8dXXnkl11xzTV566aW8+eabKYqi3vZSqZTf/e53S1wgAAAAAEBzUlZo++tf/zqjR4/Ohx9+mM6dO6dTp04N+pRKpSUuDgAAAACguSkrtD3uuOOyzjrr5Oabb85nP/vZStcEAAAAANBslbWm7YwZM3LwwQcLbAEAAAAAKqys0HbQoEGZPHlypWsBAAAAAGj2ygptzz///Fx33XW5+eabK10PAAAAAECzVtaatuutt15+9KMfZa+99kr79u3Tp0+ftGzZsl6fUqmUf/3rXxUpEgAAAACguSgrtL3kkkty2GGHpW3btllrrbXSqVOnStcFAAAAANAslRXa/vjHP87gwYPzhz/8QWALAAAAAFBBZa1pO2vWrHzta18T2AIAAAAAVFhZoe2QIUPyxBNPVLoWAAAAAIBmr6zQ9uc//3nuu+++nH322XnjjTcqXRMAAAAAQLNVVmj7uc99LpMmTcpxxx2XHj16pH379unYsWO9L0snAAAAAAAsvrIuRLbbbrulVCpVuhYAAAAAgGavrND26quvrnAZAAAAAAAkZS6PAAAAAADA0lHWTNtrr722Sf323Xffcg4PAAAAANBslRXa7rfffgvc9vG1boW2AAAAAACLp6zQdtKkSQ3a5s2bl5dffjmXXHJJJk+enGuuuWaJiwMAAAAAaG7KCm379u3baPuaa66ZbbfdNiNGjMhFF12Uiy++eImKAwAAAABobpbKhch22mmn3HDDDUvj0AAAAAAAy7WlEtq++OKLmTt37tI4NAAAAADAcq2s0Pavf/1ro1+///3vc/TRR2fMmDHZYYcdyiro4osvzuqrr562bdtm0KBBefDBBxfa/6abbso666yTtm3bZr311ssdd9xRb3tRFDnppJPSq1evtGvXLkOHDs3zzz/f4Di33357Bg0alHbt2qVLly4ZOXJkWfUDAAAAACyJsta03XrrrVMqlRq0F0WRli1bZo899siFF1642Me94YYbctRRR+XSSy/NoEGDcv7552fYsGF57rnn0qNHjwb977///owaNSpnnnlmdtppp4wbNy4jR47MxIkTM2DAgCTJ2WefnTFjxuSaa67JGmuskRNPPDHDhg3L008/nbZt2yZJbrnllhx44IH58Y9/nG233TYffvhhnnzyycWuHwAAAABgSZWKoigWd6f77ruv4YFKpXTp0iV9+/ZNx44dyypm0KBB+eIXv5iLLrooSVJTU5NVV101hx12WH7wgx806L/nnnvm3XffzR/+8Ie6tk033TQbbLBBLr300hRFkd69e+d73/tejj766CTJrFmzsvLKK+fqq6/OXnvtlQ8//DCrr756Tj311HzjG98oq+4kmT17djp16pRZs2aVff6fNjU1NZk2bVp69OiRFi2WykobNCPGE5ViLFFJxhOVYixRScYTlVLtY+n+V2c2aBvcp+sC21m2qn088emyPI+npmaIZc20HTJkSNmFLcj777+fRx55JMcdd1xdW4sWLTJ06NBMmDCh0X0mTJiQo446ql7bsGHDcuuttyZJJk2alClTpmTo0KF12zt16pRBgwZlwoQJ2WuvvTJx4sS89tpradGiRb7whS9kypQp2WCDDXLOOefUzdZtzNy5c+ut2zt79uwkHw2qmpqaxT7/T6OampoURdFszpely3iiUowlKsl4olKMJSrJeKJSqn0sNTbHrLbmxtpZtqp9PPHpsjyPp6aeU1mh7dIwY8aMzJs3LyuvvHK99pVXXjnPPvtso/tMmTKl0f5Tpkyp217btqA+L730UpLklFNOyXnnnZfVV1895557brbeeuv8+9//Tteujf+27swzz8ypp57aoH369OmZM2fOok53uVBTU5NZs2alKIrl7rcefPKMJyrFWKKSjCcqxViikownKqXax9L7s95u0DZthQ8W2M6yVe3jiU+X5Xk8vf12w/ewxpQd2l533XW56qqr8tJLL+XNN99s8JuuUqmUWbNmlXv4T0xtuv3DH/4wu+22W5Jk7Nix6dOnT2666aZ861vfanS/4447rt4s39mzZ2fVVVdN9+7dm9XyCKVSKd27d1/uXkB88ownKsVYopKMJyrFWKKSjCcqpdrH0osfrNCgrUePLgtsZ9mq9vHEp8vyPJ5qr7G1KGWFtt///vfz05/+NKussko23njjdOrUqZzD1NOtW7e0bNkyU6dOrdc+derU9OzZs9F9evbsudD+tf9OnTo1vXr1qtdngw02SJK69s997nN129u0aZM111wzkydPXmC9bdq0SZs2bRq0t2jRYrkbTAtTKpWa3Tmz9BhPVIqxRCUZT1SKsUQlGU9USjWPpcYugN6iRYsFtrPsVfN44tNneR1PTT2fskLbyy+/PDvttFN++9vfVuyBa926dTbaaKOMHz8+I0eOTPJRqj5+/Pgceuihje6z2WabZfz48TnyyCPr2u66665sttlmSZI11lgjPXv2zPjx4+tC2tmzZ+eBBx7IIYcckiTZaKON0qZNmzz33HPZYostkiQffPBBXn755fTt27ci5wYAAAAA0FRlL4+w4447VjzpPuqoozJ69OhsvPHG2WSTTXL++efn3Xffzf77758k2XfffbPKKqvkzDPPTJIcccQRGTJkSM4999yMGDEi119/fR5++OFcdtllST5K5I888sicccYZ6devX9ZYY42ceOKJ6d27d10w3LFjxxx88ME5+eSTs+qqq6Zv374555xzkiR77LFHRc8PAAAAAGBRygptd9ppp/z9739f4Hqv5dpzzz0zffr0nHTSSZkyZUo22GCD3HnnnXUXEps8eXK9oHjw4MEZN25cTjjhhBx//PHp169fbr311gwYMKCuz7HHHpt33303Bx10UN56661sscUWufPOO+utH3HOOeekVatW2WefffK///0vgwYNyt13350uXayJAwAAAAB8skrF/FcQa4JZs2Zl5513zsCBA3PAAQdk1VVXTcuWLRv069q1a0WK/DSYPXt2OnXqlFmzZjWrC5FNmzYtPXr0WO7WF+GTZzxRKcYSlWQ8USnGEpVkPFEp1T6W7n91ZoO2wX26LrCdZavaxxOfLsvzeGpqhljWTNv27dtn8ODBOeecc/Lzn/98gf3mzZtXzuEBAAAAAJqtskLbQw89NJdffnk23XTTDBo0KJ06dap0XQAAAAAAzVJZoe0NN9yQffbZJ1dffXWFywEAAAAAaN7KWhRihRVWyKabblrpWgAAAAAAmr2yQtu99tort912W6VrAQAAAABo9spaHmHPPffMYYcdlhEjRuSAAw7IaqutlpYtWzbot+GGGy5xgQAAAAAAzUlZoe2WW26ZJHnsscdy5513NtheFEVKpVLmzZu3ZNUBAAAAADQzZYW2Y8eOrXQdAAAAAACkzNB29OjRla4DAAAAAICUeSEyAAAAAACWjibNtD3ggANSKpVy2WWXpWXLljnggAMWuU+pVMqVV165xAUCAAAAADQnTQpt77777rRo0SI1NTVp2bJl7r777pRKpYXus6jtAAAAAAA01KTQ9uWXX17obQAAAAAAKsOatgAAAAAAVaRJM20X5p133smbb76ZoigabFtttdWW9PAAAAAAAM1KWaHtnDlzcuqpp+bKK6/MG2+8scB+8+bNK7swAAAAAIDmqKzQ9tvf/nauueaajBw5MltuuWW6dOlS6boAAAAAAJqlskLb3/zmN/nmN7+ZX/ziF5WuBwAAAACgWSvrQmSlUikbbrhhpWsBAAAAAGj2ygptd9lll/zlL3+pdC0AAAAAAM1eWaHtiSeemJdeeikHHXRQHnnkkUyfPj0zZ85s8AUAAAAAwOIpa03bfv36JUkeffTRXHnllQvsN2/evPKqAgAAAABopsoKbU866aSUSqVK1wIAAAAA0OyVFdqecsopFS4DAAAAAICkzDVtAQAAAABYOpo00/a0005LqVTKD3/4w7Ro0SKnnXbaIvcplUo58cQTl7hAAAAAAIDmpEmh7SmnnJJSqZTvf//7ad26dZOWRxDaAgAAAAAsviaFtjU1NQu9DQAAAABAZVjTFgAAAACgightAQAAAACqSJOWR2jMddddl6uuuiovvfRS3nzzzRRFUW97qVTKrFmzlrhAAAAAAIDmpKzQ9vvf/35++tOfZpVVVsnGG2+cTp06VbouAAAAAIBmqazQ9vLLL89OO+2U3/72t2nRwgoLAAAAAACVUnbiuuOOOwpsAQAAAAAqrKzUdaeddsrf//73StcCAAAAANDslRXaXnjhhXnllVdy6KGHZuLEiZk+fXpmzpzZ4AsAAAAAgMVT1pq27du3z+DBg3POOefk5z//+QL7zZs3r+zCAAAAAACao7JC20MPPTSXX355Nt100wwaNCidOnWqdF0AAAAAAM1SWaHtDTfckH322SdXX311hcsBAAAAAGjeylrTdoUVVsimm25a6VoAAAAAAJq9skLbvfbaK7fddlulawEAAAAAaPbKWh5hzz33zGGHHZYRI0bkgAMOyGqrrZaWLVs26LfhhhsucYEAAAAAAM1JWaHtlltumSR57LHHcueddzbYXhRFSqVS5s2bt2TVAQAAAAA0M2WFtmPHjq10HQAAAAAApMzQdvTo0ZWuAwAAAACAlHkhMgAAAAAAlg6hLQAAAABAFRHaAgAAAABUEaEtAAAAAEAVKetCZAAAAACfBve/OrNB2+A+XZdBJQBNZ6YtAAAAAEAVKXum7bx58/KnP/0pL730Ut58880URVFve6lUyoknnrjEBQIAAAAANCdlhbYPP/xwdtttt7z66qsNwtpaQlsAAAAAgMVX1vII3/72t/O///0vt956a2bOnJmampoGX/Pmzat0rQAAAAAAy72yZto+/vjj+dGPfpSdd9650vUAAAAAADRrZc207dOnzwKXRQAAAAAAoHxlhbbf//73c/nll2f27NmVrgcAAAAAoFkra3mEt99+OyuttFLWXnvt7LXXXll11VXTsmXLen1KpVK++93vVqRIAAAAAIDmoqzQ9uijj677/0UXXdRoH6EtAAAAAMDiKyu0nTRpUqXrAAAAAAAgZYa2ffv2rXQdAAAAAACkzNC21rvvvpv77rsvr7zySpKPwtwhQ4akffv2FSkOAAAAAKC5KTu0vfDCC3PCCSfknXfeSVEUde0dOnTIj370oxx66KEVKRAAAAAAoDlpUc5O1157bY444ogMGDAg48aNy2OPPZbHHnssv/71r7PeeuvliCOOyC9/+ctK1woAAAAAsNwra6bteeedl6222irjx49Py5Yt69oHDhyY3XffPdttt13OPffc7LPPPhUrFAAAAACgOShrpu1zzz2XPfbYo15gW6tly5bZY4898txzzy1xcQAAAAAAzU1ZoW2nTp3y8ssvL3D7yy+/nI4dO5ZbEwAAAABAs1VWaDtixIhceOGFuf766xtsu+GGG3LRRRdl5513XuLiAAAAAACam7LWtP3JT36SCRMm5Gtf+1q+973vpV+/fkmS559/PlOmTMk666yTn/zkJxUtFAAAAACgOShrpm337t0zceLEnHfeeVlvvfUyderUTJ06Neutt15+9rOf5ZFHHkm3bt0qXSsAAAAAwHKvrJm2SdK2bdscccQROeKIIypZDwAAAABAs1bWTFsAAAAAAJaOJs203WabbdKiRYv86U9/SqtWrbLtttsucp9SqZTx48cvcYEAAAAAAM1Jk0LboihSU1NTd7umpialUmmR+wAAAAAAsHiaFNree++9C70NAAAAAEBlWNMWAAAAAKCKlBXaHnfccfnggw8WuH3KlCnZeeedyy4KAAAAAKC5Kiu0Peecc7LRRhvl0UcfbbDtuuuuy+c///n8/e9/X+LiAAAAAACam7JC23vvvTfvvfdeNt1005x66qmZN29epk2bll133TX77rtvNt544zzxxBOVrhUAAAAAYLnXpAuRzW+LLbbI448/nmOPPTann356fvOb3+T111/P3Llzc+mll+aggw6qdJ0AAAAAAM1CWaFtkqy44oo57bTT8tBDD+Whhx5KqVTKj370I4EtAAAAAMASKGt5hCT5wx/+kAEDBuSZZ57JOeeck+222y4//OEPs+eee+aNN96oZI0AAAAAAM1GWaHtfvvtl1122SVrr712HnvssXzve9/Ln//851x88cX54x//mM9//vP53e9+V+laAQAAAACWe2WFtjfeeGPOPvvs3HfffVlzzTXr2g8++OD861//yrrrrpuvfOUrFSsSAAAAAKC5KGtN24kTJ2adddZpdNsaa6yRe+65JxdeeOESFQYAAAAA0ByVNdN2/sB21qxZmTdvXr22ww47rPyqAAAAAACaqbIvRPbwww9n+PDhWXHFFfOZz3wm9913X5JkxowZ2WWXXXLvvfdWqkYAAAAAgGajrND2/vvvzxZbbJHnn38+X//611NTU1O3rVu3bpk1a1Z+8YtfVKxIAAAAAIDmoqzQ9vjjj8+6666bp59+Oj/+8Y8bbN9mm23ywAMPLHFxAAAAAADNTVmh7UMPPZT9998/bdq0SalUarB9lVVWyZQpU5a4OAAAAACA5qas0HaFFVaotyTC/F577bWstNJKZRcFAAAAANBclRXabrrpprn55psb3fbuu+9m7NixGTJkyBIVBgAAAADQHJUV2p566ql5+OGHM2LEiPzxj39MkvzrX//KFVdckY022ijTp0/PiSeeWNFCAQAAAACag1bl7DRo0KDccccdOeSQQ7LvvvsmSb73ve8lSdZaa63ccccdGThwYOWqBAAAAABoJsoKbZNk2223zXPPPZfHHnsszz//fGpqarLWWmtlo402avTiZAAAAAAALFrZoW2tDTbYIBtssEEFSgEAAAAAoEmh7V//+teyDr7VVluVtR8AAAAAQHPVpNB26623XqwlD4qiSKlUyrx588ouDAAAAACgOWpSaHvPPfcs7ToAAAAAAEgTQ9t//etfGT58eD772c8u7XoAAAAAAJq1Fk3p9N3vfjcPP/xw3e2WLVtm3LhxS60oAAAAAIDmqkmhbZcuXTJ16tS620VRLLWCAAAAAACasyZfiOyUU07JY489lk6dOiVJrr322vzzn/9c4D6lUikXXHBBZaoEAAAAAGgmmhTaXnLJJTnyyCPz5z//OdOmTUupVMqf//zn/PnPf17gPkJbAAAAAIDF16TlEXr06JFx48blv//9b+bNm5eiKHLdddelpqZmgV/z5s1b2rUDAAAAACx3mhTazm/s2LEZPHhwpWsBAAAAAGj2mrQ8wvxGjx6dJJk7d24mTpyYadOmZfPNN0+3bt0qWhwAAAAAQHNT1kzbJBkzZkx69eqVLbbYIl/5ylfy+OOPJ0lmzJiRbt265aqrrqpYkQAAAAAAzcUiQ9tf/epXefHFF+u1jR07NkceeWSGDx+eK6+8MkVR1G3r1q1btt1221x//fWVrxYAAAAAYDm3yND2zTffzODBg/PQQw/VtZ177rnZZZddMm7cuOy8884N9tloo43y1FNPVbZSAAAAAIBmYJFr2h566KEpiiLDhw/Pb37zmwwZMiQvvPBCDj/88AXu07Vr17zxxhsVLRQAAAAAoDlo0oXIDjvssGy66aZ55plnkiSdO3fOjBkzFtj/6aefTs+ePStTIQAAAMAi3P/qzAZtg/t0XQaVACy5Jl+I7Itf/GL23XffJMmOO+6Yyy67LG+99VaDfk899VQuv/zyfPnLX65YkQAAAAAAzUWTQ9uPO+OMMzJv3rwMGDAgJ5xwQkqlUq655pp8/etfz8Ybb5wePXrkpJNOqnStAAAAAADLvbJC2969e+eRRx7J8OHDc8MNN6Qoivzyl7/MbbfdllGjRuWf//xnunXrVulaAQAAAACWe01a07YxPXr0yBVXXJErrrgi06dPT01NTbp3754WLcrKgQEAAAAAyBKEth/XvXv3ShwGAAAAAKDZa1Joe9pppy32gUulUk488cTF3g8AAAAAoDlrUmh7yimnNGgrlUpJkqIoGrQXRSG0BQAAAAAoQ5MWoK2pqan39Z///CfrrbdeRo0alQcffDCzZs3KrFmz8sADD2SvvfbK+uuvn//85z9Lu3YAAAAAgOVOWWvafuc730m/fv1y3XXX1Wv/4he/mF/96lfZfffd853vfCe//e1vK1IkAAAAAFSr+1+dWe/24D5dl1ElLC+aNNN2fnfffXe23XbbBW7fbrvtMn78+LKLAgAAAABorsoKbdu2bZsJEyYscPv999+ftm3bll0UAAAAAEBzVVZo+7WvfS2/+tWvcvjhh+f555+vW+v2+eefz2GHHZZx48bla1/7WqVrBQAAAABY7pW1pu1ZZ52VGTNm5KKLLsrFF1+cFi0+yn5rampSFEVGjRqVs846q6KFAgAAAAA0B2WFtq1bt84vf/nLHHPMMbnjjjvyyiuvJEn69u2bHXbYIeuvv35FiwQAAAAAaC7KCm1rDRw4MAMHDqxULQAAAAAAzV5Za9oCAAAAALB0CG0BAAAAAKqI0BYAAAAAoIos0Zq2AAAAAMuT+1+d2aBtcJ+uy6ASoDkz0xYAAAAAoIos0UzbuXPnZuLEiZk2bVo233zzdOvWrVJ1AQAAAAA0S2XPtB0zZkx69eqVLbbYIl/5ylfy+OOPJ0lmzJiRbt265aqrrqpYkQAAAAAAzUVZoe3YsWNz5JFHZvjw4bnyyitTFEXdtm7dumXbbbfN9ddfX7EiAQAAAACai7JC23PPPTe77LJLxo0bl5133rnB9o022ihPPfXUEhcHAAAAANDclBXavvDCC9lhhx0WuL1r16554403yi4KAAAAAKC5Kiu07dy5c2bMmLHA7U8//XR69uxZdlEAAAAAAM1VWaHtjjvumMsuuyxvvfVWg21PPfVULr/88nz5y19e0toAAAAAAJqdskLbM844I/PmzcuAAQNywgknpFQq5ZprrsnXv/71bLzxxunRo0dOOumkStcKAAAAALDcKyu07d27dx555JEMHz48N9xwQ4qiyC9/+cvcdtttGTVqVP75z3+mW7duZRd18cUXZ/XVV0/btm0zaNCgPPjggwvtf9NNN2WdddZJ27Zts9566+WOO+6ot70oipx00knp1atX2rVrl6FDh+b5559v9Fhz587NBhtskFKplMcee6zscwAAAAAAKEdZoW2S9OjRI1dccUVmzpyZqVOn5r///W/efPPNXHXVVenRo0fZBd1www056qijcvLJJ2fixIlZf/31M2zYsEybNq3R/vfff39GjRqVb3zjG3n00UczcuTIjBw5Mk8++WRdn7PPPjtjxozJpZdemgceeCDt27fPsGHDMmfOnAbHO/bYY9O7d++y6wcAAAAAWBJlh7Yf171796y88spp0WLJD3feeeflwAMPzP7775/Pfe5zufTSS7PiiivmqquuarT/BRdckOHDh+eYY47Juuuum9NPPz0bbrhhLrrooiQfzbI9//zzc8IJJ2SXXXbJwIEDc+211+b111/PrbfeWu9Yf/zjH/PnP/85P/3pT5f4PAAAAAAAytGqKZ1OO+20xT5wqVTKiSeeuFj7vP/++3nkkUdy3HHH1bW1aNEiQ4cOzYQJExrdZ8KECTnqqKPqtQ0bNqwukJ00aVKmTJmSoUOH1m3v1KlTBg0alAkTJmSvvfZKkkydOjUHHnhgbr311qy44oqLVTcAAAAAQKU0KbQ95ZRTGrSVSqUkH81knb+9KIqyQtsZM2Zk3rx5WXnlleu1r7zyynn22Wcb3WfKlCmN9p8yZUrd9tq2BfUpiiL77bdfDj744Gy88cZ5+eWXF1nr3LlzM3fu3Lrbs2fPTpLU1NSkpqZmkfsvD2pqalIURbM5X5aOCa+9meSj1+H7s2bnhfdbZXCfrsu4Kj7NvDdRScYTlWIsUUnGE5VS7WNp/rwh+b+aK9G+uPfLwlX7eFra5h83zfVxqJTleTw19ZyaFNrOf7DXXnstI0aMyIABA3LkkUemf//+SZJnn302559/fp5++uncfvvti1nysnPhhRfm7bffrjfDd1HOPPPMnHrqqQ3ap0+f3uhaucujmpqazJo1K0VRVGRpDJqn92e9neSjD7gP33snSTKt9YfLsiQ+5bw3UUnGE5ViLFFJxhOVUu1jqfZnhY+btsIHFWtf3Ptl4ap9PC1t848bY2bJLM/j6e23G77HNKZJoe38vvOd76Rfv3657rrr6rV/8YtfzK9+9avsvvvu+c53vpPf/va3i3Xcbt26pWXLlpk6dWq99qlTp6Znz56N7tOzZ8+F9q/9d+rUqenVq1e9PhtssEGS5O67786ECRPSpk2besfZeOON87WvfS3XXHNNg/s97rjj6i3LMHv27Ky66qrp3r17Onbs2MQz/nSrqalJqVRK9+7dl7sXEJ+cFz9YIcn//Vaydaeu6dHDTFvK572JSjKeqBRjiUoynqiUah9LtT8rfFyPHl0q1r6498vCVft4WtrmHzfGzJJZnsdT27Ztm9SvrND27rvvzllnnbXA7dttt12+//3vL/ZxW7dunY022ijjx4/PyJEjk3z0JI0fPz6HHnpoo/tsttlmGT9+fI488si6trvuuiubbbZZkmSNNdZIz549M378+LqQdvbs2XnggQdyyCGHJEnGjBmTM844o27/119/PcOGDcsNN9yQQYMGNXq/bdq0aRDyJh+twbu8DaaFKZVKze6cqazapVZq/187pmBJeG+ikownKsVYopKMJyqlmsfSx39WqNWiRYuKtS/u/bJo1Tyelrb5x01zfAwqbXkdT009n7JC27Zt22bChAl1oef87r///ianxvM76qijMnr06Gy88cbZZJNNcv755+fdd9/N/vvvnyTZd999s8oqq+TMM89MkhxxxBEZMmRIzj333IwYMSLXX399Hn744Vx22WVJPnqCjzzyyJxxxhnp169f1lhjjZx44onp3bt3XTC82mqr1athpZVWSpKstdZa6dOnT1nnAQAAAABQjrJC26997WsZM2ZMOnfunMMOOyxrrbVWkuTFF1/MmDFjMm7cuBx++OFlFbTnnntm+vTpOemkkzJlypRssMEGufPOO+suJDZ58uR6ifTgwYMzbty4nHDCCTn++OPTr1+/3HrrrRkwYEBdn2OPPTbvvvtuDjrooLz11lvZYostcuedd5YdLAMAAAAALC1lhbZnnXVWZsyYkYsuuigXX3xxXYhae2W3UaNGLXT5hEU59NBDF7gcwr333tugbY899sgee+yxwOOVSqWcdtppOe2005p0/6uvvnqjV4sEAAAAAFjaygptW7dunV/+8pc55phjcscdd+SVV15JkvTt2zc77LBD1l9//YoWCQAAAADQXJQV2tYaOHBgBg4cWKlaAAAAAACaveXr8msAAAAAAJ9yQlsAAAAAgCoitAUAAAAAqCJCWwAAAACAKiK0BQAAAACoImWFtmuuuWZ+//vfL3D7H/7wh6y55pplFwUAAAAA0FyVFdq+/PLLeeeddxa4/Z133skrr7xSdlEAAAAAAM1V2csjlEqlBW576KGH0rlz53IPDQAAAADQbLVqascLLrggF1xwQZKPAtsjjzwyP/zhDxv0mzVrVt56663svffelasSAAAAAKCZaHJo26NHj3z+859P8tHyCKusskpWWWWVen1KpVLat2+fjTbaKN/+9rcrWykAAAAAQDPQ5NB21KhRGTVqVJJkm222yQknnJDttttuqRUGAAAAANAcNTm0/bh77rmn0nUAAAAAAJAyQ9taTz/9dF566aW8+eabKYqiwfZ99913SQ4PAAAAANDslBXavvjii/n617+eBx98sNGwNvlofVuhLQAAAADA4ikrtP3Wt76VJ554Iueff3623HLLdOnSpdJ1AQAAAAA0S2WFtv/4xz9y/PHH57DDDqt0PQAAAAAAzVqLcnbq1q1bOnXqVOlaAAAAAACavbJC24MPPjjXXXdd5s2bV+l6AAAAAACatbKWR/jsZz+befPmZf31188BBxyQVVddNS1btmzQ7ytf+coSFwgAAAAA0JyUFdruueeedf8/+uijG+1TKpXMxAUAAIDlwP2vzmzQNrhP12VQCUDzUFZoe88991S6DgAAAAAAUmZoO2TIkErXAQAAAABAygxta82dOzcTJ07MtGnTsvnmm6dbt26VqgsAAAAAoFlqUe6OY8aMSa9evbLFFlvkK1/5Sh5//PEkyYwZM9KtW7dcddVVFSsSAAAAAKC5KCu0HTt2bI488sgMHz48V155ZYqiqNvWrVu3bLvttrn++usrViQAAAAAQHNRVmh77rnnZpdddsm4ceOy8847N9i+0UYb5amnnlri4gAAAAAAmpuyQtsXXnghO+ywwwK3d+3aNW+88UbZRQEAAAAANFdlhbadO3fOjBkzFrj96aefTs+ePcsuCgAAAACguSortN1xxx1z2WWX5a233mqw7amnnsrll1+eL3/5y0taGwAAANAM3P/qzHpfAM1dWaHtGWeckXnz5mXAgAE54YQTUiqVcs011+TrX/96Nt544/To0SMnnXRSpWsFAAAAAFjulRXa9u7dO4888kiGDx+eG264IUVR5Je//GVuu+22jBo1Kv/85z/TrVu3StcKAAAAALDca1Xujj169MgVV1yRK664ItOnT09NTU26d++eFi3KyoEBAAAAAMgShLYf171790ocBgAAAACg2WtSaHvaaaelVCrlhz/8YVq0aJHTTjttkfuUSqWceOKJS1wgAAAAAEBz0qTQ9pRTTkmpVMr3v//9tG7dOqeccsoi9xHaAgAAAAAsviaFtjU1NQu9DQAAAADLu/tfnVnv9uA+XZdRJSzvXDUMAAAAAKCKlBXaTpo0KbfddtsCt9922215+eWXy60JAAAAAKDZatLyCPM7+uijM3v27Oy8886Nbr/44ovTuXPnXH/99UtUHAAAAABAc1PWTNsJEybkS1/60gK3b7fddvnb3/5WdlEAAAAAAM1VWaHtm2++mQ4dOixw+0orrZQ33nij7KIAAAAAAJqrskLb1VZbLf/4xz8WuP1vf/tb+vTpU3ZRAAAAAADNVVmh7ahRo/LrX/86Y8aMSU1NTV37vHnzcsEFF+SGG27I3nvvXbEiAQAAAACai7IuRHbcccfl73//e4488sj86Ec/Sv/+/ZMkzz33XKZPn56tt946P/zhDytaKAAAAABAc1DWTNs2bdrkz3/+c6688spssskmmTFjRmbMmJFNNtkkV111Vf7yl7+kTZs2la4VAAAAAGC5V9ZM2yRp0aJF9t9//+y///6VrAcAAAAAoFkrO7QFuP/VmfVuD+7TdRlVAgAAALD8aFJou80226RFixb505/+lFatWmXbbbdd5D6lUinjx49f4gIBAAAAAJqTJoW2RVGkpqam7nZNTU1KpdIi9wEo1/yzeBMzeQEAAD5N/HUmlK9Joe2999670NsAAAAAAFRGi6Z06tq1a2655Za626eddlqefPLJpVYUAAAAAEBz1aTQ9p133sm7775bd/uUU07J448/vtSKAgAAAABorpq0PMJaa62Vm2++OVtuuWU6duyYJHn33Xczc2bDNSc/rmtXa5UAAAAAACyOJoW2xx9/fPbff//cfvvtSZJSqZSDDz44Bx988EL3mzdv3pJXCAAAAADQjDQptN1nn32yySab5N57783UqVNzyimnZNddd83AgQOXdn0AAAAAAM1Kk0Lb2bNnZ+21107//v2TJGPHjs3o0aPz5S9/eakWBwAAAADQ3DTpQmRdunTJDTfcUHd76623zsorr7zUigIAAAAAaK6aFNq2bt06c+fOrbt97bXX5sUXX1xqRQEAAAAANFdNWh5hnXXWyRVXXJHVV189nTp1SlEUefnllzNx4sSF7rfhhhtWpEgAAAAAgOaiSaHtmWeemT333DNDhw5NkpRKpZx44ok58cQTG+1fFEVKpVLmzZtXuUoBAAAAAJqBJoW2w4cPz6RJk/LQQw9l6tSp2W+//XLQQQdls802W9r1AQAAAAA0K00KbZOka9euGTZsWJJk7Nix2WOPPbLddtsttcIAAAAAAJqjJoe2H3fPPfdUug4AAAAAAJK0KHfHyZMn5+CDD07//v3TtWvX/PWvf02SzJgxI4cffngeffTRihUJAAAAwPLr/ldnNviC5qysmbZPP/10ttxyy9TU1GTQoEF54YUX8uGHHyZJunXrlr///e959913c+WVV1a0WAAAAACA5V1Zoe2xxx6bzp0755///GdKpVJ69OhRb/uIESNyww03VKRAAAAAgEprbCbn4D5dl0ElAA2VtTzCX//61xxyyCHp3r17SqVSg+2rrbZaXnvttSUuDgAAAACguSkrtK2pqcmKK664wO3Tp09PmzZtyi4KAAAAAKC5Kiu03XDDDXP77bc3uu3DDz/M9ddfn0033XSJCgOASnFBAwAAAD5NygptjzvuuNx555055JBD8uSTTyZJpk6dmr/85S/Zfvvt88wzz+QHP/hBRQsFAAAAAGgOyroQ2Q477JCrr746RxxxRC677LIkyde//vUURZGOHTvm2muvzVZbbVXRQgEAAAAAmoOyQtsk2WefffKVr3wlf/7zn/PCCy+kpqYma621VoYNG5YOHTpUskYAAAAAgGaj7NA2Sdq3b59dd921UrUAAAAAADR7SxTa3nfffbn99tvzyiuvJEn69u2bESNGZMiQIRUpDgAAAACguSkrtH3//fczatSo3HrrrSmKIp07d06SvPXWWzn33HOz66675te//nVWWGGFStYKAAAAALDca1HOTqeeemp++9vf5nvf+17++9//ZubMmZk5c2amTJmSo48+Or/5zW9y2mmnVbpWAAAAAIDlXlmh7bhx4zJ69OicffbZWXnllevae/TokbPOOiv77rtvfvnLX1asSAAAAACA5qKs0Pa///1vBg0atMDtgwYNypQpU8ouCgAAAACguSortO3Tp0/uvffeBW6/77770qdPn3JrAgAAAABotsq6ENno0aNz8sknp3Pnzvnud7+btddeO6VSKc8//3zOP//83HTTTTn11FMrXSvwKXD/qzMbtA3u03UZVAIAAADw6VRWaHv88cfnxRdfzGWXXZbLL788LVp8NGG3pqYmRVFk9OjROf744ytaKAAAAABAc1BWaNuyZctcffXVOeqoo3LHHXfklVdeSZL07ds3O+64YwYOHFjRIgEAAAAAmouyQttaAwcOFNACAAAAAFRQky9ENmfOnBx88MG58MILF9pvzJgxOeSQQ/LBBx8scXEAAAAAAM1Nk0Pbyy67LFdffXVGjBix0H4jRozI2LFjc8UVVyxxcQAAAAAAzU2TQ9sbb7wxu+22W9Zcc82F9ltrrbWyxx575Ne//vUSFwcAAAAA0Nw0ObR94oknssUWWzSp7+DBg/P444+XXRQAAAAAQHPV5ND2/fffT+vWrZvUt3Xr1pk7d27ZRQEAAAAANFetmtqxd+/eefLJJ5vU98knn0zv3r3LLgpgcd3/6swGbYP7dF0GlQAAAAAsmSbPtB06dGiuvfbaTJs2baH9pk2blmuvvTZf+tKXlrg4AIBKuf/VmfW+AAAAqlWTQ9vvf//7mTNnTrbddts88MADjfZ54IEHst1222XOnDk55phjKlYkAAAAAEBz0eTlEdZcc83ceOONGTVqVAYPHpw111wz6623Xjp06JC33347Tz75ZF588cWsuOKKuf7667PWWmstzboBAAAAAJZLTQ5tk2TEiBF5/PHHc9ZZZ+UPf/hDbr311rptvXv3zoEHHphjjz02a665ZqXrBAAAAABoFhYrtE2S1VdfPT//+c/z85//PG+//XZmz56djh07pkOHDkujPgAAAACAZmWxQ9uP69Chg7AWAAAAAKCCmnwhMgAAAAAAlj6hLQAAAABAFRHaAgAAAABUEaEtAAAAAEAVEdoCAAAAAFQRoS0AAAAAQBUR2gIAAAAAVJFWy7oAAAD4NLn/1ZkN2gb36boMKgEAYHkltAUAAAAo0/y/zPOLPKASLI8AAAAAAFBFhLYAAAAAAFVEaAsAAAAAUEWEtgAAAAAAVURoCwAAAABQRYS2AAAAAABVRGgLAAAAAFBFhLYAAAAAAFVEaAsAAAAAUEWEtgAAAAAAVURoCwAAAABQRYS2AAAAAABVRGgLAAAAAFBFhLYAAAAAAFVEaAsAAAAAUEWEtgAAAAAAVURoCwAAAABQRYS2AAAAAABVpNWyLgAAAAAAWDbuf3Vmg7bBfboug0r4ODNtAQAAAACqiNAWAAAAAKCKCG0BAAAAAKqI0BYAAAAAoIoIbQEAAAAAqojQFgAAAACgightAQAAAACqiNAWAAAAAKCKtFrWBQAAAADAMjF5cjJjRv22bt2S1VZbNvXA/ye0BQAAAKD5mTw56d8/mTOnfnvbtslzzwluWaYsjwAAAABA8zNjRsPANvmobf7Zt/AJE9oCAAAAAFQRoS0AAAAAQBUR2gIAAAAAVBGhLQAAAABAFRHaAgAAAABUkaoMbS+++OKsvvrqadu2bQYNGpQHH3xwof1vuummrLPOOmnbtm3WW2+93HHHHfW2F0WRk046Kb169Uq7du0ydOjQPP/883XbX3755XzjG9/IGmuskXbt2mWttdbKySefnPfff3+pnB8AAJ8CkycnEyfW/5o8eVlXBQBAM1B1oe0NN9yQo446KieffHImTpyY9ddfP8OGDcu0adMa7X///fdn1KhR+cY3vpFHH300I0eOzMiRI/Pkk0/W9Tn77LMzZsyYXHrppXnggQfSvn37DBs2LHPmzEmSPPvss6mpqckvfvGLPPXUU/nZz36WSy+9NMcff/wncs4AAFSZyZOT/v2TjTaq/9W/f1q/9uqyrg4AgOVc1YW25513Xg488MDsv//++dznPpdLL700K664Yq666qpG+19wwQUZPnx4jjnmmKy77ro5/fTTs+GGG+aiiy5K8tEs2/PPPz8nnHBCdtlllwwcODDXXnttXn/99dx6661JkuHDh2fs2LHZfvvts+aaa+bLX/5yjj766PzmN7/5pE4bAIBqMmNG8v9/wV/PnDlZYeYbn3w9AAA0K1UV2r7//vt55JFHMnTo0Lq2Fi1aZOjQoZkwYUKj+0yYMKFe/yQZNmxYXf9JkyZlypQp9fp06tQpgwYNWuAxk2TWrFnp2rXrkpwOAAAAAMBia7WsC/i4GTNmZN68eVl55ZXrta+88sp59tlnG91nypQpjfafMmVK3fbatgX1md8LL7yQCy+8MD/96U8XWOvcuXMzd+7cutuzZ89OktTU1KSmpmaB+y1PampqUhRFszlfGiqKot7t2jExv4WNkdr+RVHUfS3JcZran+VXY+9NjY1Vmp9yxoHPumasKJIWjc9vqP28+rhFjRFjiUoynqiUxRlLS/v77cX52WJZtS9O7c3Rkn4fvkx+plvI532KImnk/hd0Tp/mcVCNP08vz591TT2nqgptq8Frr72W4cOHZ4899siBBx64wH5nnnlmTj311Abt06dPr1srd3lXU1OTWbNmpSiKtFjQmxzLtfdnvV3v9rQVPmjQVtu+qGMURZEP33vno/6tPyz7OE3tz/KrsfemxsYqzU8548BnXTM2Z85Ha9g24n/Fh3l/1sx6bYsaT8YSlWQ8USmLM5aW9vfbi/OzxbJqX5zam6Ml/T58mfxMt5DP+8yZkzRyfaUFndOneRxU48/Ty/Nn3dtvN3y8G1NVoW23bt3SsmXLTJ06tV771KlT07Nnz0b36dmz50L71/47derU9OrVq16fDTbYoN5+r7/+erbZZpsMHjw4l1122UJrPe6443LUUUfV3Z49e3ZWXXXVdO/ePR07dlz4iS4nampqUiqV0r179+XuBUTTvPjBCvVu9+jRpUFbbfuijlH7m73WnbqmR4+uZR+nqf1ZfjX23tTYWKX5KWcc+Kxrxl57LXnkkUY3TSm1Sk2n+stoLWo8GUtUkvFEpSzOWFra328vzs8Wy6p9cWpvjpb0+/Bl8jPdQj7v07Zt0qNHg+YFndOneRxU48/Ty/NnXdu2bZvUr6pC29atW2ejjTbK+PHjM3LkyCQfPUnjx4/PoYce2ug+m222WcaPH58jjzyyru2uu+7KZpttliRZY4010rNnz4wfP74upJ09e3YeeOCBHHLIIXX7vPbaa9lmm22y0UYbZezYsYscEG3atEmbNm0atLdo0WK5G0wLUyqVmt05839KpVK92y1atGjQVtvelGOUSqW6MbUkx2lKf5Zv8783NTZWaX7KHQc+65qpUqnRP4n8aFOprPFkLFFJxhOV0tSxtLS/316cny2WVfvi1N5cLcn34cvkZ7qFfN6nVGp06YQFndOneRxU68/Ty+tnXVPPp6pC2yQ56qijMnr06Gy88cbZZJNNcv755+fdd9/N/vvvnyTZd999s8oqq+TMM89MkhxxxBEZMmRIzj333IwYMSLXX399Hn744bqZsqVSKUceeWTOOOOM9OvXL2ussUZOPPHE9O7duy4Yfu2117L11lunb9+++elPf5rp06fX1bOgGb4AAAAAAEtD1YW2e+65Z6ZPn56TTjopU6ZMyQYbbJA777yz7kJikydPrpdIDx48OOPGjcsJJ5yQ448/Pv369cutt96aAQMG1PU59thj8+677+aggw7KW2+9lS222CJ33nln3XTku+66Ky+88EJeeOGF9OnTp149jS3GDAAAAACwtFRdaJskhx566AKXQ7j33nsbtO2xxx7ZY489Fni8UqmU0047Laeddlqj2/fbb7/st99+5ZQKAAAAAFBRy9eiEAAAAAAAn3JCWwAAAACAKiK0BQAAAACoIkJbAAAAAIAqIrQFAAAAAKgiQlsAAAAAgCoitAUAAAAAqCJCWwAAAACAKiK0BQAAAACoIkJbAAAAAIAqIrQFAAAAAKgiQlsAAAAAgCoitAUAAAAAqCJCWwAAAACAKiK0BQAAAACoIq2WdQEAAAAAUE3ub9MleXVm3e3Bfbouw2pojsy0BQAAAACoIkJbAAAAAIAqIrQFAAAAAKgiQlsAAAAAgCoitAUAAAAAqCJCWwAAAACAKtJqWRcAAMvM5MnJjBn/d7tbt2S11ZZdPQAAABChLQDN1eTJSf/+yZw5/9fWtm3y3HOCWwAAAJYpyyMA0DzNmFE/sE0+uv3xmbcAAACwDJhpCwAAy8L8S7QklmkBACCJ0BYAAD55jS3RklimBQCAJJZHAACAT15jS7QklmkBACCJmbYAAAAAfMrc/+rMBm2D+3RdBpXA0mGmLQAAAABAFRHaAgAAAABUEaEtAAAAAEAVEdoCAAAAAFQRoS0AAAAAQBUR2gIAAAAAVBGhLQAAAABAFWm1rAsA+FSYPDmZMeP/bnfrlqy22rKrBwAAAFhuCW0BFmXy5KR//2TOnP9ra9s2ee45wS0AAABQcZZHAFiUGTPqB7bJR7c/PvMWAAAAoEKEtgAAAAAAVURoCwAAAABQRYS2AAAAAABVRGgLAAAAAFBFhLYAAAAAAFVEaAsAAAAAUEWEtgAAAAAAVURoCwAAAABQRYS2AAAAAABVRGgLAAAAAFBFhLYAAAAAAFVEaAsAAAAAUEVaLesCAAAAYEnc/+rMBm2D+3RdBpUAQGWYaQsAAAAAUEXMtAUA6jFbCQAAYNky0xYAAAAAoIoIbQEAAAAAqojQFgAAAACgightAQAAAACqiNAWAAAAAKCKCG0BAAAAAKqI0BYAAAAAoIoIbQEAAAAAqojQFgAAAACgightAQAAAACqiNAWAAAAAKCKtFrWBQAAAADA8ub+V2c2aBvcp+syqIRPIzNtAQAAAACqiJm2AADAUmWmEQDA4jHTFgAAAACgightAQAAAACqiOURAAAAWC5ZmgOATyszbQEAAAAAqojQFgAAAACgightAQAAAACqiNAWAAAAAKCKuBAZwBKY/+IWLmwBAAAALCmhLQAAAADQJCYvfTKEtsBybf4Pk8QHCgAAAFDdrGkLAAAAAFBFhLYAAAAAAFXE8ggAAFXOUi8AANC8mGkLAAAAAFBFhLYAAAAAAFVEaAsAAAAAUEWsaQsAAM2E9ZEBAD4dzLQFAAAAAKgiZtoCAEuFGX1AueZ///DeAQA0N0JbAACoIve36ZIILQEAmjXLIwAAAAAAVBGhLQAAAABAFbE8AgAAACynltUa89amBlgyZtoCAAAAAFQRM22hGXJFdwAAaN7uf3VmiqLI+7PezosfrJBSqeRnAoAqYqYtAAAAAEAVMdMWAJaAmesAAABUmtAWAABYLH5hBQCwdAltgU+EH+4AWN75rAPqTJ6czJhRv61bt2S11ZZNPVAh83/W+ZyDpUdoCwAAAJUyeXLSv38yZ0799rZtk+eeE9wC0CQuRAYAAACVMmNGw8A2+aht/tm3ALAAZtoCi+RPYAAAAAA+OUJbgOWEtRQ/HZb282QcAAAs/3zPB8s/yyMAAAAAAFQRM20BAIBGmckFwPKi0c+0ZVAHNJWZtgAAAAAAVcRMW4DlnFlSAAAA8OkitAUAAJoNv8wEAD4NLI8AAAAAAFBFhLYAAAAAAFXE8ggALDfuf3VmiqLI+7PezosfrJBSqeRPXgEAAPjUMdMWAAAAAKCKmGkLAAAAQB0XbYRlT2gLAAAAANQzf3gvuP9kCW0BAJYzvsEGAIBPN2vaAgAAAABUEaEtAAAAAEAVEdoCAAAAAFQRa9oCdVwhFAAAAGDZE9oCAAAAUJVMLqK5EtoCQDM2/zfBvgEGaBohwqeb5w+AamdNWwAAAACAKmKmLQDw/9q787ga0/9/4K9zWk6FNpVKSspSlpiIshSasmvGWGfIPh/hY/kNYxaSLfu+ZMwIM5YZM5hhKFtZE6JhyC4hCUkpref6/eHb+ThaVE5Oh9fz8ejBua7r3Pf7Pvd1rnPO+1znut+thATg8WPlMjMzwNZWPfEQEWcdEhFVUhyfiT5cTNoSERHRu5OQANSvD2RlKZfr6QFXrzJxS0SVDhMmREREpA5M2pJKcW1EIiIq0ePHhRO2wMuyx4+ZtCWiEvG9JlHx+AUDEdH7hUlbIiIiIqr0mKwjIiIiog8Jk7ZEREREREQa4uS9FAghkPMsHTdzddC6VnV1h/Re4WxVIiKqLJi0JSL6QKlj1pomfxDS5NiJPkScmUtEqsT3AURE9K4xaUtERJUWPyBReTBZR0REREREmo5JWyIVY7KAiIiIiIiIiIjeBpO2REREGoizkImIiNSvyNdjNcRBVF58T0lUeTFpS0REpcJZ5EQVjx+c3k+vXzhKIpEwqUNEREREJWLSloiIiN4KE41EpG4f+heLHIeJiIjeP0zaEhERVQYJCcDjx8plZmaAra164iEiIiIiIiK1YdKWiIioApRp1lNCAlC/PpCVpVyupwdcvcrELRER0QeIM6g/LDzfRPQ6Jm2JiDQM39C9hx4/LpywBV6WPX7MpC1VKI4pRFReHD+IiIgqDpO2RO8xvpEmInqzD30tTOLV30m1+P6LiEi9OA7T+4JJWyIiog8AE5NERCXjh3wiUjW+/yJ14Wva+4FJWyIiUjm+SSAiIiIiIiIqPyZtiYhI7ZjkJSIiIiKqOBX9fvvkvRQIIZDzLB03c3XQulZ1lW2b6EPFpC0RERFVCidlJgB/Rkj0QeGXdurBx52IiKjyY9KWiOgVlWndKX6gIiJNUtyYVZnGssoUC3142P+IiIioLJi0JSJSM36IKzs+ZkREREREpKn4eYaPQWlI1R1AUVatWoXatWtDT08PLVu2xOnTp0tsv337djRo0AB6enpo3Lgx9u7dq1QvhMC0adNgZWUFfX19eHt74/r160ptUlJS8Pnnn8PQ0BDGxsYYNmwYnj9/rvJjI6LK4eS9lEJ/REREmoivZ5qN54+IiN4XfE1TrUqXtP31118xceJEBAYG4ty5c3BxcYGvry+Sk5OLbH/y5En0798fw4YNw/nz5+Hn5wc/Pz/8+++/ijbz58/H8uXLERISgujoaFSpUgW+vr7IyspStPn8889x6dIlHDhwAHv27MHRo0cxcuTICj/eDwWfuERE5XNSZsIxlIiIiIiICuFkpPdbpVseYfHixRgxYgSGDBkCAAgJCcHff/+N9evXY8qUKYXaL1u2DJ06dcKkSZMAADNnzsSBAwewcuVKhISEQAiBpUuX4vvvv0fPnj0BAJs2bUKNGjWwa9cu9OvXD3FxcQgLC8OZM2fQvHlzAMCKFSvQpUsXLFy4ENbW1u/o6DUHrwxJVIKEBODxY+UyMzPA1lY98VSg9/UnLZVpbWPSbBW9zmtF9tX39flNpCn4WkRqU9x7WWlV9cRDZaMB54/vMYhKp1IlbXNychATE4NvvvlGUSaVSuHt7Y2oqKgi7xMVFYWJEycqlfn6+mLXrl0AgNu3byMpKQne3t6KeiMjI7Rs2RJRUVHo168foqKiYGxsrEjYAoC3tzekUimio6PxySefFNpvdnY2srOzFbefPXsGAEhNTYVcLi/7wWuY52lpL5O26enQlWgjNVVLUf6q1FRpieVlcToxtVCZm7Ux8PAhkJSkXGFpCdSoUeZ9lGa/btbGJbYv07EWE/vpfFmhpm/ab2liKYjnbctLOq/l3fbr/am49sWp0GNNTwckksI7TU/Hc73X2v+bCLi5Aa/M5AcA6OkBp08DNjbvLPaylise39f75f89n1R5viuy/NW+JJFIVHNeKzr24mIB8Dw9rVCdup7HxanoYy3rOF/W16KSxm25XI60tDTo6upCKi37a5e6nsdleS0q6z6LOx/Pi3jtKk9/Ku71vqzvJd56zCqhr6ZnPEdmGbdfaGwqx3OhuPckRR1raR/H0sRelnJV9Q9VPI/V9RhU9PO4qPfhqhq3i1XG81qcYt9XF/Peoywqy/kDSh4/kJ4OpKYWXfead/E8KzQ2lfBeNnv3AeRa1SwcTynjL2hblrGsUp3XchxrRb4eF1lejvOnyjGrtDmCSnVey/B6XJ7XIlX1seJUVB8ubjtv8766tPss8LbvwyuztFfyICWpVEnbx48fIz8/HzVee4GuUaMGrly5UuR9kpKSimyf9H8vYgX/vqmNhYWFUr22tjZMTU0VbV4XHByMoKCgQuV2dnbFHR4RvW+8vErf9sULoHHjCguFVKgs5/VdcG+q7gjenQ/pWEmz9e5Rsdvnc4Ho/VXZ3meU1osXgHcbdUdB5cXzVz58PaYKlp6eDiMjo2LrK1XSVpN88803SjN85XI5UlJSUL16dUiK+1b1PZOWloZatWrh7t27MDQ0VHc4pOHYn0hV2JdIldifSFXYl0iV2J9IVdiXSJXYn0iV3uf+JIRAenr6G5djrVRJWzMzM2hpaeHhw4dK5Q8fPoSlpWWR97G0tCyxfcG/Dx8+hJWVlVKbpk2bKtq8fqGzvLw8pKSkFLtfmUwGmUz55yfGxsYlH+B7ytDQ8L17ApH6sD+RqrAvkSqxP5GqsC+RKrE/kaqwL5EqsT+RKr2v/amkGbYFKtWiELq6unB1dcWhQ4cUZXK5HIcOHYK7u3uR93F3d1dqDwAHDhxQtLe3t4elpaVSm7S0NERHRyvauLu7IzU1FTExMYo2hw8fhlwuR8uWLVV2fERERERERERERERvUqlm2gLAxIkT4e/vj+bNm8PNzQ1Lly5FRkYGhgwZAgAYNGgQatasieDgYADAuHHj4OnpiUWLFqFr167Ytm0bzp49ix9++AEAIJFIMH78eMyaNQt169aFvb09pk6dCmtra/j5+QEAnJyc0KlTJ4wYMQIhISHIzc3FmDFj0K9fvzdOVSYiIiIiIiIiIiJSpUqXtO3bty8ePXqEadOmISkpCU2bNkVYWJjiQmIJCQlKV43z8PDAli1b8P333+Pbb79F3bp1sWvXLjRq1EjRZvLkycjIyMDIkSORmpqKNm3aICwsDHp6eoo2mzdvxpgxY9CxY0dIpVL06tULy5cvf3cHroFkMhkCAwMLLRNBVB7sT6Qq7EukSuxPpCrsS6RK7E+kKuxLpErsT6RK7E+ARAgh1B0EEREREREREREREb1Uqda0JSIiIiIiIiIiIvrQMWlLREREREREREREVIkwaUtERERERERERERUiTBpS0RERERERERERFSJMGlLREREREREREREVIloqzsA0hyPHz/G+vXrERUVhaSkJACApaUlPDw8MHjwYJibm6s5QiIiIiIiIiIi0kR5eXm4dOmSUs7J2dkZOjo6ao5MPSRCCKHuIKjyO3PmDHx9fWFgYABvb2/UqFEDAPDw4UMcOnQImZmZCA8PR/PmzdUcKWmq27dv48aNG7CyskKjRo3UHQ4REZFKJCUlITo6WunDR8uWLWFpaanmyEiT5ebmIj4+HhYWFjAyMlJ3OKSBODZRReH4ROUhl8sxbdo0rFq1Cs+ePVOqMzIywpgxYxAUFASp9MNaMIBJWyqVVq1awcXFBSEhIZBIJEp1Qgj85z//wYULFxAVFaWmCEmTBAQEYP78+ahatSpevHiBgQMHYufOnRBCQCKRwNPTE3/99ReqVq2q7lBJQ5w+fbrQrwDc3d3h5uam5shI0/ELJSqvjIwMfPnll9i2bRskEglMTU0BACkpKRBCoH///li7di0MDAzUHClVdvPnz8fYsWOhr6+P/Px8fP3111ixYgXy8vIglUoxcOBArF279oOdhURlw7GJVInjE6nK5MmTsWHDBsycORO+vr5KEwX379+PqVOnYvDgwZg3b56aI33HBFEp6Onpibi4uGLr4+LihJ6e3juMiDSZVCoVDx8+FEII8c033wgbGxtx+PBhkZGRIY4fPy4cHBzElClT1BwlaYKHDx+KNm3aCIlEIuzs7ISbm5twc3MTdnZ2QiKRiDZt2ij6GtGbjBo1SqSnpwshhMjMzBS9evUSUqlUSCQSIZVKRfv27RX1RG8ybNgwUbduXREWFiby8vIU5Xl5eSI8PFzUq1dPDB8+XI0RkqZ49X3TggULhImJiVi/fr24dOmS+OWXX4SFhYWYN2+emqMkTcGxiVSJ4xOpSo0aNURYWFix9WFhYcLCwuIdRlQ5fFjziqncLC0tcfr06WLrT58+rfgmhOhNxCsT/Hfv3o358+ejffv2MDAwQOvWrbF48WLs2LFDjRGSpggICEB+fj7i4uIQHx+P6OhoREdHIz4+HnFxcZDL5Rg9erS6wyQNsXbtWmRmZgIAZs6ciejoaBw8eBDPnz/H0aNHkZCQgNmzZ6s5StIUf/zxBzZs2ABfX19oaWkpyrW0tODj44P169fj999/V2OEpClefd+0ZcsWzJ07F0OGDIGzszM+//xzLF68GJs2bVJjhKRJODaRKnF8IlVJT0+HtbV1sfVWVlbIyMh4hxFVDrwQGZXKV199hZEjRyImJgYdO3YstKbtunXrsHDhQjVHSZqkYJmNpKQkNGnSRKnOxcUFd+/eVUdYpGHCw8Nx9OhR1K9fv1Bd/fr1sXz5cnh5eb37wEgjFfeFEgDFF0qTJk1CcHCwukIkDSKXy6Grq1tsva6uLuRy+TuMiDRZwfumhIQEeHh4KNV5eHjg9u3b6giLNBDHJlI1jk+kCl5eXvjqq6+wefNmmJmZKdU9fvwYX3/99Qf5uY5JWyqV0aNHw8zMDEuWLMHq1auRn58P4OU3sq6urtiwYQP69Omj5ihJk0ydOhUGBgaQSqVITExEw4YNFXVPnjxBlSpV1BgdaQqZTIa0tLRi69PT0yGTyd5hRKTp+IUSqUq3bt0wcuRI/PTTT2jWrJlS3fnz5zFq1Ch0795dTdGRplm3bh2qVq0KXV1dpKSkKNXxtY7KgmMTqRrHJ1KFkJAQdOnSBVZWVmjcuLHSRMGLFy/C2dkZe/bsUXOU7x6TtlRqffv2Rd++fZGbm4vHjx8DAMzMzLioOJVZu3btcPXqVQCAs7Mz7ty5o1S/d+9epSQuUXH69u0Lf39/LFmyBB07doShoSEAIC0tDYcOHcLEiRPRv39/NUdJmoRfKJGqrFy5EgMGDICrqytMTExgYWEBAEhOTkZqaip8fX2xcuVKNUdJmsDW1hbr1q0D8PLLynPnzqFdu3aK+oiIiCJ/cUJUFI5NpEocn0hVatWqhX/++Qfh4eE4deqU4gLTbm5umDNnDnx8fCCVfngrvErEq78FJCKqBG7dugVdXV3Y2NioOxSq5LKzszF+/HisX78eeXl5ip/75eTkQFtbG8OGDcOSJUv4DT+VipeXl2KmLQB8/vnnGD58uOL2rFmzcPDgQURGRqohOtJUcXFxSh8+LC0t4e7ujgYNGqg5MnpfnDp1CjKZrNCsSaKSXLlyBVFRURybqEJxfCJ6O0zaEhGRxktLS0NMTIzSBw9XV1fFzFsiVeAXSkREREREFef06dOFvlDy8PBAixYt1ByZejBpS0Rq8eLFC8TExMDU1BTOzs5KdVlZWfjtt98waNAgNUVHmiojIwO//fYbbty4AWtra/Tr1w/Vq1dXd1ikIcaOHYs+ffqgbdu26g6F3hM5OTnYtWtXkR8+evbsWeLFgIhed+/ePRgbG6Nq1apK5bm5uYiKilL6STJRaQkhEBkZiRs3bsDKygq+vr5c/o5KbdGiRejVqxdq166t7lBIwyUnJ6NXr144ceIEbG1tlda0TUhIQOvWrfHHH38olnT5UDBpS0Tv3LVr1+Dj44OEhARIJBK0adMG27Ztg5WVFYCXA7O1tbXigndExXF2dsbx48dhamqKu3fvol27dnj69Cnq1auHmzdvQltbG6dOnYK9vb26QyUNIJVKIZFI4ODggGHDhsHf3x+WlpbqDos01I0bN+Dr64vExES0bNlS6cNHdHQ0bGxssG/fPjg6Oqo5UqrsHjx4gJ49eyImJgYSiQQDBgzA6tWrFclbvm+isujSpQu2bt0KIyMjpKSkoEuXLjh9+jTMzMzw5MkT1KtXD0ePHoW5ubm6QyUNIJVKIZVK0b59ewwfPhyffPIJv5Ckcvnss8+QmJiI0NDQQusgX716FUOHDoW1tTW2b9+upgjV48NbxZeI1O7rr79Go0aNkJycjKtXr6JatWpo3bo1EhIS1B0aaZgrV64gLy8PAPDNN9/A2toad+7cwenTp3Hnzh00adIE3333nZqjJE2yf/9+dOnSBQsXLoStrS169uyJPXv2QC6Xqzs00jCjRo1C48aN8fDhQ0RGRuLXX3/Fr7/+isjISDx8+BANGzbE6NGj1R0maYApU6ZAKpUiOjoaYWFhuHz5Mtq3b4+nT58q2nAeDpVWWFgYsrOzAQDff/890tPTcfPmTSQnJ+POnTuoUqUKpk2bpuYoSZP8+OOPqFKlCgYOHAhra2uMHz8e//77r7rDIg0THh6OVatWFXnhuvr162P58uUICwtTQ2TqxaQtEb1zJ0+eRHBwMMzMzODo6Ijdu3fD19cXbdu2xa1bt9QdHmmoqKgoTJ8+HUZGRgCAqlWrIigoCMePH1dzZKRJGjdujKVLlyIxMRG//PILsrOz4efnh1q1auG7777DjRs31B0iaYgTJ05g1qxZRa6tbWhoiJkzZ+LYsWNqiIw0zcGDB7F8+XI0b94c3t7eOHHiBKysrNChQwekpKQAgNJFFIlK6/DhwwgODlb8IsnGxgbz5s1DeHi4miMjTdKlSxfs2rUL9+7dw+TJkxEeHg4XFxe4ublh3bp1SE9PV3eIpAFkMhnS0tKKrU9PT/8gLy7NpC0RvXMvXryAtra24rZEIsGaNWvQvXt3eHp64tq1a2qMjjRNwQfVrKwsxRIbBWrWrIlHjx6pIyzScDo6OujTpw/CwsJw69YtjBgxAps3by7y23+iohgbGyM+Pr7Y+vj4eBgbG7+zeEhzPXv2DCYmJorbMpkMO3bsQO3atdG+fXskJyerMTrSRAXvnZ4+fQoHBwelOkdHRyQmJqojLNJwFhYWmDx5MuLi4hAZGQlnZ2dMmDCh0PtzoqL07dsX/v7+2Llzp1LyNi0tDTt37sSQIUPQv39/NUaoHtpvbkJEpFoNGjTA2bNn4eTkpFS+cuVKAECPHj3UERZpqI4dO0JbWxtpaWm4evUqGjVqpKi7c+cOL0RGb83W1hbTp09HYGAgDh48qO5wSEMMHz4cgwYNwtSpU9GxY0elNW0PHTqEWbNmYezYsWqOkjRBnTp1cOHCBdStW1dRpq2tje3bt6N3797o1q2bGqMjTTR48GDIZDLk5ubi9u3baNiwoaIuKSmJXyhRqRU3y79t27Zo27Ytli9fjl9//fUdR0WaaPHixZDL5ejXrx/y8vIUayPn5ORAW1sbw4YNw8KFC9Uc5bvHC5ER0TsXHByMY8eOYe/evUXWBwQEICQkhGtI0hsFBQUp3W7VqhV8fX0VtydNmoR79+5h69at7zo00kD29vY4e/YsE/2kMvPmzcOyZcuQlJSk+GArhIClpSXGjx+PyZMnqzlC0gRff/01YmNji/zJel5eHnr16oXdu3fzfROVypAhQ5Rud+7cGX369FHcnjx5Mi5cuPBBrh1JZSeVSpGUlAQLCwt1h0LvibS0NMTExCApKQkAYGlpCVdX1yKXm/oQMGlLRERERFSBbt++rfTho2D9SKLSyMvLQ2ZmZrEfWPPy8nD//n3Y2dm948jofZSRkQEtLS3o6empOxQiog8e17QlIiIiIqpA9vb2cHd3h7u7uyJhe/fuXQwdOlTNkZEm0NbWLnGG0YMHDwr98oSovFJSUhAQEKDuMOg9wdc6KosXL17g+PHjuHz5cqG6rKwsbNq0SQ1RqRdn2hIRERERvWP//PMPPvroI+Tn56s7FNJw7EukSuxPpErsT1Ra165dg4+PDxISEiCRSNCmTRts3boV1tbWAF5eE8Da2vqD60u8EBkRERERkYr99ddfJdbfunXrHUVCmo59iVSJ/YlUif2JVOXrr79Go0aNcPbsWaSmpmL8+PFo06YNIiMjYWtrq+7w1IYzbYmIiIiIVEwqlUIikaCkt9oSieSDmzFCZce+RKrE/kSqxP5EqlKjRg0cPHgQjRs3BvDywq0BAQHYu3cvIiIiUKVKlQ9ypi3XtCUiIiIiUjErKyvs2LEDcrm8yL9z586pO0TSEOxLpErsT6RK7E+kKi9evIC29v8WA5BIJFizZg26d+8OT09PXLt2TY3RqQ+TtkREREREKubq6oqYmJhi6980M4moAPsSqRL7E6kS+xOpSoMGDXD27NlC5StXrkTPnj3Ro0cPNUSlflzTloiIiIhIxSZNmoSMjIxi6x0dHREREfEOIyJNxb5EqsT+RKrE/kSq8sknn2Dr1q0YOHBgobqVK1dCLpcjJCREDZGpF9e0JSIiIiIiIiIiIqpEuDwCERERERERERERUSXCpC0RERERERERERFRJcKkLREREREREREREVElwqQtERERERERERERUSXCpC0RERERfTCmT58OiUSi7jA0XmRkJCQSCSIjI9UdChEREdF7iUlbIiIiIlK5DRs2QCKRKP1ZWFigffv22Ldvn7rD+yCsXr0aGzZsUHcYRERERFQO2uoOgIiIiIjeXzNmzIC9vT2EEHj48CE2bNiALl26YPfu3ejWrZu6w3uvrV69GmZmZhg8eLC6QyEiIiKiMmLSloiIiIgqTOfOndG8eXPF7WHDhqFGjRrYunUrk7aVSEZGBqpUqaLuMIiIiIjo/3B5BCIiIiJ6Z4yNjaGvrw9tbeW5AwsXLoSHhweqV68OfX19uLq64vfffy90f4lEgjFjxmDXrl1o1KgRZDIZGjZsiLCwsEJtjx8/jhYtWkBPTw8ODg5Yu3ZtsXH98ssvcHV1hb6+PkxNTdGvXz/cvXv3jceTnp6O8ePHo3bt2pDJZLCwsMDHH3+Mc+fOKdp4eXmhUaNGiImJgYeHB/T19WFvb4+QkJBC28vOzkZgYCAcHR0hk8lQq1YtTJ48GdnZ2UXG7ObmBgMDA5iYmKBdu3bYv38/AKB27dq4dOkSjhw5oliewsvLC8D/lq44cuQIAgICYGFhARsbGwDAnTt3EBAQgPr160NfXx/Vq1dH7969ER8f/8bHAgCio6PRqVMnGBkZwcDAAJ6enjhx4kSp7ktERERE/8OZtkRERERUYZ49e4bHjx9DCIHk5GSsWLECz58/xxdffKHUbtmyZejRowc+//xz5OTkYNu2bejduzf27NmDrl27KrU9fvw4duzYgYCAAFSrVg3Lly9Hr169kJCQgOrVqwMALl68CB8fH5ibm2P69OnIy8tDYGAgatSoUSjG2bNnY+rUqejTpw+GDx+OR48eYcWKFWjXrh3Onz8PY2PjYo/vP//5D37//XeMGTMGzs7OePLkCY4fP464uDh89NFHinZPnz5Fly5d0KdPH/Tv3x+//fYbRo0aBV1dXQwdOhQAIJfL0aNHDxw/fhwjR46Ek5MTLl68iCVLluDatWvYtWuXYntBQUGYPn06PDw8MGPGDOjq6iI6OhqHDx+Gj48Pli5dirFjx6Jq1ar47rvvAKDQsQcEBMDc3BzTpk1DRkYGAODMmTM4efIk+vXrBxsbG8THx2PNmjXw8vLC5cuXYWBgUOxjcfjwYXTu3Bmurq4IDAyEVCpFaGgoOnTogGPHjsHNza3Y+xIRERHRawQRERERkYqFhoYKAIX+ZDKZ2LBhQ6H2mZmZSrdzcnJEo0aNRIcOHZTKAQhdXV1x48YNRdk///wjAIgVK1Yoyvz8/ISenp64c+eOouzy5ctCS0tLvPoWOD4+XmhpaYnZs2cr7efixYtCW1u7UPnrjIyMxOjRo0ts4+npKQCIRYsWKcqys7NF06ZNhYWFhcjJyRFCCPHzzz8LqVQqjh07pnT/kJAQAUCcOHFCCCHE9evXhVQqFZ988onIz89XaiuXyxX/b9iwofD09CwUT8G5adOmjcjLy1Oqe/08CCFEVFSUACA2bdqkKIuIiBAAREREhGK/devWFb6+vkoxZGZmCnt7e/Hxxx+X9BARERER0Wu4PAIRERERVZhVq1bhwIEDOHDgAH755Re0b98ew4cPx44dO5Ta6evrK/7/9OlTPHv2DG3btlVaZqCAt7c3HBwcFLebNGkCQ0ND3Lp1CwCQn5+P8PBw+Pn5wdbWVtHOyckJvr6+StvasWMH5HI5+vTpg8ePHyv+LC0tUbduXURERJR4fMbGxoiOjkZiYmKJ7bS1tfHll18qbuvq6uLLL79EcnIyYmJiAADbt2+Hk5MTGjRooBRLhw4dAEARy65duyCXyzFt2jRIpcpv5yUSSYlxvGrEiBHQ0tJSKnv1POTm5uLJkydwdHSEsbFxkeeiQGxsLK5fv44BAwbgyZMnitgzMjLQsWNHHD16FHK5vNSxEREREX3ouDwCEREREVUYNzc3pQuR9e/fH82aNcOYMWPQrVs36OrqAgD27NmDWbNmITY2Vmn91qKSkK8mYguYmJjg6dOnAIBHjx7hxYsXqFu3bqF29evXx969exW3r1+/DiFEkW0BQEdHp8Tjmz9/Pvz9/VGrVi24urqiS5cuGDRoEOrUqaPUztrautCFvurVqwcAiI+PR6tWrXD9+nXExcXB3Ny8yH0lJycDAG7evAmpVApnZ+cSY3sTe3v7QmUvXrxAcHAwQkNDcf/+fQghFHXPnj0rdlvXr18HAPj7+xfb5tmzZzAxMXmLiImIiIg+HEzaEhEREdE7I5VK0b59eyxbtgzXr19Hw4YNcezYMfTo0QPt2rXD6tWrYWVlBR0dHYSGhmLLli2FtvH67NACryYYS0sul0MikWDfvn1Fbrdq1aol3r9Pnz5o27Ytdu7cif3792PBggWYN28eduzYgc6dO5c5lsaNG2Px4sVF1teqVatM23uTV2fVFhg7dixCQ0Mxfvx4uLu7w8jICBKJBP369StxpmxB3YIFC9C0adMi27zpsSQiIiKi/2HSloiIiIjeqby8PADA8+fPAQB//PEH9PT0EB4eDplMpmgXGhparu2bm5tDX19fMfvzVVevXlW67eDgACEE7O3tFTNfy8rKygoBAQEICAhAcnIyPvroI8yePVspaZuYmIiMjAyl2bbXrl0DANSuXVsRyz///IOOHTuWuMyBg4MD5HI5Ll++XGyCFCjbUgkFfv/9d/j7+2PRokWKsqysLKSmppZ4v4LlKgwNDeHt7V3m/RIRERGRMq5pS0RERETvTG5uLvbv3w9dXV04OTkBeDlzViKRID8/X9EuPj4eu3btKtc+tLS04Ovri127diEhIUFRHhcXh/DwcKW2n376KbS0tBAUFFRopq4QAk+ePCl2P/n5+YWWDLCwsIC1tbXSEg/Ay0T12rVrFbdzcnKwdu1amJubw9XVFcDLWbv379/HunXrCu3rxYsXyMjIAAD4+flBKpVixowZhWa/vnoMVapUeWOy9XVaWlqFHocVK1YonZuiuLq6wsHBAQsXLlQk41/16NGjMsVBRERE9KHjTFsiIiIiqjD79u3DlStXALxck3XLli24fv06pkyZAkNDQwBA165dsXjxYnTq1AkDBgxAcnIyVq1aBUdHR1y4cKFc+w0KCkJYWBjatm2LgIAA5OXlYcWKFWjYsKHSNh0cHDBr1ix88803iI+Ph5+fH6pVq4bbt29j586dGDlyJL766qsi95Geng4bGxt89tlncHFxQdWqVXHw4EGcOXNGaaYq8HJN23nz5iE+Ph716tXDr7/+itjYWPzwww+KdXMHDhyI3377Df/5z38QERGB1q1bIz8/H1euXMFvv/2G8PBwNG/eHI6Ojvjuu+8wc+ZMtG3bFp9++ilkMhnOnDkDa2trBAcHA3iZSF2zZg1mzZoFR0dHWFhYKC5qVpxu3brh559/hpGREZydnREVFYWDBw+ievXqJd5PKpXixx9/ROfOndGwYUMMGTIENWvWxP379xEREQFDQ0Ps3r37jeeNiIiIiF5i0paIiIiIKsy0adMU/9fT00ODBg2wZs0afPnll4ryDh064KeffsLcuXMxfvx42NvbKxKc5U3aNmnSBOHh4Zg4cSKmTZsGGxsbBAUF4cGDB4W2OWXKFNSrVw9LlixBUFAQgJfrx/r4+KBHjx7F7sPAwAABAQHYv38/duzYAblcDkdHR6xevRqjRo1SamtiYoKNGzdi7NixWLduHWrUqIGVK1dixIgRijZSqRS7du3CkiVLsGnTJuzcuRMGBgaoU6cOxo0bp7R8w4wZM2Bvb48VK1bgu+++g4GBAZo0aYKBAwcq2kybNg137tzB/PnzkZ6eDk9PzzcmbZctWwYtLS1s3rwZWVlZaN26NQ4ePAhfX983PuZeXl6IiorCzJkzsXLlSjx//hyWlpZo2bKl0vkmIiIiojeTiPJcsYGIiIjeO7Vr18b48eMxfvz4d77vwYMHIzU1tdw/hyeqzLy8vPD48WP8+++/6g6FiIiIiDQE17QlIiJ6CxKJpMS/6dOnq3yfO3bsgI+PD6pXrw6JRILY2NhCbbKysjB69GhUr14dVatWRa9evfDw4UOVx6LJZs+eDQ8PDxgYGMDY2LjINkWd023btinqBw8eXGSbhg0blimWH374AV5eXjA0NIREIilyHdLatWsX2s/cuXOV2oSHh6NVq1aoVq0azM3N0atXL8THx5cplqNHj6J79+6wtraGRCIpNpEeFxeHHj16wMjICFWqVEGLFi2U1o9VRR988OABBgwYgHr16kEqlRb5hcKGDRsKPS56enpKbR4+fIjBgwfD2toaBgYG6NSpU5EXKXuT//73v3B1dYVMJiv2AmBFnYOsrCylNtnZ2fjuu+9gZ2cHmUyG2rVrY/369WWKpTTjAABERUWhQ4cOqFKlCgwNDdGuXTu8ePECwMt1g4cNGwZ7e3vo6+vDwcEBgYGByMnJKVMs69atQ9u2bWFiYgITExN4e3vj9OnTSm2Keq506tSpTPvJysrC4MGD0bhxY2hra8PPz69Qm8jIyCKfk0lJSYo2wcHBaNGiBapVqwYLCwv4+fkVukBdabypP8THxxcZy6lTp5TapaamYvTo0bCysoJMJkO9evWwd+/eMsVSmjHk2rVr6NmzJ8zMzGBoaIg2bdogIiJCqc2bxjwiIiJ6d5i0JSIiegsPHjxQ/C1duhSGhoZKZcWthfk2MjIy0KZNG8ybN6/YNhMmTMDu3buxfft2HDlyBImJifj0009VHosmy8nJQe/evQv9jP11oaGhSuf01UTRsmXLlOru3r0LU1NT9O7du0yxZGZmolOnTvj2229LbDdjxgyl/Y0dO1ZRd/v2bfTs2RMdOnRAbGwswsPD8fjx4zKf94yMDLi4uGDVqlXFtrl58ybatGmDBg0aIDIyEhcuXMDUqVOVkqWq6IPZ2dkwNzfH999/DxcXl2Lbvf68u3PnjqJOCAE/Pz/cunULf/75J86fPw87Ozt4e3srLuxVFkOHDkXfvn2LrCvuHFy6dEmpXZ8+fXDo0CH89NNPuHr1KrZu3Yr69euXKY7SjANRUVHo1KkTfHx8cPr0aZw5cwZjxoyBVPryI8CVK1cgl8uxdu1aXLp0CUuWLEFISMgb++HrIiMj0b9/f0RERCAqKkqxtMT9+/eV2nXq1EnpPG3durVM+8nPz4e+vj7++9//wtvbu8S2V69eVdqXhYWFou7IkSMYPXo0Tp06hQMHDiA3Nxc+Pj4q7w8FDh48qBRLwYXngJfj0Mcff4z4+Hj8/vvvuHr1KtatW4eaNWuWKY7SjCHdunVDXl4eDh8+jJiYGLi4uKBbt25KCW2g5DGPiIiI3iFBREREKhEaGiqMjIwUt/Pz80VQUJCoWbOm0NXVFS4uLmLfvn2K+tu3bwsAYuvWrcLd3V3IZDLRsGFDERkZWar9Fdz//PnzSuWpqalCR0dHbN++XVEWFxcnAIioqKhit2dnZydmzJgh+vXrJwwMDIS1tbVYuXKlUptFixaJRo0aCQMDA2FjYyNGjRol0tPTCz0GYWFhokGDBqJKlSrC19dXJCYmKtrk5eWJCRMmCCMjI2FqaiomTZokBg0aJHr27Kn02M2ZM0fUrl1b6OnpiSZNmigdT0pKihgwYIAwMzMTenp6wtHRUaxfv75Uj9vrXj9vrwIgdu7cWept7dy5U0gkEhEfH1+uWCIiIgQA8fTp00J1dnZ2YsmSJcXed/v27UJbW1vk5+cryv766y8hkUhETk5OueIp7vj79u0rvvjii2LvV94+WBJPT08xbty4QuUlnT8hhLh69aoAIP79919FWX5+vjA3Nxfr1q0rVyyBgYHCxcWlUHlx5wCAcHZ2FkIIsW/fPmFkZCSePHlSrn2/rrhxQAghWrZsKb7//vsybW/+/PnC3t7+rWLKy8sT1apVExs3blSU+fv7Kz3H31Zx2yvpOVSc5ORkAUAcOXKkXLEU1x9KOjcF1qxZI+rUqVPu5+jrijv+R48eCQDi6NGjirK0tDQBQBw4cEBRVtYxj4iIiCoOZ9oSERFVkGXLlmHRokVYuHAhLly4AF9fX/To0aPQz7InTZqE//f//h/Onz8Pd3d3dO/eHU+ePCn3fmNiYpCbm6s0E61BgwawtbVFVFRUifddsGABXFxccP78eUyZMgXjxo3DgQMHFPVSqRTLly/HpUuXsHHjRhw+fBiTJ09W2kZmZiYWLlyIn3/+GUePHkVCQoLSjONFixZhw4YNWL9+PY4fP46UlBTs3LlTaRvBwcHYtGkTQkJCcOnSJUyYMAFffPEFjhw5AgCYOnUqLl++jH379iEuLg5r1qyBmZmZ4v5eXl4YPHhwmR+7oowePRpmZmZwc3PD+vXrIUq4HMBPP/0Eb29v2NnZqWTfr5s7dy6qV6+OZs2aYcGCBcjLy1PUubq6QiqVIjQ0FPn5+Xj27Bl+/vlneHt7Q0dHR2UxyOVy/P3336hXrx58fX1hYWGBli1bKi2j8DZ9sDyeP38OOzs71KpVCz179lSa2ZqdnQ0ASrOApVIpZDIZjh8/rtI4ijsHH3/8sSKmv/76C82bN8f8+fNRs2ZN1KtXD1999ZViyQJVSU5ORnR0NCwsLODh4YEaNWrA09Pzjcf87NkzmJqavtW+MzMzkZubW2g7kZGRsLCwQP369TFq1Ki3GufepGnTprCyssLHH3+MEydOlNj22bNnAPDWx12cHj16wMLCAm3atMFff/2lVPfXX3/B3d0do0ePRo0aNdCoUSPMmTMH+fn5Ko2hevXqqF+/PjZt2oSMjAzk5eVh7dq1sLCwUJr5C5RtzCMiIqIKpO6sMRER0fvi9Rl/1tbWYvbs2UptWrRoIQICAoQQ/5uFNXfuXEV9bm6usLGxEfPmzXvj/oqbxbV582ahq6tbqH2LFi3E5MmTi92enZ2d6NSpk1JZ3759RefOnYu9z/bt20X16tUVt0NDQwUAcePGDUXZqlWrRI0aNRS3raysxPz58xW3C465YNZcVlaWMDAwECdPnlTa17Bhw0T//v2FEEJ0795dDBkypNi4Bg4cKKZMmVJs/atKmqk5Y8YMcfz4cXHu3Dkxd+5cIZPJxLJly4pse//+faGlpSV+/fXXUu23KCXNEly0aJGIiIgQ//zzj1izZo0wNjYWEyZMUGoTGRkpLCwshJaWlgAg3N3dyzTj8HUoYtbdgwcPBABhYGAgFi9eLM6fPy+Cg4OFRCJRzBIvbx8sSXEzbU+ePCk2btwozp8/LyIjI0W3bt2EoaGhuHv3rhBCiJycHGFrayt69+4tUlJSRHZ2tpg7d64AIHx8fMoVS3EzK4V48znw9fUVMplMdO3aVURHR4u///5b2NnZicGDB5crluLGgaioKAFAmJqaivXr14tz586J8ePHC11dXXHt2rUit3X9+nVhaGgofvjhh3LFUmDUqFGiTp064sWLF4qyrVu3ij///FNcuHBB7Ny5Uzg5OYkWLVqIvLy8cu2juJm2V65cESEhIeLs2bPixIkTYsiQIUJbW1vExMQUuZ38/HzRtWtX0bp163LFIUTx/eHRo0di0aJF4tSpU+L06dPi66+/FhKJRPz555+KNvXr1xcymUwMHTpUnD17Vmzbtk2YmpqK6dOnlyuWksaQu3fvCldXVyGRSISWlpawsrIS586dU2pTljGPiIiIKpa2elLFRERE77e0tDQkJiaidevWSuWtW7fGP//8o1Tm7u6u+L+2tjaaN2+OuLi4dxLn616NpeD20qVLFbcPHjyI4OBgXLlyBWlpacjLy0NWVhYyMzNhYGAAADAwMICDg4PiPlZWVkhOTgbwckbbgwcP0LJlS0V9wTGL/5vNdePGDWRmZuLjjz9WiiUnJwfNmjUDAIwaNQq9evXCuXPn4OPjAz8/P3h4eCjabtq0SQWPxssZvQWaNWuGjIwMLFiwAP/9738Ltd24cSOMjY0rbP3HiRMnKv7fpEkT6Orq4ssvv0RwcDBkMhmSkpIwYsQI+Pv7o3///khPT8e0adPw2Wef4cCBA5BIJCqJQy6XAwB69uyJCRMmAHg5q/HkyZMICQmBp6enSvZTWu7u7kr91sPDA05OTli7di1mzpwJHR0d7NixA8OGDYOpqSm0tLTg7e2Nzp07q3wGYWnOgVwuh0QiwebNm2FkZAQAWLx4MT777DOsXr0a+vr6Koml4Dx9+eWXGDJkCICXffjQoUNYv349goODldrfv38fnTp1Qu/evTFixIhy73fu3LnYtm0bIiMjlWY39+vXT/H/xo0bo0mTJnBwcEBkZCQ6duxY7v29rn79+krrA3t4eODmzZtYsmQJfv7550LtR48ejX///Vfls64BwMzMTOl526JFCyQmJmLBggXo0aMHgJfnycLCAj/88AO0tLTg6uqK+/fvY8GCBQgMDFRZLEIIjB49GhYWFjh27Bj09fXx448/onv37jhz5gysrKwAlG3MIyIioorF5RGIiIjeM5aWlsjJySl09fCHDx/C0tKy3NuNj49Ht27d0KRJE/zxxx+IiYlRXKzq1avNv/5TfIlEUqbk2PPnzwEAf//9N2JjYxV/ly9fxu+//w4A6Ny5M+7cuYMJEyYgMTERHTt2rJCLvr2uZcuWuHfvnuIn9wWEEFi/fj0GDhwIXV3dCo+jIJa8vDzEx8cDAFatWgUjIyPMnz8fzZo1Q7t27fDLL7/g0KFDiI6OVtl+zczMoK2tDWdnZ6VyJycnJCQkAKi4PlgaOjo6aNasGW7cuKEoc3V1RWxsLFJTU/HgwQOEhYXhyZMnqFOnjkr3XZpzYGVlhZo1ayoStsDLx04IgXv37qksloIkXEnnqUBiYiLat28PDw8P/PDDD+Xe58KFCzF37lzs378fTZo0KbFtnTp1YGZmpnSeKoqbm1uR+xkzZgz27NmDiIgI2NjYVHgcwMvn7auxWFlZoV69etDS0lKUOTk5ISkpSWlcfVuHDx/Gnj17sG3bNrRu3RofffSR4kuCjRs3lhhvUWMeERERVTwmbYmIiCqAoaEhrK2tC62leOLEiUJJlFOnTin+n5eXh5iYGDg5OZV7366urtDR0cGhQ4cUZVevXkVCQkKhmbSvezWWgtsFscTExEAul2PRokVo1aoV6tWrh8TExDLFZmRkBCsrK6UkYsExF3B2doZMJkNCQgIcHR2V/mrVqqVoZ25uDn9/f/zyyy9YunTpWyWbSis2NhYmJiaQyWRK5UeOHMGNGzcwbNiwCo/h1VikUiksLCwAvFxHVCpVfmtXkAgqmHWpCrq6umjRogWuXr2qVH7t2jXFWr5v0wffVn5+Pi5evKhIWr7KyMgI5ubmuH79Os6ePYuePXuqdN+lOQetW7dGYmKi4ssJ4OVjJ5VKVZo4rF27NqytrUs8T8DLGbZeXl5wdXVFaGhoofhLa/78+Zg5cybCwsLQvHnzN7a/d+8enjx5UuR5UrXY2Fil/QghMGbMGOzcuROHDx+Gvb19hcdQXCytW7fGjRs3lJ6j165dg5WVlUq/AMrMzASAQudXKpWWOD4UN+YRERFRxePyCERERBVk0qRJCAwMhIODA5o2bYrQ0FDExsZi8+bNSu1WrVqFunXrwsnJCUuWLMHTp08xdOjQYrebkpKChIQERcK0ICljaWkJS0tLGBkZYdiwYZg4cSJMTU1haGiIsWPHwt3dHa1atSox5hMnTmD+/Pnw8/PDgQMHsH37dvz9998AAEdHR+Tm5mLFihXo3r07Tpw4gZCQkDI/LuPGjcPcuXNRt25dNGjQAIsXL1aakVmtWjV89dVXmDBhAuRyOdq0aYNnz57hxIkTMDQ0hL+/P6ZNmwZXV1c0bNgQ2dnZ2LNnj1Kie9CgQahZs2ahn4C/KiEhQfFY5ufnIzY2VnGcVatWxe7du/Hw4UO0atUKenp6OHDgAObMmVPkjN6ffvoJLVu2RKNGjcr8eAAvf1aflJSkmIF38eJFVKtWDba2tjA1NUVUVBSio6PRvn17VKtWDVFRUYqLs5mYmAAAunbtiiVLlmDGjBmKn+Z/++23sLOzUywrURrPnz9Xmgl4+/ZtxMbGwtTUFLa2tgBe9u2+ffuiXbt2aN++PcLCwrB7925ERkYCwFv1wdcVnJfnz5/j0aNHiI2Nha6uruLLjxkzZqBVq1ZwdHREamoqFixYgDt37mD48OGKbWzfvh3m5uawtbXFxYsXMW7cOPj5+cHHx6dMsdy4cQPPnz9HUlISXrx4oYjN2dkZurq6pToHAwYMwMyZMzFkyBAEBQXh8ePHmDRpEoYOHVqmpRHeNA5IJBLFGOTi4oKmTZti48aNuHLlimLGekHC1s7ODgsXLsSjR48U2y/LjOh58+Zh2rRp2LJlC2rXro2kpCQAQNWqVVG1alU8f/4cQUFB6NWrFywtLXHz5k1MnjwZjo6O8PX1LfV+AODy5cvIyclBSkoK0tPTFeegadOmAIClS5fC3t4eDRs2RFZWFn788UccPnwY+/fvV2xj9OjR2LJlC/78809Uq1ZNEa+RkVGZzsGb+sPGjRuhq6urOPc7duzA+vXr8eOPPyq2MWrUKKxcuRLjxo3D2LFjcf36dcyZM6fMyxG8aQxxd3eHiYmJYvzU19fHunXrcPv2bXTt2hUASjXmnT59GoMGDcKhQ4dQs2ZNAEDHjh3xySefYMyYMQCAlStXYufOnUpf2hAREVE5qHE9XSIiovfK6xe0ys/PF9OnTxc1a9YUOjo6wsXFRezbt09RX3ABoS1btgg3Nzehq6srnJ2dxeHDh9+4HwCF/gIDAxVtXrx4IQICAoSJiYkwMDAQn3zyiXjw4EGJ27WzsxNBQUGid+/ewsDAQFhaWha6AM3ixYuFlZWV0NfXF76+vmLTpk1KF70p6qJeO3fuFK++5cjNzRXjxo0ThoaGwtjYWEycOFEMGjRI6aJCcrlcLF26VNSvX1/o6OgIc3Nz4evrK44cOSKEEGLmzJnCyclJ6OvrC1NTU9GzZ09x69Ytxf09PT2Fv79/icfr7+9f5OMYEREhhBBi3759omnTpqJq1aqiSpUqwsXFRYSEhIj8/Hyl7aSmpgp9ff1iL95UcL5KEhgYWGQsoaGhQgghYmJiRMuWLYWRkZHQ09MTTk5OYs6cOSIrK0tpO1u3bhXNmjUTVapUEebm5qJHjx4iLi5OUV/Q5wqOsSgFFzJ6/e/1x/Onn34Sjo6OQk9PT7i4uIhdu3Yp1ZemD9rZ2Sn126IUFYudnZ2ifvz48cLW1lbo6uqKGjVqiC5duhS6uNKyZcuEjY2N0NHREba2tuL7778X2dnZSm38/f2Fp6dnibF4enoWGc/t27cVbd50DoQQIi4uTnh7ewt9fX1hY2MjJk6cKDIzMxX1Befg1e2+rjTjgBBCBAcHCxsbG2FgYCDc3d3FsWPH3riN1/vrq32xKHZ2diXGkpmZKXx8fIS5ubnQ0dERdnZ2YsSIESIpKanQ4/um521x+yowb9484eDgIPT09ISpqanw8vIqNKYWd8yvHqMq+sOGDRuEk5OTMDAwEIaGhsLNzU1s37690HZOnjwpWrZsKWQymahTp46YPXu20gXaVDGGCCHEmTNnhI+PjzA1NRXVqlUTrVq1Env37lXUl2bMK6pvvv48DgwMVHqOEhERUflIhFDxFRiIiIioVOLj42Fvb4/z588rZonR+ycwMBBHjhxRzEJVp4iICHz66ae4deuWYoauumRmZqJ69erYt28fvLy81BoLAHh6eqJ9+/aYPn26ukNBaGgo5syZg8uXLxdaI/pdu337NurVq4fLly+jbt26FbovOzs7BAUFYfDgwRW6n9KoTP2hMo0hRERE9O5weQQiIiKiCrRv3z6sXLlS3WEAAPbu3Ytvv/1W7Qlb4GUCuUOHDpUiYfvs2TPcvHlTsRSIuu3duxdz5sxRe8K2IJaRI0dWeML20qVLMDIywqBBgyp0P6VR2fpDZRpDiIiI6N3hTFsiIiI14UxbIiIiIiIiKgqTtkRERERERERERESViFTdARARERERERERERHR/zBpS0RERERERERERFSJMGlLREREREREREREVIkwaUtERERERERERERUiTBpS0RERERERERERFSJMGlLREREREREREREVIkwaUtERERERERERERUiTBpS0RERERERERERFSJMGlLREREREREREREVIn8fx2WMEkkOUe3AAAAAElFTkSuQmCC\n"},"metadata":{}},{"name":"stdout","text":"Graphique sauvegardé dans 'coefficients_deflexion_par_bande.png'\n\n####################################################################################################\n#                                      FIN DE L'ANALYSE                                      #\n####################################################################################################\n\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"### Étape 8: Classification multiclasse avec MLP en utilisant différents ensembles de bandes discriminantes","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, BatchNormalization, Input, Dropout, Activation\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.utils import to_categorical\nimport os\nfrom scipy.io import loadmat\nfrom tqdm import tqdm\n\n# Définir explicitement les noms des classes\nclass_names_custom = [\n    'Background',\n    'Alfalfa',\n    'Corn-notill',\n    'Corn-mintill',\n    'Corn',\n    'Grass-pasture',\n    'Grass-trees',\n    'Grass-pasture-mowed',\n    'Hay-windrowed',\n    'Oats',\n    'Soybean-notill',\n    'Soybean-mintill',\n    'Soybean-clean',\n    'Wheat',\n    'Woods',\n    'Buildings-Grass-Trees-Drives',\n    'Stone-Steel-Towers'\n]\n\n# Définir le chemin du dataset\ndataset_path = \"/kaggle/input/dataset-deflection-coefficient\"  # Chemin corrigé\n\n# Charger les données hyperspectrales et vérités terrain\ndef charger_donnees_pour_mlp():\n    print(\"Chargement des données pour le MLP...\")\n    # Charger l'image hyperspectrale\n    chemin_image = os.path.join(dataset_path, \"Indian_pines_corrected.mat\")\n    donnees_mat = loadmat(chemin_image)\n    donnees_hyperspectrales = donnees_mat['indian_pines_corrected']\n    \n    # Charger la vérité terrain\n    chemin_gt = os.path.join(dataset_path, \"Indian_pines_gt.mat\")\n    gt_mat = loadmat(chemin_gt)\n    verite_terrain = gt_mat['indian_pines_gt']\n    \n    # Réorganiser les données\n    height, width, n_bands = donnees_hyperspectrales.shape\n    pixels = donnees_hyperspectrales.reshape(height * width, n_bands)\n    classes = verite_terrain.reshape(height * width)\n    \n    print(f\"Dimensions des données: pixels {pixels.shape}, classes {classes.shape}\")\n    return pixels, classes\n\n# 1. Fonction pour préparer les données selon les bandes sélectionnées\ndef preparer_donnees_mlp(pixels, classes, bandes_selectionnees):\n    \"\"\"\n    Prépare les données pour l'entraînement avec les bandes sélectionnées.\n    Inclut la classe de fond (background).\n    \n    Args:\n        pixels: Données spectrales (n_pixels x n_bandes)\n        classes: Étiquettes de classe pour chaque pixel\n        bandes_selectionnees: Liste des indices des bandes à utiliser\n    \n    Returns:\n        X_train, X_test, y_train, y_test: Ensembles d'entraînement et de test\n    \"\"\"\n    # Utiliser tous les pixels, y compris la classe de fond (0)\n    X = pixels   # Toutes les bandes\n    y = classes  # Étiquettes (garder les indices originaux y compris 0)\n    \n    # Nombre de classes (incluant le fond)\n    n_classes = len(np.unique(y))\n    print(f\"Nombre de classes (avec background): {n_classes}\")\n    \n    # Sélectionner uniquement les bandes choisies\n    X_selected = X[:, bandes_selectionnees]\n    print(f\"Dimensions des données: {X_selected.shape}\")\n    \n    # Standardisation des données\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X_selected)\n    \n    # Conversion des étiquettes en format one-hot\n    y_onehot = to_categorical(y)\n    \n    # Diviser en ensembles d'entraînement et de test\n    X_train, X_test, y_train, y_test = train_test_split(\n        X_scaled, y_onehot, test_size=0.3, random_state=42, stratify=y\n    )\n    \n    print(f\"Ensemble d'entraînement: {X_train.shape}, {y_train.shape}\")\n    print(f\"Ensemble de test: {X_test.shape}, {y_test.shape}\")\n    \n    return X_train, X_test, y_train, y_test, n_classes\n\n# 2. Définition du modèle MLP multiclasse avec BatchNorm avant activation\ndef creer_modele_mlp_multiclasse(input_dim, n_classes):\n    \"\"\"\n    Crée un modèle MLP pour la classification multiclasse avec architecture 512-64-64-32\n    et BatchNorm avant activation.\n    \"\"\"\n    inputs = Input(shape=(input_dim,))\n    \n    # Première couche cachée\n    x = Dense(512)(inputs)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    \n    # Deuxième couche cachée\n    x = Dense(64)(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x = Dropout(0.3)(x)\n    \n    # Troisième couche cachée\n    x = Dense(64)(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    \n    # Quatrième couche cachée\n    x = Dense(32)(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    \n    # Couche de sortie avec softmax pour la classification multiclasse\n    outputs = Dense(n_classes, activation='softmax')(x)\n    \n    model = Model(inputs=inputs, outputs=outputs)\n    \n    # Compiler le modèle\n    model.compile(\n        optimizer=Adam(learning_rate=0.001),\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    \n    return model\n\n# 3. Fonction pour entraîner et évaluer un modèle\ndef entrainer_evaluer_modele(X_train, X_test, y_train, y_test, n_classes, \n                           bandes_selectionnees, nom_modele):\n    \"\"\"\n    Entraîne et évalue un modèle MLP avec les données fournies.\n    \n    Args:\n        X_train, X_test, y_train, y_test: Ensembles d'entraînement et de test\n        n_classes: Nombre de classes\n        bandes_selectionnees: Liste des indices des bandes utilisées\n        nom_modele: Nom pour sauvegarder le modèle et les résultats\n    \n    Returns:\n        model: Le modèle entraîné\n        history: L'historique d'entraînement\n        metrics: Dictionnaire des métriques d'évaluation\n    \"\"\"\n    # Créer un dossier pour les résultats de ce modèle\n    os.makedirs(f\"resultats_{nom_modele}\", exist_ok=True)\n    \n    print(f\"\\nCréation et entraînement du modèle {nom_modele}...\")\n    start_time = time.time()\n    \n    # Créer le modèle\n    model = creer_modele_mlp_multiclasse(input_dim=len(bandes_selectionnees), n_classes=n_classes)\n    model.summary()\n    \n    # Définir l'early stopping\n    early_stopping = EarlyStopping(\n        monitor='val_loss',\n        patience=15,\n        restore_best_weights=True,\n        verbose=1\n    )\n    \n    # Entraîner le modèle\n    history = model.fit(\n        X_train, y_train,\n        validation_split=0.2,\n        epochs=100,\n        batch_size=32,\n        callbacks=[early_stopping],\n        verbose=1\n    )\n    \n    train_time = time.time() - start_time\n    print(f\"\\nTemps d'entraînement: {train_time:.2f} secondes\")\n    \n    # Évaluation du modèle\n    print(\"\\nÉvaluation du modèle sur l'ensemble de test...\")\n    start_time = time.time()\n    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n    predict_time = time.time() - start_time\n    \n    print(f\"Précision (accuracy): {accuracy:.4f}\")\n    print(f\"Temps de prédiction: {predict_time:.2f} secondes\")\n    \n    # Générer les prédictions\n    y_pred_prob = model.predict(X_test, verbose=0)\n    y_pred = np.argmax(y_pred_prob, axis=1)\n    y_true = np.argmax(y_test, axis=1)\n    \n    # Utiliser tous les noms de classes, y compris le background\n    class_labels = class_names_custom[:n_classes]\n    \n    # Rapport de classification détaillé\n    report = classification_report(y_true, y_pred, target_names=class_labels, zero_division=0)\n    print(\"\\nRapport de classification:\")\n    print(report)\n    \n    # Sauvegarder le rapport dans un fichier\n    with open(f\"resultats_{nom_modele}/rapport_classification_{nom_modele}.txt\", \"w\") as f:\n        f.write(f\"Précision (accuracy): {accuracy:.4f}\\n\")\n        f.write(f\"Temps d'entraînement: {train_time:.2f} secondes\\n\")\n        f.write(f\"Temps de prédiction: {predict_time:.2f} secondes\\n\\n\")\n        f.write(report)\n    \n    # Visualisations\n    # Courbes d'apprentissage\n    plt.figure(figsize=(12, 5))\n    \n    # Loss\n    plt.subplot(1, 2, 1)\n    plt.plot(history.history['loss'], label='Training Loss')\n    plt.plot(history.history['val_loss'], label='Validation Loss')\n    plt.title('Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    \n    # Accuracy\n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['accuracy'], label='Training Accuracy')\n    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n    plt.title('Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    \n    plt.tight_layout()\n    plt.savefig(f\"resultats_{nom_modele}/learning_curves_{nom_modele}.png\")\n    plt.close()\n    \n    # Matrice de confusion\n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(16, 14))\n    \n    # Utiliser des étiquettes sécurisées pour les axes\n    x_labels = [label[:10] for label in class_labels]\n    y_labels = [label[:10] for label in class_labels]\n    \n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n                xticklabels=x_labels, \n                yticklabels=y_labels)\n    plt.title(f'Matrice de confusion - {nom_modele}')\n    plt.xlabel('Prédit')\n    plt.ylabel('Réel')\n    plt.tight_layout()\n    plt.savefig(f\"resultats_{nom_modele}/confusion_matrix_{nom_modele}.png\")\n    plt.close()\n    \n    # Précision par classe\n    # Récupérer le rapport sous forme de dictionnaire\n    report_dict = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n    \n    # Préparer les données pour la visualisation\n    precision_by_class = []\n    for i, classe in enumerate(class_labels):\n        if classe in report_dict:\n            classe_dict = report_dict[classe]\n            precision_by_class.append({\n                'Classe': classe,\n                'Précision': classe_dict['precision'],\n                'Rappel': classe_dict['recall'],\n                'F1-score': classe_dict['f1-score'],\n                'Support': classe_dict['support']\n            })\n    \n    # Créer le DataFrame et trier\n    precision_df = pd.DataFrame(precision_by_class)\n    if not precision_df.empty:\n        precision_df = precision_df.sort_values('F1-score', ascending=False)\n        \n        # Visualisation\n        plt.figure(figsize=(14, 8))\n        sns.barplot(x='Classe', y='F1-score', data=precision_df)\n        plt.title(f'F1-score par classe - {nom_modele}')\n        plt.xlabel('Classe')\n        plt.ylabel('F1-score')\n        plt.xticks(rotation=90)\n        plt.tight_layout()\n        plt.savefig(f\"resultats_{nom_modele}/f1score_by_class_{nom_modele}.png\")\n        plt.close()\n        \n        # Sauvegarder les F1-scores par classe\n        precision_df.to_csv(f\"resultats_{nom_modele}/f1scores_{nom_modele}.csv\", index=False)\n    else:\n        print(\"Impossible de créer la visualisation du F1-score par classe - données insuffisantes\")\n    \n    # Sauvegarde du modèle\n    model.save(f\"resultats_{nom_modele}/model_{nom_modele}.h5\")\n    print(f\"\\nModèle sauvegardé sous 'resultats_{nom_modele}/model_{nom_modele}.h5'\")\n    \n    # Enregistrer les informations sur les bandes sélectionnées\n    pd.DataFrame({\n        'Bande': bandes_selectionnees,\n        'CoefficientDeflexion': [float(min_dc_par_bande[min_dc_par_bande['Bande'] == b]['CoefficientDeflexion'].values[0]) \n                         for b in bandes_selectionnees]\n    }).to_csv(f\"resultats_{nom_modele}/bandes_selectionnees_{nom_modele}.csv\", index=False)\n    \n    # Rassembler les métriques pour la comparaison finale\n    metrics = {\n        'accuracy': accuracy,\n        'train_time': train_time,\n        'predict_time': predict_time,\n        'n_bands': len(bandes_selectionnees)\n    }\n    \n    return model, history, metrics\n\n# 4. Fonction principale pour entraîner tous les modèles\ndef entrainer_tous_modeles():\n    \"\"\"\n    Fonction principale pour entraîner et évaluer tous les modèles.\n    \"\"\"\n    # Créer un dossier de sortie pour les résultats globaux\n    os.makedirs(\"resultats_comparaison\", exist_ok=True)\n    \n    # Liste pour stocker les métriques de tous les modèles\n    all_metrics = []\n    \n    # 1. Modèle avec Top 5 bandes\n    print(\"\\n========== MODÈLE TOP 5 BANDES ==========\")\n    X_train, X_test, y_train, y_test, n_classes = preparer_donnees_mlp(pixels, classes, top5_bandes)\n    _, _, metrics_top5 = entrainer_evaluer_modele(\n        X_train, X_test, y_train, y_test, n_classes, top5_bandes, \"top5_deflection\"\n    )\n    metrics_top5['model'] = 'Top 5 bandes'\n    all_metrics.append(metrics_top5)\n    \n    # 2. Modèle avec Top 10 bandes\n    print(\"\\n========== MODÈLE TOP 10 BANDES ==========\")\n    X_train, X_test, y_train, y_test, n_classes = preparer_donnees_mlp(pixels, classes, top10_bandes)\n    _, _, metrics_top10 = entrainer_evaluer_modele(\n        X_train, X_test, y_train, y_test, n_classes, top10_bandes, \"top10_deflection\"\n    )\n    metrics_top10['model'] = 'Top 10 bandes'\n    all_metrics.append(metrics_top10)\n    \n    # 3. Modèle avec Top 15 bandes\n    print(\"\\n========== MODÈLE TOP 15 BANDES ==========\")\n    X_train, X_test, y_train, y_test, n_classes = preparer_donnees_mlp(pixels, classes, top15_bandes)\n    _, _, metrics_top15 = entrainer_evaluer_modele(\n        X_train, X_test, y_train, y_test, n_classes, top15_bandes, \"top15_deflection\"\n    )\n    metrics_top15['model'] = 'Top 15 bandes'\n    all_metrics.append(metrics_top15)\n    \n    # 4. Modèle avec Top 20 bandes\n    print(\"\\n========== MODÈLE TOP 20 BANDES ==========\")\n    X_train, X_test, y_train, y_test, n_classes = preparer_donnees_mlp(pixels, classes, top20_bandes)\n    _, _, metrics_top20 = entrainer_evaluer_modele(\n        X_train, X_test, y_train, y_test, n_classes, top20_bandes, \"top20_deflection\"\n    )\n    metrics_top20['model'] = 'Top 20 bandes'\n    all_metrics.append(metrics_top20)\n    \n    # Créer un tableau de comparaison\n    comparison_df = pd.DataFrame(all_metrics)\n    comparison_df = comparison_df[['model', 'n_bands', 'accuracy', 'train_time', 'predict_time']]\n    comparison_df.columns = ['Modèle', 'Nombre de bandes', 'Précision', 'Temps d\\'entraînement (s)', 'Temps de prédiction (s)']\n    \n    # Sauvegarder le tableau de comparaison\n    comparison_df.to_csv(\"resultats_comparaison/comparaison_modeles_deflection.csv\", index=False)\n    print(\"\\nTableau de comparaison sauvegardé dans 'resultats_comparaison/comparaison_modeles_deflection.csv'\")\n    \n    # Visualiser la comparaison des précisions\n    plt.figure(figsize=(10, 6))\n    sns.barplot(x='Modèle', y='Précision', data=comparison_df)\n    plt.title('Comparaison de la précision des modèles')\n    plt.xlabel('Modèle')\n    plt.ylabel('Précision')\n    plt.ylim(0, 1)\n    plt.tight_layout()\n    plt.savefig(\"resultats_comparaison/comparaison_precision_deflection.png\")\n    plt.close()\n    \n    # Visualiser la comparaison des temps d'entraînement\n    plt.figure(figsize=(10, 6))\n    sns.barplot(x='Modèle', y='Temps d\\'entraînement (s)', data=comparison_df)\n    plt.title('Comparaison des temps d\\'entraînement')\n    plt.xlabel('Modèle')\n    plt.ylabel('Temps d\\'entraînement (s)')\n    plt.tight_layout()\n    plt.savefig(\"resultats_comparaison/comparaison_temps_entrainement_deflection.png\")\n    plt.close()\n    \n    print(\"\\nAnalyse comparative terminée!\")\n    print(\"\\nRécapitulatif des précisions:\")\n    for metric in all_metrics:\n        print(f\"{metric['model']}: {metric['accuracy']:.4f}\")\n    \n    return comparison_df\n\n# Charger les données\npixels, classes = charger_donnees_pour_mlp()\n\n# Charger les résultats de coefficient de déflexion\nprint(\"Chargement des résultats de coefficient de déflexion...\")\nmin_dc_par_bande = pd.read_csv('coefficients_deflexion_par_bande_tri.csv')\n\n# Sélection des meilleures bandes selon les différentes configurations\nprint(\"Sélection des meilleures bandes selon différentes configurations...\")\ntop5_bandes = min_dc_par_bande.head(5)['Bande'].values\ntop10_bandes = min_dc_par_bande.head(10)['Bande'].values\ntop15_bandes = min_dc_par_bande.head(15)['Bande'].values\ntop20_bandes = min_dc_par_bande.head(20)['Bande'].values\n\nprint(f\"Top 5 bandes: {top5_bandes}\")\nprint(f\"Top 10 bandes: {top10_bandes}\")\nprint(f\"Top 15 bandes: {top15_bandes}\")\nprint(f\"Top 20 bandes: {top20_bandes}\")\n\n# Exécuter l'entraînement des modèles\nif __name__ == \"__main__\":\n    comparison_results = entrainer_tous_modeles()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T12:26:50.431008Z","iopub.execute_input":"2025-05-09T12:26:50.431299Z","iopub.status.idle":"2025-05-09T12:31:57.044319Z","shell.execute_reply.started":"2025-05-09T12:26:50.431279Z","shell.execute_reply":"2025-05-09T12:31:57.043643Z"}},"outputs":[{"name":"stdout","text":"Chargement des données pour le MLP...\nDimensions des données: pixels (21025, 200), classes (21025,)\nChargement des résultats de coefficient de déflexion...\nSélection des meilleures bandes selon différentes configurations...\nTop 5 bandes: [157 158 160 159 186]\nTop 10 bandes: [157 158 160 159 186 162  25 152 156 185]\nTop 15 bandes: [157 158 160 159 186 162  25 152 156 185  22 176  26 178 161]\nTop 20 bandes: [157 158 160 159 186 162  25 152 156 185  22 176  26 178 161  51  23  28\n  73  98]\n\n========== MODÈLE TOP 5 BANDES ==========\nNombre de classes (avec background): 17\nDimensions des données: (21025, 5)\nEnsemble d'entraînement: (14717, 5), (14717, 17)\nEnsemble de test: (6308, 5), (6308, 17)\n\nCréation et entraînement du modèle top5_deflection...\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1746793612.023464      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1746793612.024236      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)                   │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │           \u001b[38;5;34m3,072\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │           \u001b[38;5;34m2,048\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation (\u001b[38;5;33mActivation\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │          \u001b[38;5;34m32,832\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_1                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │             \u001b[38;5;34m256\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_1 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │           \u001b[38;5;34m4,160\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_2                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │             \u001b[38;5;34m256\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_2 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │           \u001b[38;5;34m2,080\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_3                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │             \u001b[38;5;34m128\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_3 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m)                  │             \u001b[38;5;34m561\u001b[0m │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)                   │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">3,072</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">32,832</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_1                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_2                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_3                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">561</span> │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m45,393\u001b[0m (177.32 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">45,393</span> (177.32 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m44,049\u001b[0m (172.07 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">44,049</span> (172.07 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,344\u001b[0m (5.25 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,344</span> (5.25 KB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/100\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1746793617.254940      97 service.cc:148] XLA service 0x7f2064029c70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1746793617.255619      97 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1746793617.255635      97 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\nI0000 00:00:1746793617.685744      97 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m 96/368\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.1873 - loss: 2.6856","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1746793619.714723      97 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.3817 - loss: 2.0820 - val_accuracy: 0.5459 - val_loss: 1.3880\nEpoch 2/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5649 - loss: 1.3385 - val_accuracy: 0.5707 - val_loss: 1.2730\nEpoch 3/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5667 - loss: 1.3093 - val_accuracy: 0.5771 - val_loss: 1.2485\nEpoch 4/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5569 - loss: 1.3164 - val_accuracy: 0.5730 - val_loss: 1.2525\nEpoch 5/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5588 - loss: 1.2956 - val_accuracy: 0.5771 - val_loss: 1.2432\nEpoch 6/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5516 - loss: 1.3366 - val_accuracy: 0.5673 - val_loss: 1.2491\nEpoch 7/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5626 - loss: 1.2808 - val_accuracy: 0.5707 - val_loss: 1.2354\nEpoch 8/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5597 - loss: 1.2905 - val_accuracy: 0.5696 - val_loss: 1.2379\nEpoch 9/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5563 - loss: 1.2870 - val_accuracy: 0.5730 - val_loss: 1.2392\nEpoch 10/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5629 - loss: 1.2726 - val_accuracy: 0.5717 - val_loss: 1.2498\nEpoch 11/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5624 - loss: 1.2763 - val_accuracy: 0.5690 - val_loss: 1.2422\nEpoch 12/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5589 - loss: 1.2821 - val_accuracy: 0.5696 - val_loss: 1.2457\nEpoch 13/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5679 - loss: 1.2556 - val_accuracy: 0.5683 - val_loss: 1.2399\nEpoch 14/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5662 - loss: 1.2684 - val_accuracy: 0.5730 - val_loss: 1.2324\nEpoch 15/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5611 - loss: 1.2593 - val_accuracy: 0.5734 - val_loss: 1.2623\nEpoch 16/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5621 - loss: 1.2712 - val_accuracy: 0.5737 - val_loss: 1.2315\nEpoch 17/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5646 - loss: 1.2556 - val_accuracy: 0.5717 - val_loss: 1.2332\nEpoch 18/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5608 - loss: 1.2629 - val_accuracy: 0.5737 - val_loss: 1.2257\nEpoch 19/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5570 - loss: 1.2613 - val_accuracy: 0.5557 - val_loss: 1.2363\nEpoch 20/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5620 - loss: 1.2588 - val_accuracy: 0.5764 - val_loss: 1.2180\nEpoch 21/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5617 - loss: 1.2593 - val_accuracy: 0.5703 - val_loss: 1.2231\nEpoch 22/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5593 - loss: 1.2630 - val_accuracy: 0.5751 - val_loss: 1.2346\nEpoch 23/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5669 - loss: 1.2377 - val_accuracy: 0.5659 - val_loss: 1.2399\nEpoch 24/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5655 - loss: 1.2495 - val_accuracy: 0.5713 - val_loss: 1.2167\nEpoch 25/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5657 - loss: 1.2399 - val_accuracy: 0.5703 - val_loss: 1.2209\nEpoch 26/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5609 - loss: 1.2447 - val_accuracy: 0.5690 - val_loss: 1.2168\nEpoch 27/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5663 - loss: 1.2356 - val_accuracy: 0.5737 - val_loss: 1.2327\nEpoch 28/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5628 - loss: 1.2445 - val_accuracy: 0.5751 - val_loss: 1.2351\nEpoch 29/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5569 - loss: 1.2594 - val_accuracy: 0.5734 - val_loss: 1.2096\nEpoch 30/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5665 - loss: 1.2305 - val_accuracy: 0.5737 - val_loss: 1.2145\nEpoch 31/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5731 - loss: 1.2326 - val_accuracy: 0.5703 - val_loss: 1.2094\nEpoch 32/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5660 - loss: 1.2378 - val_accuracy: 0.5690 - val_loss: 1.2198\nEpoch 33/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5585 - loss: 1.2493 - val_accuracy: 0.5693 - val_loss: 1.2137\nEpoch 34/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5717 - loss: 1.2230 - val_accuracy: 0.5673 - val_loss: 1.2117\nEpoch 35/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5627 - loss: 1.2409 - val_accuracy: 0.5720 - val_loss: 1.2154\nEpoch 36/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5652 - loss: 1.2442 - val_accuracy: 0.5757 - val_loss: 1.2069\nEpoch 37/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5711 - loss: 1.2228 - val_accuracy: 0.5700 - val_loss: 1.2112\nEpoch 38/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5694 - loss: 1.2290 - val_accuracy: 0.5703 - val_loss: 1.2086\nEpoch 39/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5681 - loss: 1.2350 - val_accuracy: 0.5717 - val_loss: 1.2060\nEpoch 40/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5667 - loss: 1.2406 - val_accuracy: 0.5751 - val_loss: 1.2232\nEpoch 41/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5688 - loss: 1.2238 - val_accuracy: 0.5734 - val_loss: 1.2106\nEpoch 42/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5555 - loss: 1.2432 - val_accuracy: 0.5659 - val_loss: 1.2267\nEpoch 43/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5635 - loss: 1.2401 - val_accuracy: 0.5737 - val_loss: 1.2111\nEpoch 44/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5651 - loss: 1.2416 - val_accuracy: 0.5737 - val_loss: 1.2097\nEpoch 45/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5715 - loss: 1.2223 - val_accuracy: 0.5724 - val_loss: 1.2072\nEpoch 46/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5650 - loss: 1.2263 - val_accuracy: 0.5717 - val_loss: 1.2238\nEpoch 47/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5560 - loss: 1.2487 - val_accuracy: 0.5747 - val_loss: 1.2021\nEpoch 48/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5688 - loss: 1.2253 - val_accuracy: 0.5710 - val_loss: 1.2117\nEpoch 49/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5669 - loss: 1.2229 - val_accuracy: 0.5757 - val_loss: 1.1991\nEpoch 50/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5680 - loss: 1.2190 - val_accuracy: 0.5696 - val_loss: 1.2107\nEpoch 51/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5666 - loss: 1.2287 - val_accuracy: 0.5710 - val_loss: 1.2121\nEpoch 52/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5620 - loss: 1.2329 - val_accuracy: 0.5754 - val_loss: 1.2033\nEpoch 53/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5654 - loss: 1.2296 - val_accuracy: 0.5713 - val_loss: 1.1997\nEpoch 54/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5646 - loss: 1.2331 - val_accuracy: 0.5757 - val_loss: 1.1990\nEpoch 55/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5563 - loss: 1.2485 - val_accuracy: 0.5639 - val_loss: 1.2130\nEpoch 56/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5730 - loss: 1.2254 - val_accuracy: 0.5751 - val_loss: 1.2437\nEpoch 57/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5588 - loss: 1.2416 - val_accuracy: 0.5720 - val_loss: 1.2047\nEpoch 58/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5646 - loss: 1.2277 - val_accuracy: 0.5720 - val_loss: 1.2045\nEpoch 59/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5599 - loss: 1.2396 - val_accuracy: 0.5751 - val_loss: 1.2226\nEpoch 60/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5616 - loss: 1.2245 - val_accuracy: 0.5696 - val_loss: 1.2137\nEpoch 61/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5662 - loss: 1.2354 - val_accuracy: 0.5696 - val_loss: 1.2143\nEpoch 62/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5640 - loss: 1.2367 - val_accuracy: 0.5768 - val_loss: 1.2046\nEpoch 63/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5627 - loss: 1.2212 - val_accuracy: 0.5737 - val_loss: 1.2165\nEpoch 64/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5605 - loss: 1.2329 - val_accuracy: 0.5700 - val_loss: 1.2037\nEpoch 65/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5575 - loss: 1.2514 - val_accuracy: 0.5734 - val_loss: 1.2078\nEpoch 66/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5558 - loss: 1.2574 - val_accuracy: 0.5703 - val_loss: 1.2128\nEpoch 67/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5708 - loss: 1.2110 - val_accuracy: 0.5761 - val_loss: 1.1991\nEpoch 68/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5629 - loss: 1.2425 - val_accuracy: 0.5656 - val_loss: 1.2096\nEpoch 69/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5702 - loss: 1.2211 - val_accuracy: 0.5717 - val_loss: 1.2073\nEpoch 69: early stopping\nRestoring model weights from the end of the best epoch: 54.\n\nTemps d'entraînement: 65.23 secondes\n\nÉvaluation du modèle sur l'ensemble de test...\nPrécision (accuracy): 0.5770\nTemps de prédiction: 1.32 secondes\n\nRapport de classification:\n                              precision    recall  f1-score   support\n\n                  Background       0.66      0.92      0.77      3233\n                     Alfalfa       0.00      0.00      0.00        14\n                 Corn-notill       0.44      0.17      0.24       428\n                Corn-mintill       0.00      0.00      0.00       249\n                        Corn       0.00      0.00      0.00        71\n               Grass-pasture       0.00      0.00      0.00       145\n                 Grass-trees       0.00      0.00      0.00       219\n         Grass-pasture-mowed       0.00      0.00      0.00         8\n               Hay-windrowed       0.60      0.10      0.18       143\n                        Oats       0.00      0.00      0.00         6\n              Soybean-notill       0.00      0.00      0.00       292\n             Soybean-mintill       0.36      0.80      0.49       737\n               Soybean-clean       0.00      0.00      0.00       178\n                       Wheat       0.00      0.00      0.00        61\n                       Woods       0.00      0.00      0.00       380\nBuildings-Grass-Trees-Drives       0.00      0.00      0.00       116\n          Stone-Steel-Towers       1.00      0.04      0.07        28\n\n                    accuracy                           0.58      6308\n                   macro avg       0.18      0.12      0.10      6308\n                weighted avg       0.43      0.58      0.47      6308\n\nImpossible de créer la visualisation du F1-score par classe - données insuffisantes\n\nModèle sauvegardé sous 'resultats_top5_deflection/model_top5_deflection.h5'\n\n========== MODÈLE TOP 10 BANDES ==========\nNombre de classes (avec background): 17\nDimensions des données: (21025, 10)\nEnsemble d'entraînement: (14717, 10), (14717, 17)\nEnsemble de test: (6308, 10), (6308, 17)\n\nCréation et entraînement du modèle top10_deflection...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_1\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │           \u001b[38;5;34m5,632\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_4                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │           \u001b[38;5;34m2,048\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_4 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │          \u001b[38;5;34m32,832\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_5                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │             \u001b[38;5;34m256\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_5 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │           \u001b[38;5;34m4,160\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_6                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │             \u001b[38;5;34m256\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_6 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │           \u001b[38;5;34m2,080\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_7                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │             \u001b[38;5;34m128\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_7 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m)                  │             \u001b[38;5;34m561\u001b[0m │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">5,632</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_4                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">32,832</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_5                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_6                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_7                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">561</span> │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m47,953\u001b[0m (187.32 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">47,953</span> (187.32 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m46,609\u001b[0m (182.07 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">46,609</span> (182.07 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,344\u001b[0m (5.25 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,344</span> (5.25 KB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - accuracy: 0.4875 - loss: 1.8012 - val_accuracy: 0.5720 - val_loss: 1.3322\nEpoch 2/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5714 - loss: 1.2719 - val_accuracy: 0.5819 - val_loss: 1.1757\nEpoch 3/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5888 - loss: 1.1927 - val_accuracy: 0.5764 - val_loss: 1.1679\nEpoch 4/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5846 - loss: 1.1990 - val_accuracy: 0.5883 - val_loss: 1.1560\nEpoch 5/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5950 - loss: 1.1749 - val_accuracy: 0.6097 - val_loss: 1.1091\nEpoch 6/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5860 - loss: 1.1665 - val_accuracy: 0.5910 - val_loss: 1.1679\nEpoch 7/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5902 - loss: 1.1554 - val_accuracy: 0.6097 - val_loss: 1.0940\nEpoch 8/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5916 - loss: 1.1499 - val_accuracy: 0.6230 - val_loss: 1.0558\nEpoch 9/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5986 - loss: 1.1169 - val_accuracy: 0.6131 - val_loss: 1.0990\nEpoch 10/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6022 - loss: 1.1174 - val_accuracy: 0.6002 - val_loss: 1.0738\nEpoch 11/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5893 - loss: 1.1196 - val_accuracy: 0.6104 - val_loss: 1.0519\nEpoch 12/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5988 - loss: 1.1095 - val_accuracy: 0.6005 - val_loss: 1.1077\nEpoch 13/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5908 - loss: 1.1374 - val_accuracy: 0.6158 - val_loss: 1.0406\nEpoch 14/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6043 - loss: 1.1073 - val_accuracy: 0.6056 - val_loss: 1.0571\nEpoch 15/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6091 - loss: 1.1094 - val_accuracy: 0.6264 - val_loss: 1.0256\nEpoch 16/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6025 - loss: 1.1067 - val_accuracy: 0.6111 - val_loss: 1.0590\nEpoch 17/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5966 - loss: 1.1012 - val_accuracy: 0.6070 - val_loss: 1.0859\nEpoch 18/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5987 - loss: 1.0947 - val_accuracy: 0.6206 - val_loss: 1.0597\nEpoch 19/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6039 - loss: 1.0896 - val_accuracy: 0.6287 - val_loss: 1.0449\nEpoch 20/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6085 - loss: 1.0827 - val_accuracy: 0.6236 - val_loss: 1.0337\nEpoch 21/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6054 - loss: 1.0796 - val_accuracy: 0.6223 - val_loss: 1.0527\nEpoch 22/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6012 - loss: 1.0716 - val_accuracy: 0.6189 - val_loss: 1.0536\nEpoch 23/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5994 - loss: 1.0894 - val_accuracy: 0.6226 - val_loss: 1.0264\nEpoch 24/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6100 - loss: 1.0638 - val_accuracy: 0.6223 - val_loss: 1.0208\nEpoch 25/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6040 - loss: 1.0751 - val_accuracy: 0.6236 - val_loss: 1.0390\nEpoch 26/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6110 - loss: 1.0556 - val_accuracy: 0.6101 - val_loss: 1.0451\nEpoch 27/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5941 - loss: 1.0827 - val_accuracy: 0.6196 - val_loss: 1.0349\nEpoch 28/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6074 - loss: 1.0736 - val_accuracy: 0.6185 - val_loss: 1.0250\nEpoch 29/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6048 - loss: 1.0803 - val_accuracy: 0.6131 - val_loss: 1.0387\nEpoch 30/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6109 - loss: 1.0628 - val_accuracy: 0.6264 - val_loss: 1.0089\nEpoch 31/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6066 - loss: 1.0634 - val_accuracy: 0.6230 - val_loss: 1.0259\nEpoch 32/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6106 - loss: 1.0538 - val_accuracy: 0.6230 - val_loss: 1.0187\nEpoch 33/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6152 - loss: 1.0572 - val_accuracy: 0.6060 - val_loss: 1.0733\nEpoch 34/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6111 - loss: 1.0666 - val_accuracy: 0.6226 - val_loss: 1.0120\nEpoch 35/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6153 - loss: 1.0414 - val_accuracy: 0.6226 - val_loss: 1.0161\nEpoch 36/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6156 - loss: 1.0523 - val_accuracy: 0.6135 - val_loss: 1.0310\nEpoch 37/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6088 - loss: 1.0654 - val_accuracy: 0.6138 - val_loss: 1.0530\nEpoch 38/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6084 - loss: 1.0601 - val_accuracy: 0.6223 - val_loss: 1.0308\nEpoch 39/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6077 - loss: 1.0434 - val_accuracy: 0.6196 - val_loss: 1.0382\nEpoch 40/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6147 - loss: 1.0503 - val_accuracy: 0.6060 - val_loss: 1.0234\nEpoch 41/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6112 - loss: 1.0587 - val_accuracy: 0.6253 - val_loss: 1.0157\nEpoch 42/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5997 - loss: 1.0537 - val_accuracy: 0.6321 - val_loss: 1.0058\nEpoch 43/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6132 - loss: 1.0555 - val_accuracy: 0.6321 - val_loss: 1.0000\nEpoch 44/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6029 - loss: 1.0506 - val_accuracy: 0.6094 - val_loss: 1.0488\nEpoch 45/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6056 - loss: 1.0538 - val_accuracy: 0.6213 - val_loss: 0.9968\nEpoch 46/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6214 - loss: 1.0318 - val_accuracy: 0.6328 - val_loss: 0.9949\nEpoch 47/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6120 - loss: 1.0404 - val_accuracy: 0.6270 - val_loss: 0.9922\nEpoch 48/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6031 - loss: 1.0527 - val_accuracy: 0.6216 - val_loss: 1.0164\nEpoch 49/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6087 - loss: 1.0492 - val_accuracy: 0.6270 - val_loss: 0.9948\nEpoch 50/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6166 - loss: 1.0305 - val_accuracy: 0.6223 - val_loss: 1.0000\nEpoch 51/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6117 - loss: 1.0481 - val_accuracy: 0.6281 - val_loss: 1.0131\nEpoch 52/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6175 - loss: 1.0229 - val_accuracy: 0.6315 - val_loss: 1.0297\nEpoch 53/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6124 - loss: 1.0217 - val_accuracy: 0.6128 - val_loss: 1.0376\nEpoch 54/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6185 - loss: 1.0352 - val_accuracy: 0.6318 - val_loss: 0.9904\nEpoch 55/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6101 - loss: 1.0315 - val_accuracy: 0.6253 - val_loss: 0.9925\nEpoch 56/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6056 - loss: 1.0375 - val_accuracy: 0.5859 - val_loss: 1.0718\nEpoch 57/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6149 - loss: 1.0284 - val_accuracy: 0.6257 - val_loss: 0.9855\nEpoch 58/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6086 - loss: 1.0352 - val_accuracy: 0.6124 - val_loss: 1.0403\nEpoch 59/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6090 - loss: 1.0350 - val_accuracy: 0.6114 - val_loss: 1.0300\nEpoch 60/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6137 - loss: 1.0374 - val_accuracy: 0.6277 - val_loss: 0.9947\nEpoch 61/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6129 - loss: 1.0225 - val_accuracy: 0.6413 - val_loss: 0.9780\nEpoch 62/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6113 - loss: 1.0308 - val_accuracy: 0.6223 - val_loss: 1.0350\nEpoch 63/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6118 - loss: 1.0395 - val_accuracy: 0.6294 - val_loss: 0.9970\nEpoch 64/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6209 - loss: 1.0208 - val_accuracy: 0.6199 - val_loss: 1.0410\nEpoch 65/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6251 - loss: 1.0034 - val_accuracy: 0.6311 - val_loss: 1.0279\nEpoch 66/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6094 - loss: 1.0347 - val_accuracy: 0.6321 - val_loss: 0.9915\nEpoch 67/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6220 - loss: 1.0234 - val_accuracy: 0.6270 - val_loss: 1.0091\nEpoch 68/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6224 - loss: 1.0240 - val_accuracy: 0.6298 - val_loss: 0.9838\nEpoch 69/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6102 - loss: 1.0294 - val_accuracy: 0.6321 - val_loss: 0.9765\nEpoch 70/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6108 - loss: 1.0377 - val_accuracy: 0.6308 - val_loss: 0.9812\nEpoch 71/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6197 - loss: 1.0256 - val_accuracy: 0.6250 - val_loss: 0.9871\nEpoch 72/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6103 - loss: 1.0368 - val_accuracy: 0.6311 - val_loss: 0.9802\nEpoch 73/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6176 - loss: 1.0277 - val_accuracy: 0.6325 - val_loss: 0.9971\nEpoch 74/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6064 - loss: 1.0393 - val_accuracy: 0.6355 - val_loss: 0.9857\nEpoch 75/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6220 - loss: 1.0155 - val_accuracy: 0.6321 - val_loss: 0.9973\nEpoch 76/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6233 - loss: 1.0233 - val_accuracy: 0.6311 - val_loss: 0.9920\nEpoch 77/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6177 - loss: 1.0285 - val_accuracy: 0.6355 - val_loss: 0.9796\nEpoch 78/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6212 - loss: 1.0181 - val_accuracy: 0.6321 - val_loss: 0.9928\nEpoch 79/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6248 - loss: 1.0164 - val_accuracy: 0.6376 - val_loss: 0.9692\nEpoch 80/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6064 - loss: 1.0409 - val_accuracy: 0.6359 - val_loss: 0.9605\nEpoch 81/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6234 - loss: 1.0088 - val_accuracy: 0.6403 - val_loss: 0.9805\nEpoch 82/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6109 - loss: 1.0306 - val_accuracy: 0.6145 - val_loss: 1.0203\nEpoch 83/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6165 - loss: 1.0248 - val_accuracy: 0.6335 - val_loss: 0.9787\nEpoch 84/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6220 - loss: 1.0084 - val_accuracy: 0.6294 - val_loss: 0.9962\nEpoch 85/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6141 - loss: 1.0230 - val_accuracy: 0.6046 - val_loss: 1.0402\nEpoch 86/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6180 - loss: 1.0096 - val_accuracy: 0.6267 - val_loss: 0.9867\nEpoch 87/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6193 - loss: 0.9986 - val_accuracy: 0.6308 - val_loss: 0.9746\nEpoch 88/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6165 - loss: 1.0271 - val_accuracy: 0.6349 - val_loss: 1.0030\nEpoch 89/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6134 - loss: 1.0207 - val_accuracy: 0.6352 - val_loss: 0.9736\nEpoch 90/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6202 - loss: 1.0135 - val_accuracy: 0.6369 - val_loss: 0.9777\nEpoch 91/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6178 - loss: 1.0130 - val_accuracy: 0.6328 - val_loss: 0.9951\nEpoch 92/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6211 - loss: 1.0202 - val_accuracy: 0.6338 - val_loss: 0.9753\nEpoch 93/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6110 - loss: 1.0315 - val_accuracy: 0.6355 - val_loss: 0.9711\nEpoch 94/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6229 - loss: 1.0172 - val_accuracy: 0.6318 - val_loss: 1.0033\nEpoch 95/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6139 - loss: 1.0296 - val_accuracy: 0.6338 - val_loss: 0.9767\nEpoch 95: early stopping\nRestoring model weights from the end of the best epoch: 80.\n\nTemps d'entraînement: 82.24 secondes\n\nÉvaluation du modèle sur l'ensemble de test...\nPrécision (accuracy): 0.6398\nTemps de prédiction: 1.02 secondes\n\nRapport de classification:\n                              precision    recall  f1-score   support\n\n                  Background       0.72      0.88      0.79      3233\n                     Alfalfa       0.00      0.00      0.00        14\n                 Corn-notill       0.50      0.34      0.40       428\n                Corn-mintill       0.48      0.36      0.41       249\n                        Corn       0.38      0.04      0.08        71\n               Grass-pasture       1.00      0.01      0.01       145\n                 Grass-trees       0.61      0.29      0.39       219\n         Grass-pasture-mowed       0.50      0.12      0.20         8\n               Hay-windrowed       0.66      0.90      0.76       143\n                        Oats       0.00      0.00      0.00         6\n              Soybean-notill       0.55      0.39      0.46       292\n             Soybean-mintill       0.47      0.80      0.59       737\n               Soybean-clean       0.45      0.26      0.33       178\n                       Wheat       0.00      0.00      0.00        61\n                       Woods       0.00      0.00      0.00       380\nBuildings-Grass-Trees-Drives       0.00      0.00      0.00       116\n          Stone-Steel-Towers       0.67      0.79      0.72        28\n\n                    accuracy                           0.64      6308\n                   macro avg       0.41      0.30      0.30      6308\n                weighted avg       0.58      0.64      0.58      6308\n\nImpossible de créer la visualisation du F1-score par classe - données insuffisantes\n\nModèle sauvegardé sous 'resultats_top10_deflection/model_top10_deflection.h5'\n\n========== MODÈLE TOP 15 BANDES ==========\nNombre de classes (avec background): 17\nDimensions des données: (21025, 15)\nEnsemble d'entraînement: (14717, 15), (14717, 17)\nEnsemble de test: (6308, 15), (6308, 17)\n\nCréation et entraînement du modèle top15_deflection...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_2\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │           \u001b[38;5;34m8,192\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_8                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │           \u001b[38;5;34m2,048\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_8 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │          \u001b[38;5;34m32,832\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_9                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │             \u001b[38;5;34m256\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_9 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │           \u001b[38;5;34m4,160\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_10               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │             \u001b[38;5;34m256\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_10 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_13 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │           \u001b[38;5;34m2,080\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_11               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │             \u001b[38;5;34m128\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_11 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_14 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m)                  │             \u001b[38;5;34m561\u001b[0m │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,192</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_8                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">32,832</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_9                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_10               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_11               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">561</span> │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m50,513\u001b[0m (197.32 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">50,513</span> (197.32 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m49,169\u001b[0m (192.07 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">49,169</span> (192.07 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,344\u001b[0m (5.25 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,344</span> (5.25 KB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9ms/step - accuracy: 0.4862 - loss: 1.8599 - val_accuracy: 0.5744 - val_loss: 1.2735\nEpoch 2/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5605 - loss: 1.2900 - val_accuracy: 0.6084 - val_loss: 1.1587\nEpoch 3/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5841 - loss: 1.2024 - val_accuracy: 0.5951 - val_loss: 1.1216\nEpoch 4/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5963 - loss: 1.1622 - val_accuracy: 0.5985 - val_loss: 1.1124\nEpoch 5/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5902 - loss: 1.1572 - val_accuracy: 0.6056 - val_loss: 1.0916\nEpoch 6/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5864 - loss: 1.1588 - val_accuracy: 0.6124 - val_loss: 1.0727\nEpoch 7/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5949 - loss: 1.1362 - val_accuracy: 0.6053 - val_loss: 1.0751\nEpoch 8/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5916 - loss: 1.1306 - val_accuracy: 0.6080 - val_loss: 1.0646\nEpoch 9/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6015 - loss: 1.1028 - val_accuracy: 0.5924 - val_loss: 1.0774\nEpoch 10/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5972 - loss: 1.1114 - val_accuracy: 0.6165 - val_loss: 1.0324\nEpoch 11/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6084 - loss: 1.0943 - val_accuracy: 0.6230 - val_loss: 1.0388\nEpoch 12/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6106 - loss: 1.0717 - val_accuracy: 0.5978 - val_loss: 1.0568\nEpoch 13/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6063 - loss: 1.0799 - val_accuracy: 0.6141 - val_loss: 1.0437\nEpoch 14/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5995 - loss: 1.0848 - val_accuracy: 0.6274 - val_loss: 1.0155\nEpoch 15/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6099 - loss: 1.0881 - val_accuracy: 0.5842 - val_loss: 1.0722\nEpoch 16/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6068 - loss: 1.0700 - val_accuracy: 0.6080 - val_loss: 1.1393\nEpoch 17/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6098 - loss: 1.0705 - val_accuracy: 0.6138 - val_loss: 1.0266\nEpoch 18/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6137 - loss: 1.0551 - val_accuracy: 0.6077 - val_loss: 1.0733\nEpoch 19/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6127 - loss: 1.0584 - val_accuracy: 0.5754 - val_loss: 1.1541\nEpoch 20/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6139 - loss: 1.0637 - val_accuracy: 0.6155 - val_loss: 1.0022\nEpoch 21/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6141 - loss: 1.0443 - val_accuracy: 0.6226 - val_loss: 0.9973\nEpoch 22/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6120 - loss: 1.0501 - val_accuracy: 0.5652 - val_loss: 1.1260\nEpoch 23/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6018 - loss: 1.0659 - val_accuracy: 0.6189 - val_loss: 1.0325\nEpoch 24/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6150 - loss: 1.0460 - val_accuracy: 0.6111 - val_loss: 1.0037\nEpoch 25/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6136 - loss: 1.0450 - val_accuracy: 0.6131 - val_loss: 1.0013\nEpoch 26/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6167 - loss: 1.0512 - val_accuracy: 0.6233 - val_loss: 1.0117\nEpoch 27/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6279 - loss: 1.0170 - val_accuracy: 0.6345 - val_loss: 0.9794\nEpoch 28/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6053 - loss: 1.0510 - val_accuracy: 0.6338 - val_loss: 0.9763\nEpoch 29/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6178 - loss: 1.0258 - val_accuracy: 0.6121 - val_loss: 1.0319\nEpoch 30/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6074 - loss: 1.0456 - val_accuracy: 0.6260 - val_loss: 1.0254\nEpoch 31/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6147 - loss: 1.0334 - val_accuracy: 0.6287 - val_loss: 0.9981\nEpoch 32/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6089 - loss: 1.0370 - val_accuracy: 0.6365 - val_loss: 0.9757\nEpoch 33/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6123 - loss: 1.0313 - val_accuracy: 0.6359 - val_loss: 0.9927\nEpoch 34/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6121 - loss: 1.0435 - val_accuracy: 0.6311 - val_loss: 0.9959\nEpoch 35/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6152 - loss: 1.0328 - val_accuracy: 0.6318 - val_loss: 0.9856\nEpoch 36/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6114 - loss: 1.0314 - val_accuracy: 0.6321 - val_loss: 0.9772\nEpoch 37/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6105 - loss: 1.0453 - val_accuracy: 0.6294 - val_loss: 0.9883\nEpoch 38/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6150 - loss: 1.0309 - val_accuracy: 0.6209 - val_loss: 1.0444\nEpoch 39/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6252 - loss: 1.0156 - val_accuracy: 0.6182 - val_loss: 0.9963\nEpoch 40/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6175 - loss: 1.0051 - val_accuracy: 0.6298 - val_loss: 1.0114\nEpoch 41/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6236 - loss: 1.0197 - val_accuracy: 0.6298 - val_loss: 0.9915\nEpoch 42/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6210 - loss: 1.0175 - val_accuracy: 0.6097 - val_loss: 1.0565\nEpoch 43/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6151 - loss: 1.0132 - val_accuracy: 0.6328 - val_loss: 0.9866\nEpoch 44/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6177 - loss: 1.0159 - val_accuracy: 0.6321 - val_loss: 1.0561\nEpoch 45/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6141 - loss: 1.0185 - val_accuracy: 0.6355 - val_loss: 0.9660\nEpoch 46/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6220 - loss: 1.0113 - val_accuracy: 0.6325 - val_loss: 0.9923\nEpoch 47/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6272 - loss: 0.9998 - val_accuracy: 0.6335 - val_loss: 0.9801\nEpoch 48/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6208 - loss: 1.0074 - val_accuracy: 0.6213 - val_loss: 0.9999\nEpoch 49/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6174 - loss: 1.0088 - val_accuracy: 0.6145 - val_loss: 1.0280\nEpoch 50/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6169 - loss: 1.0281 - val_accuracy: 0.6372 - val_loss: 0.9594\nEpoch 51/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6230 - loss: 1.0066 - val_accuracy: 0.6240 - val_loss: 0.9967\nEpoch 52/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6238 - loss: 0.9999 - val_accuracy: 0.6308 - val_loss: 0.9858\nEpoch 53/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6221 - loss: 1.0054 - val_accuracy: 0.6321 - val_loss: 0.9687\nEpoch 54/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6087 - loss: 1.0140 - val_accuracy: 0.6318 - val_loss: 0.9639\nEpoch 55/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6177 - loss: 1.0085 - val_accuracy: 0.6287 - val_loss: 0.9904\nEpoch 56/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6193 - loss: 1.0199 - val_accuracy: 0.6287 - val_loss: 0.9792\nEpoch 57/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6190 - loss: 0.9997 - val_accuracy: 0.6335 - val_loss: 0.9712\nEpoch 58/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6221 - loss: 1.0001 - val_accuracy: 0.6349 - val_loss: 0.9907\nEpoch 59/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6231 - loss: 0.9938 - val_accuracy: 0.6104 - val_loss: 1.0187\nEpoch 60/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6304 - loss: 0.9788 - val_accuracy: 0.6376 - val_loss: 0.9617\nEpoch 61/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6252 - loss: 0.9977 - val_accuracy: 0.6318 - val_loss: 0.9696\nEpoch 62/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6202 - loss: 1.0074 - val_accuracy: 0.6369 - val_loss: 0.9819\nEpoch 63/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6208 - loss: 0.9920 - val_accuracy: 0.6124 - val_loss: 1.0516\nEpoch 64/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6246 - loss: 1.0012 - val_accuracy: 0.6202 - val_loss: 1.0032\nEpoch 65/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6237 - loss: 0.9804 - val_accuracy: 0.6199 - val_loss: 0.9832\nEpoch 65: early stopping\nRestoring model weights from the end of the best epoch: 50.\n\nTemps d'entraînement: 58.98 secondes\n\nÉvaluation du modèle sur l'ensemble de test...\nPrécision (accuracy): 0.6489\nTemps de prédiction: 1.01 secondes\n\nRapport de classification:\n                              precision    recall  f1-score   support\n\n                  Background       0.70      0.90      0.79      3233\n                     Alfalfa       0.00      0.00      0.00        14\n                 Corn-notill       0.51      0.37      0.43       428\n                Corn-mintill       0.62      0.24      0.35       249\n                        Corn       0.67      0.03      0.05        71\n               Grass-pasture       0.89      0.06      0.10       145\n                 Grass-trees       0.63      0.26      0.37       219\n         Grass-pasture-mowed       0.00      0.00      0.00         8\n               Hay-windrowed       0.68      0.93      0.78       143\n                        Oats       0.00      0.00      0.00         6\n              Soybean-notill       0.62      0.38      0.47       292\n             Soybean-mintill       0.51      0.75      0.61       737\n               Soybean-clean       0.40      0.30      0.34       178\n                       Wheat       0.64      0.15      0.24        61\n                       Woods       0.00      0.00      0.00       380\nBuildings-Grass-Trees-Drives       0.00      0.00      0.00       116\n          Stone-Steel-Towers       0.75      0.75      0.75        28\n\n                    accuracy                           0.65      6308\n                   macro avg       0.45      0.30      0.31      6308\n                weighted avg       0.59      0.65      0.59      6308\n\nImpossible de créer la visualisation du F1-score par classe - données insuffisantes\n\nModèle sauvegardé sous 'resultats_top15_deflection/model_top15_deflection.h5'\n\n========== MODÈLE TOP 20 BANDES ==========\nNombre de classes (avec background): 17\nDimensions des données: (21025, 20)\nEnsemble d'entraînement: (14717, 20), (14717, 17)\nEnsemble de test: (6308, 20), (6308, 17)\n\nCréation et entraînement du modèle top20_deflection...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_3\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ input_layer_3 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_15 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │          \u001b[38;5;34m10,752\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_12               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │           \u001b[38;5;34m2,048\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_12 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_16 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │          \u001b[38;5;34m32,832\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_13               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │             \u001b[38;5;34m256\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_13 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_17 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │           \u001b[38;5;34m4,160\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_14               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │             \u001b[38;5;34m256\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_14 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_18 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │           \u001b[38;5;34m2,080\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_15               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │             \u001b[38;5;34m128\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_15 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_19 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m)                  │             \u001b[38;5;34m561\u001b[0m │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ input_layer_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">10,752</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_12               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">32,832</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_13               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_14               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_15               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">561</span> │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m53,073\u001b[0m (207.32 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">53,073</span> (207.32 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m51,729\u001b[0m (202.07 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">51,729</span> (202.07 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,344\u001b[0m (5.25 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,344</span> (5.25 KB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - accuracy: 0.4458 - loss: 2.0256 - val_accuracy: 0.6182 - val_loss: 1.1210\nEpoch 2/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5948 - loss: 1.1619 - val_accuracy: 0.6253 - val_loss: 1.0084\nEpoch 3/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6121 - loss: 1.0733 - val_accuracy: 0.6471 - val_loss: 0.9501\nEpoch 4/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6167 - loss: 1.0573 - val_accuracy: 0.6559 - val_loss: 0.9464\nEpoch 5/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6354 - loss: 0.9920 - val_accuracy: 0.6450 - val_loss: 0.9386\nEpoch 6/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6350 - loss: 0.9813 - val_accuracy: 0.6644 - val_loss: 0.8891\nEpoch 7/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6495 - loss: 0.9675 - val_accuracy: 0.6695 - val_loss: 0.8818\nEpoch 8/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6483 - loss: 0.9458 - val_accuracy: 0.6111 - val_loss: 0.9955\nEpoch 9/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6352 - loss: 0.9591 - val_accuracy: 0.6372 - val_loss: 0.9560\nEpoch 10/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6545 - loss: 0.9302 - val_accuracy: 0.6858 - val_loss: 0.8295\nEpoch 11/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6534 - loss: 0.9229 - val_accuracy: 0.6923 - val_loss: 0.8272\nEpoch 12/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6614 - loss: 0.9054 - val_accuracy: 0.6773 - val_loss: 0.8379\nEpoch 13/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6516 - loss: 0.9331 - val_accuracy: 0.6997 - val_loss: 0.8143\nEpoch 14/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6572 - loss: 0.9038 - val_accuracy: 0.6936 - val_loss: 0.8164\nEpoch 15/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6613 - loss: 0.8935 - val_accuracy: 0.7130 - val_loss: 0.8082\nEpoch 16/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6672 - loss: 0.8840 - val_accuracy: 0.7001 - val_loss: 0.7925\nEpoch 17/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6795 - loss: 0.8619 - val_accuracy: 0.7038 - val_loss: 0.8110\nEpoch 18/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6672 - loss: 0.8765 - val_accuracy: 0.6926 - val_loss: 0.8188\nEpoch 19/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6860 - loss: 0.8473 - val_accuracy: 0.6895 - val_loss: 0.8133\nEpoch 20/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6707 - loss: 0.8745 - val_accuracy: 0.7075 - val_loss: 0.7623\nEpoch 21/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6841 - loss: 0.8433 - val_accuracy: 0.7041 - val_loss: 0.7904\nEpoch 22/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6731 - loss: 0.8645 - val_accuracy: 0.6953 - val_loss: 0.8224\nEpoch 23/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6753 - loss: 0.8623 - val_accuracy: 0.6919 - val_loss: 0.8129\nEpoch 24/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6777 - loss: 0.8574 - val_accuracy: 0.7120 - val_loss: 0.7585\nEpoch 25/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6773 - loss: 0.8606 - val_accuracy: 0.7014 - val_loss: 0.7784\nEpoch 26/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6766 - loss: 0.8558 - val_accuracy: 0.7215 - val_loss: 0.7530\nEpoch 27/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6750 - loss: 0.8451 - val_accuracy: 0.7126 - val_loss: 0.7641\nEpoch 28/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6900 - loss: 0.8148 - val_accuracy: 0.7194 - val_loss: 0.7631\nEpoch 29/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6778 - loss: 0.8342 - val_accuracy: 0.7096 - val_loss: 0.7607\nEpoch 30/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6819 - loss: 0.8387 - val_accuracy: 0.7137 - val_loss: 0.7583\nEpoch 31/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6941 - loss: 0.8255 - val_accuracy: 0.6848 - val_loss: 0.8322\nEpoch 32/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6812 - loss: 0.8463 - val_accuracy: 0.7140 - val_loss: 0.7652\nEpoch 33/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6936 - loss: 0.8178 - val_accuracy: 0.7167 - val_loss: 0.7544\nEpoch 34/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6919 - loss: 0.8086 - val_accuracy: 0.7113 - val_loss: 0.7755\nEpoch 35/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6985 - loss: 0.8073 - val_accuracy: 0.7035 - val_loss: 0.7787\nEpoch 36/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6789 - loss: 0.8236 - val_accuracy: 0.7140 - val_loss: 0.7472\nEpoch 37/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6808 - loss: 0.8287 - val_accuracy: 0.7086 - val_loss: 0.7626\nEpoch 38/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6923 - loss: 0.8103 - val_accuracy: 0.7130 - val_loss: 0.7795\nEpoch 39/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6859 - loss: 0.8181 - val_accuracy: 0.7300 - val_loss: 0.7287\nEpoch 40/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6932 - loss: 0.8018 - val_accuracy: 0.7238 - val_loss: 0.7600\nEpoch 41/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6963 - loss: 0.8026 - val_accuracy: 0.7235 - val_loss: 0.7413\nEpoch 42/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6908 - loss: 0.7981 - val_accuracy: 0.7021 - val_loss: 0.7872\nEpoch 43/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6929 - loss: 0.8048 - val_accuracy: 0.7300 - val_loss: 0.7232\nEpoch 44/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6983 - loss: 0.8129 - val_accuracy: 0.7177 - val_loss: 0.7420\nEpoch 45/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6969 - loss: 0.8089 - val_accuracy: 0.6834 - val_loss: 0.8284\nEpoch 46/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6984 - loss: 0.8023 - val_accuracy: 0.7177 - val_loss: 0.7464\nEpoch 47/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7008 - loss: 0.7941 - val_accuracy: 0.7351 - val_loss: 0.7224\nEpoch 48/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7007 - loss: 0.7918 - val_accuracy: 0.7048 - val_loss: 0.7695\nEpoch 49/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6942 - loss: 0.7927 - val_accuracy: 0.7096 - val_loss: 0.7776\nEpoch 50/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6967 - loss: 0.7904 - val_accuracy: 0.7289 - val_loss: 0.7221\nEpoch 51/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6923 - loss: 0.8057 - val_accuracy: 0.6814 - val_loss: 0.8060\nEpoch 52/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7034 - loss: 0.7901 - val_accuracy: 0.7259 - val_loss: 0.7339\nEpoch 53/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6867 - loss: 0.8023 - val_accuracy: 0.7221 - val_loss: 0.7459\nEpoch 54/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6980 - loss: 0.8013 - val_accuracy: 0.7283 - val_loss: 0.7216\nEpoch 55/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7018 - loss: 0.7806 - val_accuracy: 0.7252 - val_loss: 0.7226\nEpoch 56/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6906 - loss: 0.7954 - val_accuracy: 0.7283 - val_loss: 0.7377\nEpoch 57/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6987 - loss: 0.7976 - val_accuracy: 0.7259 - val_loss: 0.7304\nEpoch 58/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6974 - loss: 0.7868 - val_accuracy: 0.7283 - val_loss: 0.7270\nEpoch 59/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7078 - loss: 0.7632 - val_accuracy: 0.7218 - val_loss: 0.7349\nEpoch 60/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7031 - loss: 0.7675 - val_accuracy: 0.7191 - val_loss: 0.7477\nEpoch 61/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6983 - loss: 0.7858 - val_accuracy: 0.7113 - val_loss: 0.7849\nEpoch 62/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7054 - loss: 0.7850 - val_accuracy: 0.7062 - val_loss: 0.7703\nEpoch 63/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7024 - loss: 0.7834 - val_accuracy: 0.7259 - val_loss: 0.7199\nEpoch 64/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7024 - loss: 0.7656 - val_accuracy: 0.7188 - val_loss: 0.7445\nEpoch 65/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6953 - loss: 0.7701 - val_accuracy: 0.7266 - val_loss: 0.7244\nEpoch 66/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7018 - loss: 0.7652 - val_accuracy: 0.7198 - val_loss: 0.7514\nEpoch 67/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7071 - loss: 0.7673 - val_accuracy: 0.7249 - val_loss: 0.7226\nEpoch 68/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7010 - loss: 0.7780 - val_accuracy: 0.7300 - val_loss: 0.7198\nEpoch 69/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7071 - loss: 0.7804 - val_accuracy: 0.7327 - val_loss: 0.7259\nEpoch 70/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7033 - loss: 0.7750 - val_accuracy: 0.7286 - val_loss: 0.7174\nEpoch 71/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7006 - loss: 0.7842 - val_accuracy: 0.7412 - val_loss: 0.6972\nEpoch 72/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7024 - loss: 0.7778 - val_accuracy: 0.7194 - val_loss: 0.7220\nEpoch 73/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7076 - loss: 0.7693 - val_accuracy: 0.7283 - val_loss: 0.7260\nEpoch 74/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7007 - loss: 0.7587 - val_accuracy: 0.7255 - val_loss: 0.7106\nEpoch 75/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7087 - loss: 0.7566 - val_accuracy: 0.7249 - val_loss: 0.7223\nEpoch 76/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7185 - loss: 0.7451 - val_accuracy: 0.7408 - val_loss: 0.6935\nEpoch 77/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7117 - loss: 0.7524 - val_accuracy: 0.7317 - val_loss: 0.7062\nEpoch 78/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7071 - loss: 0.7675 - val_accuracy: 0.7191 - val_loss: 0.7311\nEpoch 79/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7086 - loss: 0.7514 - val_accuracy: 0.7354 - val_loss: 0.6941\nEpoch 80/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7093 - loss: 0.7689 - val_accuracy: 0.7266 - val_loss: 0.7339\nEpoch 81/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7078 - loss: 0.7614 - val_accuracy: 0.7371 - val_loss: 0.6950\nEpoch 82/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7007 - loss: 0.7723 - val_accuracy: 0.7391 - val_loss: 0.7085\nEpoch 83/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7098 - loss: 0.7565 - val_accuracy: 0.7191 - val_loss: 0.7485\nEpoch 84/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7084 - loss: 0.7581 - val_accuracy: 0.7432 - val_loss: 0.7002\nEpoch 85/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7105 - loss: 0.7541 - val_accuracy: 0.7221 - val_loss: 0.7481\nEpoch 86/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7089 - loss: 0.7547 - val_accuracy: 0.7398 - val_loss: 0.6984\nEpoch 87/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7025 - loss: 0.7699 - val_accuracy: 0.7208 - val_loss: 0.7221\nEpoch 88/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7034 - loss: 0.7647 - val_accuracy: 0.7408 - val_loss: 0.6870\nEpoch 89/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7041 - loss: 0.7534 - val_accuracy: 0.7296 - val_loss: 0.7059\nEpoch 90/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7125 - loss: 0.7488 - val_accuracy: 0.6980 - val_loss: 0.7834\nEpoch 91/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7127 - loss: 0.7586 - val_accuracy: 0.7391 - val_loss: 0.6839\nEpoch 92/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7156 - loss: 0.7489 - val_accuracy: 0.7395 - val_loss: 0.6869\nEpoch 93/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7059 - loss: 0.7634 - val_accuracy: 0.7371 - val_loss: 0.6798\nEpoch 94/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7136 - loss: 0.7490 - val_accuracy: 0.7334 - val_loss: 0.6856\nEpoch 95/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7134 - loss: 0.7406 - val_accuracy: 0.7089 - val_loss: 0.7791\nEpoch 96/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7109 - loss: 0.7383 - val_accuracy: 0.7357 - val_loss: 0.7020\nEpoch 97/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7136 - loss: 0.7558 - val_accuracy: 0.7157 - val_loss: 0.7339\nEpoch 98/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7196 - loss: 0.7411 - val_accuracy: 0.7303 - val_loss: 0.7046\nEpoch 99/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7090 - loss: 0.7586 - val_accuracy: 0.7276 - val_loss: 0.7127\nEpoch 100/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7126 - loss: 0.7471 - val_accuracy: 0.7391 - val_loss: 0.7072\nRestoring model weights from the end of the best epoch: 93.\n\nTemps d'entraînement: 86.75 secondes\n\nÉvaluation du modèle sur l'ensemble de test...\nPrécision (accuracy): 0.7535\nTemps de prédiction: 1.04 secondes\n\nRapport de classification:\n                              precision    recall  f1-score   support\n\n                  Background       0.78      0.88      0.83      3233\n                     Alfalfa       0.67      0.57      0.62        14\n                 Corn-notill       0.67      0.63      0.65       428\n                Corn-mintill       0.69      0.61      0.65       249\n                        Corn       0.58      0.79      0.67        71\n               Grass-pasture       0.91      0.68      0.78       145\n                 Grass-trees       0.74      0.73      0.74       219\n         Grass-pasture-mowed       0.78      0.88      0.82         8\n               Hay-windrowed       0.85      0.96      0.90       143\n                        Oats       0.00      0.00      0.00         6\n              Soybean-notill       0.71      0.78      0.75       292\n             Soybean-mintill       0.71      0.75      0.73       737\n               Soybean-clean       0.60      0.61      0.60       178\n                       Wheat       0.73      0.95      0.83        61\n                       Woods       0.74      0.09      0.16       380\nBuildings-Grass-Trees-Drives       0.00      0.00      0.00       116\n          Stone-Steel-Towers       0.75      0.86      0.80        28\n\n                    accuracy                           0.75      6308\n                   macro avg       0.64      0.63      0.62      6308\n                weighted avg       0.74      0.75      0.73      6308\n\nImpossible de créer la visualisation du F1-score par classe - données insuffisantes\n\nModèle sauvegardé sous 'resultats_top20_deflection/model_top20_deflection.h5'\n\nTableau de comparaison sauvegardé dans 'resultats_comparaison/comparaison_modeles_deflection.csv'\n\nAnalyse comparative terminée!\n\nRécapitulatif des précisions:\nTop 5 bandes: 0.5770\nTop 10 bandes: 0.6398\nTop 15 bandes: 0.6489\nTop 20 bandes: 0.7535\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"### Étape 9: Classification multiclasse avec MLP en utilisant des bandes discriminantes par segments égaux","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, BatchNormalization, Input, Dropout, Activation\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.utils import to_categorical\nimport os\nfrom scipy.io import loadmat\nfrom tqdm import tqdm\n\n# Définir explicitement les noms des classes\nclass_names_custom = [\n    'Background',\n    'Alfalfa',\n    'Corn-notill',\n    'Corn-mintill',\n    'Corn',\n    'Grass-pasture',\n    'Grass-trees',\n    'Grass-pasture-mowed',\n    'Hay-windrowed',\n    'Oats',\n    'Soybean-notill',\n    'Soybean-mintill',\n    'Soybean-clean',\n    'Wheat',\n    'Woods',\n    'Buildings-Grass-Trees-Drives',\n    'Stone-Steel-Towers'\n]\n\n# Définir le chemin du dataset\ndataset_path = \"/kaggle/input/dataset-deflection-coefficient\"  # Chemin vers le dossier du dataset\n\n# Charger les données hyperspectrales et vérités terrain\ndef charger_donnees_pour_mlp():\n    print(\"Chargement des données pour le MLP...\")\n    # Charger l'image hyperspectrale\n    chemin_image = os.path.join(dataset_path, \"Indian_pines_corrected.mat\")\n    donnees_mat = loadmat(chemin_image)\n    donnees_hyperspectrales = donnees_mat['indian_pines_corrected']\n    \n    # Charger la vérité terrain\n    chemin_gt = os.path.join(dataset_path, \"Indian_pines_gt.mat\")\n    gt_mat = loadmat(chemin_gt)\n    verite_terrain = gt_mat['indian_pines_gt']\n    \n    # Réorganiser les données\n    height, width, n_bands = donnees_hyperspectrales.shape\n    pixels = donnees_hyperspectrales.reshape(height * width, n_bands)\n    classes = verite_terrain.reshape(height * width)\n    \n    print(f\"Dimensions des données: pixels {pixels.shape}, classes {classes.shape}\")\n    return pixels, classes\n\n# Fonction pour sélectionner les meilleures bandes par segments égaux selon le coefficient de déflexion\ndef selectionner_meilleures_bandes_par_segment(min_dc_par_bande, nb_segments):\n    \"\"\"\n    Sélectionne la meilleure bande (selon le coefficient de déflexion) dans chaque segment spectral.\n    \n    Args:\n        min_dc_par_bande: DataFrame contenant les coefficients de déflexion pour chaque bande\n        nb_segments: Nombre de segments spectraux à considérer\n    \n    Returns:\n        Liste des indices des bandes sélectionnées\n    \"\"\"\n    # Nombre total de bandes\n    nb_bandes_total = min_dc_par_bande['Bande'].max() + 1\n    \n    # Taille approximative de chaque segment\n    taille_segment = nb_bandes_total // nb_segments\n    \n    bandes_selectionnees = []\n    segments_info = []\n    \n    print(f\"\\nDivision du spectre en {nb_segments} segments de {taille_segment} bandes chacun\")\n    \n    # Pour chaque segment spectral\n    for i in range(nb_segments):\n        # Calculer les limites du segment\n        debut = i * taille_segment\n        fin = min((i + 1) * taille_segment - 1, nb_bandes_total - 1)\n        \n        print(f\"Segment {i+1}: bandes {debut} à {fin}\")\n        \n        # Sélectionner les bandes dans ce segment\n        segment_df = min_dc_par_bande[(min_dc_par_bande['Bande'] >= debut) & (min_dc_par_bande['Bande'] <= fin)]\n        \n        # Trier le segment par coefficient de déflexion décroissant\n        segment_df = segment_df.sort_values('CoefficientDeflexion', ascending=False)\n        \n        # Trouver la bande avec le meilleur coefficient de déflexion dans ce segment\n        if not segment_df.empty:\n            meilleure_bande = segment_df.iloc[0]['Bande']\n            meilleur_coefficient = segment_df.iloc[0]['CoefficientDeflexion']\n            bandes_selectionnees.append(int(meilleure_bande))\n            \n            segments_info.append({\n                'Segment': i+1,\n                'Debut': debut,\n                'Fin': fin,\n                'MeilleureBande': int(meilleure_bande),\n                'Coefficient': float(meilleur_coefficient)\n            })\n            \n            print(f\"  → Meilleure bande: {int(meilleure_bande)} (Coefficient: {float(meilleur_coefficient):.4f})\")\n        else:\n            print(f\"  → Aucune bande trouvée dans ce segment\")\n    \n    # Sauvegarder les informations sur les segments\n    pd.DataFrame(segments_info).to_csv(f'segments_info_{nb_segments}.csv', index=False)\n    print(f\"Informations sur les segments sauvegardées dans 'segments_info_{nb_segments}.csv'\")\n    \n    return bandes_selectionnees\n\n# 1. Fonction pour préparer les données selon les bandes sélectionnées\ndef preparer_donnees_mlp(pixels, classes, bandes_selectionnees):\n    \"\"\"\n    Prépare les données pour l'entraînement avec les bandes sélectionnées.\n    Inclut la classe de fond (background).\n    \n    Args:\n        pixels: Données spectrales (n_pixels x n_bandes)\n        classes: Étiquettes de classe pour chaque pixel\n        bandes_selectionnees: Liste des indices des bandes à utiliser\n    \n    Returns:\n        X_train, X_test, y_train, y_test: Ensembles d'entraînement et de test\n    \"\"\"\n    # Utiliser tous les pixels, y compris la classe de fond (0)\n    X = pixels   # Toutes les bandes\n    y = classes  # Étiquettes (garder les indices originaux y compris 0)\n    \n    # Nombre de classes (incluant le fond)\n    n_classes = len(np.unique(y))\n    print(f\"Nombre de classes (avec background): {n_classes}\")\n    \n    # Sélectionner uniquement les bandes choisies\n    X_selected = X[:, bandes_selectionnees]\n    print(f\"Dimensions des données: {X_selected.shape}\")\n    \n    # Standardisation des données\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X_selected)\n    \n    # Conversion des étiquettes en format one-hot\n    y_onehot = to_categorical(y)\n    \n    # Diviser en ensembles d'entraînement et de test\n    X_train, X_test, y_train, y_test = train_test_split(\n        X_scaled, y_onehot, test_size=0.3, random_state=42, stratify=y\n    )\n    \n    print(f\"Ensemble d'entraînement: {X_train.shape}, {y_train.shape}\")\n    print(f\"Ensemble de test: {X_test.shape}, {y_test.shape}\")\n    \n    return X_train, X_test, y_train, y_test, n_classes\n\n# 2. Définition du modèle MLP multiclasse avec BatchNorm avant activation\ndef creer_modele_mlp_multiclasse(input_dim, n_classes):\n    \"\"\"\n    Crée un modèle MLP pour la classification multiclasse avec architecture 512-64-64-32\n    et BatchNorm avant activation.\n    \"\"\"\n    inputs = Input(shape=(input_dim,))\n    \n    # Première couche cachée\n    x = Dense(512)(inputs)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    \n    # Deuxième couche cachée\n    x = Dense(64)(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x = Dropout(0.3)(x)\n    \n    # Troisième couche cachée\n    x = Dense(64)(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    \n    # Quatrième couche cachée\n    x = Dense(32)(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    \n    # Couche de sortie avec softmax pour la classification multiclasse\n    outputs = Dense(n_classes, activation='softmax')(x)\n    \n    model = Model(inputs=inputs, outputs=outputs)\n    \n    # Compiler le modèle\n    model.compile(\n        optimizer=Adam(learning_rate=0.001),\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    \n    return model\n\n# 3. Fonction pour entraîner et évaluer un modèle\ndef entrainer_evaluer_modele(X_train, X_test, y_train, y_test, n_classes, \n                           bandes_selectionnees, nom_modele):\n    \"\"\"\n    Entraîne et évalue un modèle MLP avec les données fournies.\n    \n    Args:\n        X_train, X_test, y_train, y_test: Ensembles d'entraînement et de test\n        n_classes: Nombre de classes\n        bandes_selectionnees: Liste des indices des bandes utilisées\n        nom_modele: Nom pour sauvegarder le modèle et les résultats\n    \n    Returns:\n        model: Le modèle entraîné\n        history: L'historique d'entraînement\n        metrics: Dictionnaire des métriques d'évaluation\n    \"\"\"\n    # Créer un dossier pour les résultats de ce modèle\n    os.makedirs(f\"resultats_{nom_modele}\", exist_ok=True)\n    \n    print(f\"\\nCréation et entraînement du modèle {nom_modele}...\")\n    start_time = time.time()\n    \n    # Créer le modèle\n    model = creer_modele_mlp_multiclasse(input_dim=len(bandes_selectionnees), n_classes=n_classes)\n    model.summary()\n    \n    # Définir l'early stopping\n    early_stopping = EarlyStopping(\n        monitor='val_loss',\n        patience=15,\n        restore_best_weights=True,\n        verbose=1\n    )\n    \n    # Entraîner le modèle\n    history = model.fit(\n        X_train, y_train,\n        validation_split=0.2,\n        epochs=100,\n        batch_size=32,\n        callbacks=[early_stopping],\n        verbose=1\n    )\n    \n    train_time = time.time() - start_time\n    print(f\"\\nTemps d'entraînement: {train_time:.2f} secondes\")\n    \n    # Évaluation du modèle\n    print(\"\\nÉvaluation du modèle sur l'ensemble de test...\")\n    start_time = time.time()\n    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n    predict_time = time.time() - start_time\n    \n    print(f\"Précision (accuracy): {accuracy:.4f}\")\n    print(f\"Temps de prédiction: {predict_time:.2f} secondes\")\n    \n    # Générer les prédictions\n    y_pred_prob = model.predict(X_test, verbose=0)\n    y_pred = np.argmax(y_pred_prob, axis=1)\n    y_true = np.argmax(y_test, axis=1)\n    \n    # Utiliser tous les noms de classes, y compris le background\n    class_labels = class_names_custom[:n_classes]\n    \n    # Rapport de classification détaillé\n    report = classification_report(y_true, y_pred, target_names=class_labels, zero_division=0)\n    print(\"\\nRapport de classification:\")\n    print(report)\n    \n    # Sauvegarder le rapport dans un fichier\n    with open(f\"resultats_{nom_modele}/rapport_classification_{nom_modele}.txt\", \"w\") as f:\n        f.write(f\"Précision (accuracy): {accuracy:.4f}\\n\")\n        f.write(f\"Temps d'entraînement: {train_time:.2f} secondes\\n\")\n        f.write(f\"Temps de prédiction: {predict_time:.2f} secondes\\n\\n\")\n        f.write(report)\n    \n    # Visualisations\n    # Courbes d'apprentissage\n    plt.figure(figsize=(12, 5))\n    \n    # Loss\n    plt.subplot(1, 2, 1)\n    plt.plot(history.history['loss'], label='Training Loss')\n    plt.plot(history.history['val_loss'], label='Validation Loss')\n    plt.title('Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    \n    # Accuracy\n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['accuracy'], label='Training Accuracy')\n    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n    plt.title('Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    \n    plt.tight_layout()\n    plt.savefig(f\"resultats_{nom_modele}/learning_curves_{nom_modele}.png\")\n    plt.close()\n    \n    # Matrice de confusion\n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(16, 14))\n    \n    # Utiliser des étiquettes sécurisées pour les axes\n    x_labels = [label[:10] for label in class_labels]\n    y_labels = [label[:10] for label in class_labels]\n    \n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n                xticklabels=x_labels, \n                yticklabels=y_labels)\n    plt.title(f'Matrice de confusion - {nom_modele}')\n    plt.xlabel('Prédit')\n    plt.ylabel('Réel')\n    plt.tight_layout()\n    plt.savefig(f\"resultats_{nom_modele}/confusion_matrix_{nom_modele}.png\")\n    plt.close()\n    \n    # Précision par classe\n    # Récupérer le rapport sous forme de dictionnaire\n    report_dict = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n    \n    # Préparer les données pour la visualisation\n    precision_by_class = []\n    for i, classe in enumerate(class_labels):\n        if classe in report_dict:\n            classe_dict = report_dict[classe]\n            precision_by_class.append({\n                'Classe': classe,\n                'Précision': classe_dict['precision'],\n                'Rappel': classe_dict['recall'],\n                'F1-score': classe_dict['f1-score'],\n                'Support': classe_dict['support']\n            })\n    \n    # Créer le DataFrame et trier\n    precision_df = pd.DataFrame(precision_by_class)\n    if not precision_df.empty:\n        precision_df = precision_df.sort_values('F1-score', ascending=False)\n        \n        # Visualisation\n        plt.figure(figsize=(14, 8))\n        sns.barplot(x='Classe', y='F1-score', data=precision_df)\n        plt.title(f'F1-score par classe - {nom_modele}')\n        plt.xlabel('Classe')\n        plt.ylabel('F1-score')\n        plt.xticks(rotation=90)\n        plt.tight_layout()\n        plt.savefig(f\"resultats_{nom_modele}/f1score_by_class_{nom_modele}.png\")\n        plt.close()\n        \n        # Sauvegarder les F1-scores par classe\n        precision_df.to_csv(f\"resultats_{nom_modele}/f1scores_{nom_modele}.csv\", index=False)\n    else:\n        print(\"Impossible de créer la visualisation du F1-score par classe - données insuffisantes\")\n    \n    # Sauvegarde du modèle\n    model.save(f\"resultats_{nom_modele}/model_{nom_modele}.h5\")\n    print(f\"\\nModèle sauvegardé sous 'resultats_{nom_modele}/model_{nom_modele}.h5'\")\n    \n    # Enregistrer les informations sur les bandes sélectionnées\n    pd.DataFrame({\n        'Bande': bandes_selectionnees,\n        'Segment': range(1, len(bandes_selectionnees) + 1),\n        'CoefficientDeflexion': [float(min_dc_par_bande[min_dc_par_bande['Bande'] == b]['CoefficientDeflexion'].values[0]) \n                         for b in bandes_selectionnees]\n    }).to_csv(f\"resultats_{nom_modele}/bandes_selectionnees_{nom_modele}.csv\", index=False)\n    \n    # Rassembler les métriques pour la comparaison finale\n    metrics = {\n        'accuracy': accuracy,\n        'train_time': train_time,\n        'predict_time': predict_time,\n        'n_bands': len(bandes_selectionnees)\n    }\n    \n    return model, history, metrics\n\n# 4. Fonction principale pour entraîner tous les modèles\ndef entrainer_tous_modeles_segments_deflection():\n    \"\"\"\n    Fonction principale pour entraîner et évaluer tous les modèles avec des bandes\n    sélectionnées par segments selon le coefficient de déflexion.\n    \"\"\"\n    # Créer un dossier de sortie pour les résultats globaux\n    os.makedirs(\"resultats_comparaison_segments_deflection\", exist_ok=True)\n    \n    # Liste pour stocker les métriques de tous les modèles\n    all_metrics = []\n    \n    # 1. Modèle avec 5 segments égaux (1 bande par segment)\n    print(\"\\n\" + \"=\"*80)\n    print(\"MODÈLE AVEC 5 SEGMENTS - COEFFICIENT DE DÉFLEXION\")\n    print(\"=\"*80)\n    seg5_bandes = selectionner_meilleures_bandes_par_segment(min_dc_par_bande, 5)\n    X_train, X_test, y_train, y_test, n_classes = preparer_donnees_mlp(pixels, classes, seg5_bandes)\n    _, _, metrics_seg5 = entrainer_evaluer_modele(\n        X_train, X_test, y_train, y_test, n_classes, seg5_bandes, \"seg5_deflection\"\n    )\n    metrics_seg5['model'] = '5 segments (5 bandes)'\n    all_metrics.append(metrics_seg5)\n    \n    # 2. Modèle avec 10 segments égaux (1 bande par segment)\n    print(\"\\n\" + \"=\"*80)\n    print(\"MODÈLE AVEC 10 SEGMENTS - COEFFICIENT DE DÉFLEXION\")\n    print(\"=\"*80)\n    seg10_bandes = selectionner_meilleures_bandes_par_segment(min_dc_par_bande, 10)\n    X_train, X_test, y_train, y_test, n_classes = preparer_donnees_mlp(pixels, classes, seg10_bandes)\n    _, _, metrics_seg10 = entrainer_evaluer_modele(\n        X_train, X_test, y_train, y_test, n_classes, seg10_bandes, \"seg10_deflection\"\n    )\n    metrics_seg10['model'] = '10 segments (10 bandes)'\n    all_metrics.append(metrics_seg10)\n    \n    # 3. Modèle avec 15 segments égaux (1 bande par segment)\n    print(\"\\n\" + \"=\"*80)\n    print(\"MODÈLE AVEC 15 SEGMENTS - COEFFICIENT DE DÉFLEXION\")\n    print(\"=\"*80)\n    seg15_bandes = selectionner_meilleures_bandes_par_segment(min_dc_par_bande, 15)\n    X_train, X_test, y_train, y_test, n_classes = preparer_donnees_mlp(pixels, classes, seg15_bandes)\n    _, _, metrics_seg15 = entrainer_evaluer_modele(\n        X_train, X_test, y_train, y_test, n_classes, seg15_bandes, \"seg15_deflection\"\n    )\n    metrics_seg15['model'] = '15 segments (15 bandes)'\n    all_metrics.append(metrics_seg15)\n    \n    # 4. Modèle avec 20 segments égaux (1 bande par segment)\n    print(\"\\n\" + \"=\"*80)\n    print(\"MODÈLE AVEC 20 SEGMENTS - COEFFICIENT DE DÉFLEXION\")\n    print(\"=\"*80)\n    seg20_bandes = selectionner_meilleures_bandes_par_segment(min_dc_par_bande, 20)\n    X_train, X_test, y_train, y_test, n_classes = preparer_donnees_mlp(pixels, classes, seg20_bandes)\n    _, _, metrics_seg20 = entrainer_evaluer_modele(\n        X_train, X_test, y_train, y_test, n_classes, seg20_bandes, \"seg20_deflection\"\n    )\n    metrics_seg20['model'] = '20 segments (20 bandes)'\n    all_metrics.append(metrics_seg20)\n    \n    # Créer un tableau de comparaison\n    comparison_df = pd.DataFrame(all_metrics)\n    comparison_df = comparison_df[['model', 'n_bands', 'accuracy', 'train_time', 'predict_time']]\n    comparison_df.columns = ['Modèle', 'Nombre de bandes', 'Précision', 'Temps d\\'entraînement (s)', 'Temps de prédiction (s)']\n    \n    # Sauvegarder le tableau de comparaison\n    comparison_df.to_csv(\"resultats_comparaison_segments_deflection/comparaison_modeles_segments.csv\", index=False)\n    print(\"\\nTableau de comparaison sauvegardé dans 'resultats_comparaison_segments_deflection/comparaison_modeles_segments.csv'\")\n    \n    # Visualiser la comparaison des précisions\n    plt.figure(figsize=(10, 6))\n    sns.barplot(x='Modèle', y='Précision', data=comparison_df)\n    plt.title('Comparaison de la précision des modèles (Segments - Coefficient de Déflexion)')\n    plt.xlabel('Modèle')\n    plt.ylabel('Précision')\n    plt.ylim(0, 1)\n    plt.tight_layout()\n    plt.savefig(\"resultats_comparaison_segments_deflection/comparaison_precision_segments.png\")\n    plt.close()\n    \n    # Visualiser la comparaison des temps d'entraînement\n    plt.figure(figsize=(10, 6))\n    sns.barplot(x='Modèle', y='Temps d\\'entraînement (s)', data=comparison_df)\n    plt.title('Comparaison des temps d\\'entraînement (Segments - Coefficient de Déflexion)')\n    plt.xlabel('Modèle')\n    plt.ylabel('Temps d\\'entraînement (s)')\n    plt.tight_layout()\n    plt.savefig(\"resultats_comparaison_segments_deflection/comparaison_temps_entrainement_segments.png\")\n    plt.close()\n    \n    # Visualiser la distribution des bandes sélectionnées\n    plt.figure(figsize=(15, 8))\n    \n    # Créer une matrice pour représenter toutes les bandes\n    markers = ['o', 's', 'D', '^']\n    colors = ['blue', 'green', 'red', 'purple']\n    labels = ['5 segments', '10 segments', '15 segments', '20 segments']\n    \n    # Tracer la distribution des bandes pour chaque configuration\n    for i, bandes in enumerate([seg5_bandes, seg10_bandes, seg15_bandes, seg20_bandes]):\n        plt.scatter(bandes, np.ones(len(bandes))*i+1, marker=markers[i], \n                   color=colors[i], s=100, label=labels[i])\n    \n    # Ajouter des lignes verticales pour montrer les segments spectraux\n    for i in range(1, 20):\n        plt.axvline(x=i*10, color='gray', linestyle='--', alpha=0.3)\n    \n    plt.title('Distribution des bandes sélectionnées par l\\'approche de segmentation avec coefficient de déflexion')\n    plt.xlabel('Indice de bande')\n    plt.yticks([1, 2, 3, 4], labels)\n    plt.xlim(-5, 205)\n    plt.grid(axis='x', alpha=0.3)\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(\"resultats_comparaison_segments_deflection/distribution_bandes_segments.png\")\n    plt.close()\n    \n    print(\"\\nAnalyse comparative terminée!\")\n    print(\"\\nRécapitulatif des précisions:\")\n    for metric in all_metrics:\n        print(f\"{metric['model']}: {metric['accuracy']:.4f}\")\n    \n    return comparison_df\n\n# Charger les données\npixels, classes = charger_donnees_pour_mlp()\n\n# Charger les résultats de coefficient de déflexion\nprint(\"Chargement des résultats de coefficient de déflexion...\")\nmin_dc_par_bande = pd.read_csv('coefficients_deflexion_par_bande_tri.csv')\n\n# Exécuter l'entraînement des modèles\nif __name__ == \"__main__\":\n    comparison_results = entrainer_tous_modeles_segments_deflection()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T12:41:16.368629Z","iopub.execute_input":"2025-05-09T12:41:16.369375Z","iopub.status.idle":"2025-05-09T12:46:57.306856Z","shell.execute_reply.started":"2025-05-09T12:41:16.369352Z","shell.execute_reply":"2025-05-09T12:46:57.306153Z"}},"outputs":[{"name":"stdout","text":"Chargement des données pour le MLP...\nDimensions des données: pixels (21025, 200), classes (21025,)\nChargement des résultats de coefficient de déflexion...\n\n================================================================================\nMODÈLE AVEC 5 SEGMENTS - COEFFICIENT DE DÉFLEXION\n================================================================================\n\nDivision du spectre en 5 segments de 40 bandes chacun\nSegment 1: bandes 0 à 39\n  → Meilleure bande: 25 (Coefficient: 0.0040)\nSegment 2: bandes 40 à 79\n  → Meilleure bande: 51 (Coefficient: 0.0030)\nSegment 3: bandes 80 à 119\n  → Meilleure bande: 98 (Coefficient: 0.0025)\nSegment 4: bandes 120 à 159\n  → Meilleure bande: 157 (Coefficient: 0.0086)\nSegment 5: bandes 160 à 199\n  → Meilleure bande: 160 (Coefficient: 0.0053)\nInformations sur les segments sauvegardées dans 'segments_info_5.csv'\nNombre de classes (avec background): 17\nDimensions des données: (21025, 5)\nEnsemble d'entraînement: (14717, 5), (14717, 17)\nEnsemble de test: (6308, 5), (6308, 17)\n\nCréation et entraînement du modèle seg5_deflection...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_4\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_4\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ input_layer_4 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)                   │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_20 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │           \u001b[38;5;34m3,072\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_16               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │           \u001b[38;5;34m2,048\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_16 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_21 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │          \u001b[38;5;34m32,832\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_17               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │             \u001b[38;5;34m256\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_17 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_22 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │           \u001b[38;5;34m4,160\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_18               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │             \u001b[38;5;34m256\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_18 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_23 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │           \u001b[38;5;34m2,080\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_19               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │             \u001b[38;5;34m128\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_19 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_24 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m)                  │             \u001b[38;5;34m561\u001b[0m │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ input_layer_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)                   │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">3,072</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_16               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">32,832</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_17               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_18               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_19               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">561</span> │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m45,393\u001b[0m (177.32 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">45,393</span> (177.32 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m44,049\u001b[0m (172.07 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">44,049</span> (172.07 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,344\u001b[0m (5.25 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,344</span> (5.25 KB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.4551 - loss: 1.9552 - val_accuracy: 0.5625 - val_loss: 1.3092\nEpoch 2/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6092 - loss: 1.1420 - val_accuracy: 0.6464 - val_loss: 0.9977\nEpoch 3/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6243 - loss: 1.0495 - val_accuracy: 0.6444 - val_loss: 0.9520\nEpoch 4/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6135 - loss: 1.0442 - val_accuracy: 0.6488 - val_loss: 0.9171\nEpoch 5/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6405 - loss: 0.9940 - val_accuracy: 0.6321 - val_loss: 0.9790\nEpoch 6/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6373 - loss: 0.9733 - val_accuracy: 0.6596 - val_loss: 0.8955\nEpoch 7/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6400 - loss: 0.9727 - val_accuracy: 0.6736 - val_loss: 0.8772\nEpoch 8/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6374 - loss: 0.9613 - val_accuracy: 0.6821 - val_loss: 0.8497\nEpoch 9/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6451 - loss: 0.9431 - val_accuracy: 0.6760 - val_loss: 0.8550\nEpoch 10/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6449 - loss: 0.9348 - val_accuracy: 0.6776 - val_loss: 0.8421\nEpoch 11/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6595 - loss: 0.9228 - val_accuracy: 0.6773 - val_loss: 0.8459\nEpoch 12/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6524 - loss: 0.9325 - val_accuracy: 0.6688 - val_loss: 0.8533\nEpoch 13/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6502 - loss: 0.9295 - val_accuracy: 0.6756 - val_loss: 0.8383\nEpoch 14/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6582 - loss: 0.9120 - val_accuracy: 0.6804 - val_loss: 0.8329\nEpoch 15/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6692 - loss: 0.8949 - val_accuracy: 0.6997 - val_loss: 0.8126\nEpoch 16/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6563 - loss: 0.8900 - val_accuracy: 0.6953 - val_loss: 0.8107\nEpoch 17/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6596 - loss: 0.8863 - val_accuracy: 0.6899 - val_loss: 0.8423\nEpoch 18/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6642 - loss: 0.8814 - val_accuracy: 0.6984 - val_loss: 0.8128\nEpoch 19/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6629 - loss: 0.8940 - val_accuracy: 0.7007 - val_loss: 0.7901\nEpoch 20/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6656 - loss: 0.8771 - val_accuracy: 0.6868 - val_loss: 0.8457\nEpoch 21/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6581 - loss: 0.8983 - val_accuracy: 0.6960 - val_loss: 0.8039\nEpoch 22/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6674 - loss: 0.8867 - val_accuracy: 0.7062 - val_loss: 0.7867\nEpoch 23/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6664 - loss: 0.8831 - val_accuracy: 0.7024 - val_loss: 0.8003\nEpoch 24/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6668 - loss: 0.8741 - val_accuracy: 0.6885 - val_loss: 0.8194\nEpoch 25/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6659 - loss: 0.8715 - val_accuracy: 0.6844 - val_loss: 0.8237\nEpoch 26/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6712 - loss: 0.8723 - val_accuracy: 0.7035 - val_loss: 0.7791\nEpoch 27/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6755 - loss: 0.8575 - val_accuracy: 0.6912 - val_loss: 0.8020\nEpoch 28/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6729 - loss: 0.8530 - val_accuracy: 0.6960 - val_loss: 0.8122\nEpoch 29/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6705 - loss: 0.8779 - val_accuracy: 0.7116 - val_loss: 0.7629\nEpoch 30/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6692 - loss: 0.8609 - val_accuracy: 0.7086 - val_loss: 0.7719\nEpoch 31/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6751 - loss: 0.8576 - val_accuracy: 0.7082 - val_loss: 0.7694\nEpoch 32/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6820 - loss: 0.8410 - val_accuracy: 0.7048 - val_loss: 0.7750\nEpoch 33/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6759 - loss: 0.8583 - val_accuracy: 0.6960 - val_loss: 0.7966\nEpoch 34/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6682 - loss: 0.8798 - val_accuracy: 0.7028 - val_loss: 0.7781\nEpoch 35/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6790 - loss: 0.8567 - val_accuracy: 0.7160 - val_loss: 0.7599\nEpoch 36/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6792 - loss: 0.8516 - val_accuracy: 0.7075 - val_loss: 0.7696\nEpoch 37/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6721 - loss: 0.8688 - val_accuracy: 0.7092 - val_loss: 0.7571\nEpoch 38/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6874 - loss: 0.8372 - val_accuracy: 0.7082 - val_loss: 0.7643\nEpoch 39/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6823 - loss: 0.8322 - val_accuracy: 0.7069 - val_loss: 0.7668\nEpoch 40/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6826 - loss: 0.8423 - val_accuracy: 0.7133 - val_loss: 0.7662\nEpoch 41/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6735 - loss: 0.8427 - val_accuracy: 0.7001 - val_loss: 0.7864\nEpoch 42/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6671 - loss: 0.8584 - val_accuracy: 0.7238 - val_loss: 0.7442\nEpoch 43/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6833 - loss: 0.8494 - val_accuracy: 0.7174 - val_loss: 0.7544\nEpoch 44/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6832 - loss: 0.8313 - val_accuracy: 0.7038 - val_loss: 0.7782\nEpoch 45/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6734 - loss: 0.8554 - val_accuracy: 0.7130 - val_loss: 0.7556\nEpoch 46/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6872 - loss: 0.8296 - val_accuracy: 0.7228 - val_loss: 0.7554\nEpoch 47/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6880 - loss: 0.8233 - val_accuracy: 0.7143 - val_loss: 0.7539\nEpoch 48/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6867 - loss: 0.8230 - val_accuracy: 0.6990 - val_loss: 0.7701\nEpoch 49/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6850 - loss: 0.8269 - val_accuracy: 0.7249 - val_loss: 0.7472\nEpoch 50/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6783 - loss: 0.8511 - val_accuracy: 0.7150 - val_loss: 0.7594\nEpoch 51/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6857 - loss: 0.8217 - val_accuracy: 0.7171 - val_loss: 0.7520\nEpoch 52/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6811 - loss: 0.8296 - val_accuracy: 0.7137 - val_loss: 0.7644\nEpoch 53/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6816 - loss: 0.8434 - val_accuracy: 0.7181 - val_loss: 0.7530\nEpoch 54/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6944 - loss: 0.8093 - val_accuracy: 0.7143 - val_loss: 0.7641\nEpoch 55/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6850 - loss: 0.8346 - val_accuracy: 0.7120 - val_loss: 0.7625\nEpoch 56/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6888 - loss: 0.8231 - val_accuracy: 0.7181 - val_loss: 0.7528\nEpoch 57/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6800 - loss: 0.8315 - val_accuracy: 0.7204 - val_loss: 0.7437\nEpoch 58/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6891 - loss: 0.8224 - val_accuracy: 0.7191 - val_loss: 0.7422\nEpoch 59/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6858 - loss: 0.8176 - val_accuracy: 0.7204 - val_loss: 0.7426\nEpoch 60/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6895 - loss: 0.8278 - val_accuracy: 0.7228 - val_loss: 0.7424\nEpoch 61/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6900 - loss: 0.8159 - val_accuracy: 0.7160 - val_loss: 0.7487\nEpoch 62/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6836 - loss: 0.8095 - val_accuracy: 0.7123 - val_loss: 0.7575\nEpoch 63/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6839 - loss: 0.8194 - val_accuracy: 0.7171 - val_loss: 0.7535\nEpoch 64/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6822 - loss: 0.8337 - val_accuracy: 0.7238 - val_loss: 0.7401\nEpoch 65/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6742 - loss: 0.8433 - val_accuracy: 0.7204 - val_loss: 0.7437\nEpoch 66/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6825 - loss: 0.8240 - val_accuracy: 0.7201 - val_loss: 0.7511\nEpoch 67/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6834 - loss: 0.8225 - val_accuracy: 0.7164 - val_loss: 0.7468\nEpoch 68/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6907 - loss: 0.7993 - val_accuracy: 0.7252 - val_loss: 0.7377\nEpoch 69/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6811 - loss: 0.8277 - val_accuracy: 0.7245 - val_loss: 0.7394\nEpoch 70/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6892 - loss: 0.8170 - val_accuracy: 0.7194 - val_loss: 0.7412\nEpoch 71/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6877 - loss: 0.8126 - val_accuracy: 0.7272 - val_loss: 0.7375\nEpoch 72/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6867 - loss: 0.8171 - val_accuracy: 0.7181 - val_loss: 0.7447\nEpoch 73/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6846 - loss: 0.8189 - val_accuracy: 0.7232 - val_loss: 0.7444\nEpoch 74/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6914 - loss: 0.8091 - val_accuracy: 0.7249 - val_loss: 0.7482\nEpoch 75/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6827 - loss: 0.8218 - val_accuracy: 0.7235 - val_loss: 0.7428\nEpoch 76/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6884 - loss: 0.8209 - val_accuracy: 0.7123 - val_loss: 0.7617\nEpoch 77/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6941 - loss: 0.7962 - val_accuracy: 0.7249 - val_loss: 0.7390\nEpoch 78/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6891 - loss: 0.8118 - val_accuracy: 0.7340 - val_loss: 0.7337\nEpoch 79/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6819 - loss: 0.8303 - val_accuracy: 0.7255 - val_loss: 0.7361\nEpoch 80/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6892 - loss: 0.8155 - val_accuracy: 0.7221 - val_loss: 0.7490\nEpoch 81/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6926 - loss: 0.8158 - val_accuracy: 0.7106 - val_loss: 0.7920\nEpoch 82/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6952 - loss: 0.8103 - val_accuracy: 0.7249 - val_loss: 0.7387\nEpoch 83/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6817 - loss: 0.8303 - val_accuracy: 0.7120 - val_loss: 0.7488\nEpoch 84/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6810 - loss: 0.8355 - val_accuracy: 0.7225 - val_loss: 0.7411\nEpoch 85/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6907 - loss: 0.8056 - val_accuracy: 0.7269 - val_loss: 0.7385\nEpoch 86/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6880 - loss: 0.8102 - val_accuracy: 0.7188 - val_loss: 0.7403\nEpoch 87/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6994 - loss: 0.7965 - val_accuracy: 0.7252 - val_loss: 0.7287\nEpoch 88/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6949 - loss: 0.7945 - val_accuracy: 0.7232 - val_loss: 0.7279\nEpoch 89/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7002 - loss: 0.7904 - val_accuracy: 0.7184 - val_loss: 0.7404\nEpoch 90/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6914 - loss: 0.7967 - val_accuracy: 0.7201 - val_loss: 0.7328\nEpoch 91/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6974 - loss: 0.7846 - val_accuracy: 0.7242 - val_loss: 0.7333\nEpoch 92/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6882 - loss: 0.8141 - val_accuracy: 0.7103 - val_loss: 0.7574\nEpoch 93/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6940 - loss: 0.8061 - val_accuracy: 0.7228 - val_loss: 0.7304\nEpoch 94/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6847 - loss: 0.8063 - val_accuracy: 0.7208 - val_loss: 0.7351\nEpoch 95/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6933 - loss: 0.8066 - val_accuracy: 0.7208 - val_loss: 0.7487\nEpoch 96/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6955 - loss: 0.7918 - val_accuracy: 0.7221 - val_loss: 0.7357\nEpoch 97/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6965 - loss: 0.7960 - val_accuracy: 0.7198 - val_loss: 0.7293\nEpoch 98/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6949 - loss: 0.8130 - val_accuracy: 0.7269 - val_loss: 0.7338\nEpoch 99/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6891 - loss: 0.8085 - val_accuracy: 0.7218 - val_loss: 0.7288\nEpoch 100/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7015 - loss: 0.7851 - val_accuracy: 0.7320 - val_loss: 0.7160\nRestoring model weights from the end of the best epoch: 100.\n\nTemps d'entraînement: 85.77 secondes\n\nÉvaluation du modèle sur l'ensemble de test...\nPrécision (accuracy): 0.7432\nTemps de prédiction: 1.06 secondes\n\nRapport de classification:\n                              precision    recall  f1-score   support\n\n                  Background       0.81      0.83      0.82      3233\n                     Alfalfa       1.00      0.43      0.60        14\n                 Corn-notill       0.62      0.68      0.65       428\n                Corn-mintill       0.69      0.56      0.62       249\n                        Corn       0.64      0.61      0.62        71\n               Grass-pasture       0.85      0.69      0.76       145\n                 Grass-trees       0.65      0.68      0.67       219\n         Grass-pasture-mowed       0.75      0.75      0.75         8\n               Hay-windrowed       0.79      0.99      0.88       143\n                        Oats       0.00      0.00      0.00         6\n              Soybean-notill       0.65      0.81      0.72       292\n             Soybean-mintill       0.67      0.77      0.72       737\n               Soybean-clean       0.69      0.49      0.58       178\n                       Wheat       0.58      0.93      0.71        61\n                       Woods       0.60      0.38      0.46       380\nBuildings-Grass-Trees-Drives       0.00      0.00      0.00       116\n          Stone-Steel-Towers       0.74      0.93      0.83        28\n\n                    accuracy                           0.74      6308\n                   macro avg       0.63      0.62      0.61      6308\n                weighted avg       0.73      0.74      0.73      6308\n\nImpossible de créer la visualisation du F1-score par classe - données insuffisantes\n\nModèle sauvegardé sous 'resultats_seg5_deflection/model_seg5_deflection.h5'\n\n================================================================================\nMODÈLE AVEC 10 SEGMENTS - COEFFICIENT DE DÉFLEXION\n================================================================================\n\nDivision du spectre en 10 segments de 20 bandes chacun\nSegment 1: bandes 0 à 19\n  → Meilleure bande: 19 (Coefficient: 0.0007)\nSegment 2: bandes 20 à 39\n  → Meilleure bande: 25 (Coefficient: 0.0040)\nSegment 3: bandes 40 à 59\n  → Meilleure bande: 51 (Coefficient: 0.0030)\nSegment 4: bandes 60 à 79\n  → Meilleure bande: 73 (Coefficient: 0.0027)\nSegment 5: bandes 80 à 99\n  → Meilleure bande: 98 (Coefficient: 0.0025)\nSegment 6: bandes 100 à 119\n  → Meilleure bande: 101 (Coefficient: 0.0016)\nSegment 7: bandes 120 à 139\n  → Meilleure bande: 137 (Coefficient: 0.0012)\nSegment 8: bandes 140 à 159\n  → Meilleure bande: 157 (Coefficient: 0.0086)\nSegment 9: bandes 160 à 179\n  → Meilleure bande: 160 (Coefficient: 0.0053)\nSegment 10: bandes 180 à 199\n  → Meilleure bande: 186 (Coefficient: 0.0043)\nInformations sur les segments sauvegardées dans 'segments_info_10.csv'\nNombre de classes (avec background): 17\nDimensions des données: (21025, 10)\nEnsemble d'entraînement: (14717, 10), (14717, 17)\nEnsemble de test: (6308, 10), (6308, 17)\n\nCréation et entraînement du modèle seg10_deflection...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_5\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_5\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ input_layer_5 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_25 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │           \u001b[38;5;34m5,632\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_20               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │           \u001b[38;5;34m2,048\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_20 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_26 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │          \u001b[38;5;34m32,832\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_21               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │             \u001b[38;5;34m256\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_21 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_27 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │           \u001b[38;5;34m4,160\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_22               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │             \u001b[38;5;34m256\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_22 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_28 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │           \u001b[38;5;34m2,080\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_23               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │             \u001b[38;5;34m128\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_23 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_29 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m)                  │             \u001b[38;5;34m561\u001b[0m │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ input_layer_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">5,632</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_20               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">32,832</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_21               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_22               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_23               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">561</span> │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m47,953\u001b[0m (187.32 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">47,953</span> (187.32 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m46,609\u001b[0m (182.07 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">46,609</span> (182.07 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,344\u001b[0m (5.25 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,344</span> (5.25 KB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.4637 - loss: 1.9437 - val_accuracy: 0.6084 - val_loss: 1.1952\nEpoch 2/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6081 - loss: 1.1349 - val_accuracy: 0.6413 - val_loss: 0.9711\nEpoch 3/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6274 - loss: 1.0472 - val_accuracy: 0.6556 - val_loss: 0.9254\nEpoch 4/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6427 - loss: 0.9890 - val_accuracy: 0.6719 - val_loss: 0.8941\nEpoch 5/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6483 - loss: 0.9656 - val_accuracy: 0.6712 - val_loss: 0.8800\nEpoch 6/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6473 - loss: 0.9541 - val_accuracy: 0.6712 - val_loss: 0.8644\nEpoch 7/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6508 - loss: 0.9449 - val_accuracy: 0.6685 - val_loss: 0.8645\nEpoch 8/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6495 - loss: 0.9434 - val_accuracy: 0.6681 - val_loss: 0.8548\nEpoch 9/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6587 - loss: 0.9231 - val_accuracy: 0.6804 - val_loss: 0.8136\nEpoch 10/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6614 - loss: 0.8977 - val_accuracy: 0.6831 - val_loss: 0.8249\nEpoch 11/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6719 - loss: 0.8663 - val_accuracy: 0.7031 - val_loss: 0.7925\nEpoch 12/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6746 - loss: 0.8741 - val_accuracy: 0.7024 - val_loss: 0.7830\nEpoch 13/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6698 - loss: 0.8718 - val_accuracy: 0.6950 - val_loss: 0.7867\nEpoch 14/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6752 - loss: 0.8721 - val_accuracy: 0.6827 - val_loss: 0.8121\nEpoch 15/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6790 - loss: 0.8589 - val_accuracy: 0.7109 - val_loss: 0.7614\nEpoch 16/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6740 - loss: 0.8523 - val_accuracy: 0.7062 - val_loss: 0.7667\nEpoch 17/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6746 - loss: 0.8498 - val_accuracy: 0.6916 - val_loss: 0.7981\nEpoch 18/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6755 - loss: 0.8447 - val_accuracy: 0.7184 - val_loss: 0.7352\nEpoch 19/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6839 - loss: 0.8281 - val_accuracy: 0.7323 - val_loss: 0.7230\nEpoch 20/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6985 - loss: 0.8069 - val_accuracy: 0.7024 - val_loss: 0.7660\nEpoch 21/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6844 - loss: 0.8327 - val_accuracy: 0.7103 - val_loss: 0.7738\nEpoch 22/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6855 - loss: 0.8265 - val_accuracy: 0.7245 - val_loss: 0.7134\nEpoch 23/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6865 - loss: 0.8246 - val_accuracy: 0.6933 - val_loss: 0.7653\nEpoch 24/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6862 - loss: 0.8312 - val_accuracy: 0.7252 - val_loss: 0.7145\nEpoch 25/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6923 - loss: 0.8202 - val_accuracy: 0.7211 - val_loss: 0.7128\nEpoch 26/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6931 - loss: 0.8139 - val_accuracy: 0.7188 - val_loss: 0.7206\nEpoch 27/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6908 - loss: 0.8051 - val_accuracy: 0.7130 - val_loss: 0.7389\nEpoch 28/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7056 - loss: 0.7764 - val_accuracy: 0.7300 - val_loss: 0.7030\nEpoch 29/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7062 - loss: 0.7849 - val_accuracy: 0.7300 - val_loss: 0.7182\nEpoch 30/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7063 - loss: 0.7868 - val_accuracy: 0.7143 - val_loss: 0.7347\nEpoch 31/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7107 - loss: 0.7753 - val_accuracy: 0.7293 - val_loss: 0.7127\nEpoch 32/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6920 - loss: 0.7958 - val_accuracy: 0.7395 - val_loss: 0.7001\nEpoch 33/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7003 - loss: 0.7928 - val_accuracy: 0.7255 - val_loss: 0.7239\nEpoch 34/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7007 - loss: 0.7924 - val_accuracy: 0.7208 - val_loss: 0.7419\nEpoch 35/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7032 - loss: 0.7771 - val_accuracy: 0.6895 - val_loss: 0.7895\nEpoch 36/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7007 - loss: 0.7902 - val_accuracy: 0.7357 - val_loss: 0.6951\nEpoch 37/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7059 - loss: 0.7725 - val_accuracy: 0.7181 - val_loss: 0.7437\nEpoch 38/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7018 - loss: 0.7818 - val_accuracy: 0.7317 - val_loss: 0.7086\nEpoch 39/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7107 - loss: 0.7628 - val_accuracy: 0.7269 - val_loss: 0.7267\nEpoch 40/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7048 - loss: 0.7786 - val_accuracy: 0.7459 - val_loss: 0.6856\nEpoch 41/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7058 - loss: 0.7685 - val_accuracy: 0.7337 - val_loss: 0.6904\nEpoch 42/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7131 - loss: 0.7508 - val_accuracy: 0.7398 - val_loss: 0.6978\nEpoch 43/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7212 - loss: 0.7433 - val_accuracy: 0.7439 - val_loss: 0.6911\nEpoch 44/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7069 - loss: 0.7665 - val_accuracy: 0.7276 - val_loss: 0.7145\nEpoch 45/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7118 - loss: 0.7556 - val_accuracy: 0.7354 - val_loss: 0.6813\nEpoch 46/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7075 - loss: 0.7524 - val_accuracy: 0.7391 - val_loss: 0.6927\nEpoch 47/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7069 - loss: 0.7611 - val_accuracy: 0.7408 - val_loss: 0.6736\nEpoch 48/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7140 - loss: 0.7610 - val_accuracy: 0.7442 - val_loss: 0.6780\nEpoch 49/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7053 - loss: 0.7634 - val_accuracy: 0.7429 - val_loss: 0.6789\nEpoch 50/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7129 - loss: 0.7416 - val_accuracy: 0.7432 - val_loss: 0.6812\nEpoch 51/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7091 - loss: 0.7642 - val_accuracy: 0.7293 - val_loss: 0.6950\nEpoch 52/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7186 - loss: 0.7475 - val_accuracy: 0.7371 - val_loss: 0.6773\nEpoch 53/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7219 - loss: 0.7452 - val_accuracy: 0.7364 - val_loss: 0.6890\nEpoch 54/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7177 - loss: 0.7468 - val_accuracy: 0.7408 - val_loss: 0.6896\nEpoch 55/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7206 - loss: 0.7422 - val_accuracy: 0.7466 - val_loss: 0.6716\nEpoch 56/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7137 - loss: 0.7484 - val_accuracy: 0.7385 - val_loss: 0.6726\nEpoch 57/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6998 - loss: 0.7767 - val_accuracy: 0.7405 - val_loss: 0.6766\nEpoch 58/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7183 - loss: 0.7434 - val_accuracy: 0.7351 - val_loss: 0.6874\nEpoch 59/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7146 - loss: 0.7504 - val_accuracy: 0.7459 - val_loss: 0.6668\nEpoch 60/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7247 - loss: 0.7315 - val_accuracy: 0.7514 - val_loss: 0.6608\nEpoch 61/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7264 - loss: 0.7260 - val_accuracy: 0.7422 - val_loss: 0.6716\nEpoch 62/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7156 - loss: 0.7486 - val_accuracy: 0.7381 - val_loss: 0.6810\nEpoch 63/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7173 - loss: 0.7361 - val_accuracy: 0.7480 - val_loss: 0.6623\nEpoch 64/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7155 - loss: 0.7441 - val_accuracy: 0.7361 - val_loss: 0.6902\nEpoch 65/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7187 - loss: 0.7425 - val_accuracy: 0.7476 - val_loss: 0.6615\nEpoch 66/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7225 - loss: 0.7338 - val_accuracy: 0.7405 - val_loss: 0.6757\nEpoch 67/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7189 - loss: 0.7435 - val_accuracy: 0.7432 - val_loss: 0.6915\nEpoch 68/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7261 - loss: 0.7303 - val_accuracy: 0.7544 - val_loss: 0.6569\nEpoch 69/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7202 - loss: 0.7268 - val_accuracy: 0.7401 - val_loss: 0.7092\nEpoch 70/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7238 - loss: 0.7304 - val_accuracy: 0.7378 - val_loss: 0.6809\nEpoch 71/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7200 - loss: 0.7243 - val_accuracy: 0.7476 - val_loss: 0.6660\nEpoch 72/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7125 - loss: 0.7483 - val_accuracy: 0.7520 - val_loss: 0.6638\nEpoch 73/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7271 - loss: 0.7208 - val_accuracy: 0.7340 - val_loss: 0.6741\nEpoch 74/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7259 - loss: 0.7304 - val_accuracy: 0.7412 - val_loss: 0.6730\nEpoch 75/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7115 - loss: 0.7457 - val_accuracy: 0.7449 - val_loss: 0.6643\nEpoch 76/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7250 - loss: 0.7179 - val_accuracy: 0.7517 - val_loss: 0.6697\nEpoch 77/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7200 - loss: 0.7331 - val_accuracy: 0.7412 - val_loss: 0.6710\nEpoch 78/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7187 - loss: 0.7246 - val_accuracy: 0.7548 - val_loss: 0.6469\nEpoch 79/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7142 - loss: 0.7424 - val_accuracy: 0.7480 - val_loss: 0.6444\nEpoch 80/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7240 - loss: 0.7136 - val_accuracy: 0.7497 - val_loss: 0.6608\nEpoch 81/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7287 - loss: 0.7136 - val_accuracy: 0.7446 - val_loss: 0.6587\nEpoch 82/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7141 - loss: 0.7294 - val_accuracy: 0.7554 - val_loss: 0.6452\nEpoch 83/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7215 - loss: 0.7294 - val_accuracy: 0.7429 - val_loss: 0.6660\nEpoch 84/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7221 - loss: 0.7298 - val_accuracy: 0.7337 - val_loss: 0.6868\nEpoch 85/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7335 - loss: 0.7173 - val_accuracy: 0.7497 - val_loss: 0.6634\nEpoch 86/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7249 - loss: 0.7128 - val_accuracy: 0.7452 - val_loss: 0.6640\nEpoch 87/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7330 - loss: 0.7042 - val_accuracy: 0.7503 - val_loss: 0.6597\nEpoch 88/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7339 - loss: 0.7047 - val_accuracy: 0.7334 - val_loss: 0.6972\nEpoch 89/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7223 - loss: 0.7248 - val_accuracy: 0.7476 - val_loss: 0.6611\nEpoch 90/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7283 - loss: 0.7169 - val_accuracy: 0.7157 - val_loss: 0.7402\nEpoch 91/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7200 - loss: 0.7254 - val_accuracy: 0.7422 - val_loss: 0.6670\nEpoch 92/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7132 - loss: 0.7267 - val_accuracy: 0.7554 - val_loss: 0.6491\nEpoch 93/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7361 - loss: 0.7074 - val_accuracy: 0.7537 - val_loss: 0.6568\nEpoch 94/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7228 - loss: 0.7170 - val_accuracy: 0.7541 - val_loss: 0.6544\nEpoch 94: early stopping\nRestoring model weights from the end of the best epoch: 79.\n\nTemps d'entraînement: 80.82 secondes\n\nÉvaluation du modèle sur l'ensemble de test...\nPrécision (accuracy): 0.7617\nTemps de prédiction: 0.98 secondes\n\nRapport de classification:\n                              precision    recall  f1-score   support\n\n                  Background       0.81      0.85      0.83      3233\n                     Alfalfa       1.00      0.79      0.88        14\n                 Corn-notill       0.62      0.80      0.70       428\n                Corn-mintill       0.69      0.59      0.63       249\n                        Corn       0.56      0.76      0.64        71\n               Grass-pasture       0.85      0.72      0.78       145\n                 Grass-trees       0.68      0.76      0.72       219\n         Grass-pasture-mowed       0.71      0.62      0.67         8\n               Hay-windrowed       0.87      0.99      0.93       143\n                        Oats       0.00      0.00      0.00         6\n              Soybean-notill       0.74      0.78      0.76       292\n             Soybean-mintill       0.76      0.73      0.74       737\n               Soybean-clean       0.65      0.70      0.67       178\n                       Wheat       0.76      0.72      0.74        61\n                       Woods       0.59      0.31      0.41       380\nBuildings-Grass-Trees-Drives       0.00      0.00      0.00       116\n          Stone-Steel-Towers       0.66      0.82      0.73        28\n\n                    accuracy                           0.76      6308\n                   macro avg       0.64      0.64      0.64      6308\n                weighted avg       0.74      0.76      0.75      6308\n\nImpossible de créer la visualisation du F1-score par classe - données insuffisantes\n\nModèle sauvegardé sous 'resultats_seg10_deflection/model_seg10_deflection.h5'\n\n================================================================================\nMODÈLE AVEC 15 SEGMENTS - COEFFICIENT DE DÉFLEXION\n================================================================================\n\nDivision du spectre en 15 segments de 13 bandes chacun\nSegment 1: bandes 0 à 12\n  → Meilleure bande: 1 (Coefficient: 0.0003)\nSegment 2: bandes 13 à 25\n  → Meilleure bande: 25 (Coefficient: 0.0040)\nSegment 3: bandes 26 à 38\n  → Meilleure bande: 26 (Coefficient: 0.0033)\nSegment 4: bandes 39 à 51\n  → Meilleure bande: 51 (Coefficient: 0.0030)\nSegment 5: bandes 52 à 64\n  → Meilleure bande: 52 (Coefficient: 0.0016)\nSegment 6: bandes 65 à 77\n  → Meilleure bande: 73 (Coefficient: 0.0027)\nSegment 7: bandes 78 à 90\n  → Meilleure bande: 81 (Coefficient: 0.0009)\nSegment 8: bandes 91 à 103\n  → Meilleure bande: 98 (Coefficient: 0.0025)\nSegment 9: bandes 104 à 116\n  → Meilleure bande: 111 (Coefficient: 0.0010)\nSegment 10: bandes 117 à 129\n  → Meilleure bande: 117 (Coefficient: 0.0005)\nSegment 11: bandes 130 à 142\n  → Meilleure bande: 137 (Coefficient: 0.0012)\nSegment 12: bandes 143 à 155\n  → Meilleure bande: 152 (Coefficient: 0.0036)\nSegment 13: bandes 156 à 168\n  → Meilleure bande: 157 (Coefficient: 0.0086)\nSegment 14: bandes 169 à 181\n  → Meilleure bande: 176 (Coefficient: 0.0033)\nSegment 15: bandes 182 à 194\n  → Meilleure bande: 186 (Coefficient: 0.0043)\nInformations sur les segments sauvegardées dans 'segments_info_15.csv'\nNombre de classes (avec background): 17\nDimensions des données: (21025, 15)\nEnsemble d'entraînement: (14717, 15), (14717, 17)\nEnsemble de test: (6308, 15), (6308, 17)\n\nCréation et entraînement du modèle seg15_deflection...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_6\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_6\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ input_layer_6 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_30 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │           \u001b[38;5;34m8,192\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_24               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │           \u001b[38;5;34m2,048\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_24 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_31 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │          \u001b[38;5;34m32,832\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_25               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │             \u001b[38;5;34m256\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_25 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_32 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │           \u001b[38;5;34m4,160\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_26               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │             \u001b[38;5;34m256\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_26 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_33 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │           \u001b[38;5;34m2,080\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_27               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │             \u001b[38;5;34m128\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_27 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_34 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m)                  │             \u001b[38;5;34m561\u001b[0m │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ input_layer_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,192</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_24               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">32,832</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_25               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_26               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_27               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_34 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">561</span> │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m50,513\u001b[0m (197.32 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">50,513</span> (197.32 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m49,169\u001b[0m (192.07 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">49,169</span> (192.07 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,344\u001b[0m (5.25 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,344</span> (5.25 KB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.4923 - loss: 1.7641 - val_accuracy: 0.6209 - val_loss: 1.1268\nEpoch 2/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6037 - loss: 1.1426 - val_accuracy: 0.6416 - val_loss: 0.9740\nEpoch 3/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6202 - loss: 1.0709 - val_accuracy: 0.6630 - val_loss: 0.9409\nEpoch 4/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6248 - loss: 1.0311 - val_accuracy: 0.6681 - val_loss: 0.9150\nEpoch 5/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6466 - loss: 0.9678 - val_accuracy: 0.6573 - val_loss: 0.9086\nEpoch 6/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6440 - loss: 0.9612 - val_accuracy: 0.6899 - val_loss: 0.8423\nEpoch 7/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6511 - loss: 0.9289 - val_accuracy: 0.6776 - val_loss: 0.8685\nEpoch 8/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6598 - loss: 0.9218 - val_accuracy: 0.6743 - val_loss: 0.8596\nEpoch 9/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6584 - loss: 0.9146 - val_accuracy: 0.6851 - val_loss: 0.8275\nEpoch 10/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6671 - loss: 0.8841 - val_accuracy: 0.6933 - val_loss: 0.8107\nEpoch 11/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6663 - loss: 0.8818 - val_accuracy: 0.6573 - val_loss: 0.8494\nEpoch 12/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6689 - loss: 0.8826 - val_accuracy: 0.7109 - val_loss: 0.7727\nEpoch 13/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6765 - loss: 0.8665 - val_accuracy: 0.7164 - val_loss: 0.7796\nEpoch 14/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6688 - loss: 0.8751 - val_accuracy: 0.6906 - val_loss: 0.7931\nEpoch 15/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6721 - loss: 0.8577 - val_accuracy: 0.7041 - val_loss: 0.7949\nEpoch 16/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6895 - loss: 0.8448 - val_accuracy: 0.7031 - val_loss: 0.7646\nEpoch 17/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6874 - loss: 0.8185 - val_accuracy: 0.7021 - val_loss: 0.7752\nEpoch 18/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6902 - loss: 0.8260 - val_accuracy: 0.7242 - val_loss: 0.7503\nEpoch 19/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6819 - loss: 0.8303 - val_accuracy: 0.7140 - val_loss: 0.7563\nEpoch 20/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6878 - loss: 0.8254 - val_accuracy: 0.7069 - val_loss: 0.7680\nEpoch 21/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6893 - loss: 0.8314 - val_accuracy: 0.7238 - val_loss: 0.7363\nEpoch 22/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6886 - loss: 0.8115 - val_accuracy: 0.7123 - val_loss: 0.7684\nEpoch 23/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6902 - loss: 0.8112 - val_accuracy: 0.7276 - val_loss: 0.7269\nEpoch 24/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6894 - loss: 0.8087 - val_accuracy: 0.6804 - val_loss: 0.8312\nEpoch 25/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6904 - loss: 0.8036 - val_accuracy: 0.7198 - val_loss: 0.7306\nEpoch 26/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6976 - loss: 0.8009 - val_accuracy: 0.6960 - val_loss: 0.7700\nEpoch 27/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6892 - loss: 0.8177 - val_accuracy: 0.7204 - val_loss: 0.7248\nEpoch 28/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6949 - loss: 0.8148 - val_accuracy: 0.7303 - val_loss: 0.7075\nEpoch 29/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6905 - loss: 0.7966 - val_accuracy: 0.7238 - val_loss: 0.7419\nEpoch 30/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6992 - loss: 0.7955 - val_accuracy: 0.7177 - val_loss: 0.7414\nEpoch 31/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6955 - loss: 0.7960 - val_accuracy: 0.7337 - val_loss: 0.7346\nEpoch 32/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7003 - loss: 0.7990 - val_accuracy: 0.7069 - val_loss: 0.7502\nEpoch 33/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6917 - loss: 0.8001 - val_accuracy: 0.7228 - val_loss: 0.7387\nEpoch 34/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7038 - loss: 0.7696 - val_accuracy: 0.7334 - val_loss: 0.6947\nEpoch 35/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7002 - loss: 0.7682 - val_accuracy: 0.7137 - val_loss: 0.7529\nEpoch 36/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6984 - loss: 0.7757 - val_accuracy: 0.7204 - val_loss: 0.7350\nEpoch 37/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7108 - loss: 0.7595 - val_accuracy: 0.7228 - val_loss: 0.7101\nEpoch 38/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6973 - loss: 0.7750 - val_accuracy: 0.7340 - val_loss: 0.7046\nEpoch 39/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7121 - loss: 0.7550 - val_accuracy: 0.7395 - val_loss: 0.6789\nEpoch 40/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7152 - loss: 0.7479 - val_accuracy: 0.7238 - val_loss: 0.7243\nEpoch 41/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7031 - loss: 0.7636 - val_accuracy: 0.7310 - val_loss: 0.7043\nEpoch 42/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7063 - loss: 0.7620 - val_accuracy: 0.7188 - val_loss: 0.7164\nEpoch 43/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7085 - loss: 0.7492 - val_accuracy: 0.7221 - val_loss: 0.7168\nEpoch 44/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7123 - loss: 0.7463 - val_accuracy: 0.7272 - val_loss: 0.7116\nEpoch 45/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7103 - loss: 0.7568 - val_accuracy: 0.7313 - val_loss: 0.6973\nEpoch 46/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7128 - loss: 0.7479 - val_accuracy: 0.7381 - val_loss: 0.6892\nEpoch 47/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7144 - loss: 0.7447 - val_accuracy: 0.7405 - val_loss: 0.6827\nEpoch 48/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7159 - loss: 0.7388 - val_accuracy: 0.7313 - val_loss: 0.6895\nEpoch 49/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7080 - loss: 0.7620 - val_accuracy: 0.7269 - val_loss: 0.6946\nEpoch 50/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7163 - loss: 0.7447 - val_accuracy: 0.7473 - val_loss: 0.6546\nEpoch 51/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7171 - loss: 0.7289 - val_accuracy: 0.7347 - val_loss: 0.6794\nEpoch 52/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7107 - loss: 0.7468 - val_accuracy: 0.7323 - val_loss: 0.7018\nEpoch 53/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7238 - loss: 0.7251 - val_accuracy: 0.7469 - val_loss: 0.6701\nEpoch 54/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7106 - loss: 0.7433 - val_accuracy: 0.7347 - val_loss: 0.6757\nEpoch 55/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7219 - loss: 0.7141 - val_accuracy: 0.7374 - val_loss: 0.6913\nEpoch 56/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7160 - loss: 0.7305 - val_accuracy: 0.7412 - val_loss: 0.6782\nEpoch 57/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7152 - loss: 0.7427 - val_accuracy: 0.7242 - val_loss: 0.6932\nEpoch 58/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7295 - loss: 0.7098 - val_accuracy: 0.7554 - val_loss: 0.6496\nEpoch 59/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7245 - loss: 0.7270 - val_accuracy: 0.7401 - val_loss: 0.6700\nEpoch 60/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7285 - loss: 0.7220 - val_accuracy: 0.7476 - val_loss: 0.6557\nEpoch 61/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7158 - loss: 0.7225 - val_accuracy: 0.7381 - val_loss: 0.6634\nEpoch 62/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7191 - loss: 0.7323 - val_accuracy: 0.7480 - val_loss: 0.6615\nEpoch 63/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7194 - loss: 0.7242 - val_accuracy: 0.7215 - val_loss: 0.7070\nEpoch 64/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7326 - loss: 0.7127 - val_accuracy: 0.7293 - val_loss: 0.6942\nEpoch 65/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7235 - loss: 0.7031 - val_accuracy: 0.7334 - val_loss: 0.6898\nEpoch 66/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7214 - loss: 0.7166 - val_accuracy: 0.7493 - val_loss: 0.6547\nEpoch 67/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7282 - loss: 0.6956 - val_accuracy: 0.7378 - val_loss: 0.6681\nEpoch 68/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7209 - loss: 0.7289 - val_accuracy: 0.7361 - val_loss: 0.6725\nEpoch 69/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7143 - loss: 0.7239 - val_accuracy: 0.7364 - val_loss: 0.6743\nEpoch 70/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7205 - loss: 0.7200 - val_accuracy: 0.7565 - val_loss: 0.6440\nEpoch 71/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7295 - loss: 0.7036 - val_accuracy: 0.7439 - val_loss: 0.6543\nEpoch 72/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7223 - loss: 0.7196 - val_accuracy: 0.7442 - val_loss: 0.6582\nEpoch 73/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7339 - loss: 0.6881 - val_accuracy: 0.7425 - val_loss: 0.6639\nEpoch 74/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7243 - loss: 0.7181 - val_accuracy: 0.7463 - val_loss: 0.6573\nEpoch 75/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7219 - loss: 0.7180 - val_accuracy: 0.7395 - val_loss: 0.6766\nEpoch 76/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7243 - loss: 0.7138 - val_accuracy: 0.7418 - val_loss: 0.6650\nEpoch 77/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7249 - loss: 0.7133 - val_accuracy: 0.7565 - val_loss: 0.6503\nEpoch 78/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7164 - loss: 0.7165 - val_accuracy: 0.7497 - val_loss: 0.6560\nEpoch 79/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7253 - loss: 0.7006 - val_accuracy: 0.7541 - val_loss: 0.6534\nEpoch 80/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7240 - loss: 0.7197 - val_accuracy: 0.7317 - val_loss: 0.6936\nEpoch 81/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7292 - loss: 0.7054 - val_accuracy: 0.7435 - val_loss: 0.6655\nEpoch 82/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7267 - loss: 0.7117 - val_accuracy: 0.7401 - val_loss: 0.6628\nEpoch 83/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7260 - loss: 0.7124 - val_accuracy: 0.7439 - val_loss: 0.6575\nEpoch 84/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7345 - loss: 0.6920 - val_accuracy: 0.7497 - val_loss: 0.6446\nEpoch 85/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7217 - loss: 0.7306 - val_accuracy: 0.7514 - val_loss: 0.6477\nEpoch 85: early stopping\nRestoring model weights from the end of the best epoch: 70.\n\nTemps d'entraînement: 73.87 secondes\n\nÉvaluation du modèle sur l'ensemble de test...\nPrécision (accuracy): 0.7646\nTemps de prédiction: 0.98 secondes\n\nRapport de classification:\n                              precision    recall  f1-score   support\n\n                  Background       0.80      0.86      0.83      3233\n                     Alfalfa       0.80      0.86      0.83        14\n                 Corn-notill       0.78      0.63      0.70       428\n                Corn-mintill       0.79      0.57      0.66       249\n                        Corn       0.65      0.70      0.68        71\n               Grass-pasture       0.78      0.68      0.73       145\n                 Grass-trees       0.76      0.76      0.76       219\n         Grass-pasture-mowed       1.00      0.62      0.77         8\n               Hay-windrowed       0.89      0.97      0.93       143\n                        Oats       0.00      0.00      0.00         6\n              Soybean-notill       0.74      0.78      0.76       292\n             Soybean-mintill       0.66      0.85      0.74       737\n               Soybean-clean       0.66      0.79      0.72       178\n                       Wheat       0.67      0.95      0.79        61\n                       Woods       0.62      0.23      0.33       380\nBuildings-Grass-Trees-Drives       0.00      0.00      0.00       116\n          Stone-Steel-Towers       0.73      0.79      0.76        28\n\n                    accuracy                           0.76      6308\n                   macro avg       0.67      0.65      0.65      6308\n                weighted avg       0.75      0.76      0.75      6308\n\nImpossible de créer la visualisation du F1-score par classe - données insuffisantes\n\nModèle sauvegardé sous 'resultats_seg15_deflection/model_seg15_deflection.h5'\n\n================================================================================\nMODÈLE AVEC 20 SEGMENTS - COEFFICIENT DE DÉFLEXION\n================================================================================\n\nDivision du spectre en 20 segments de 10 bandes chacun\nSegment 1: bandes 0 à 9\n  → Meilleure bande: 1 (Coefficient: 0.0003)\nSegment 2: bandes 10 à 19\n  → Meilleure bande: 19 (Coefficient: 0.0007)\nSegment 3: bandes 20 à 29\n  → Meilleure bande: 25 (Coefficient: 0.0040)\nSegment 4: bandes 30 à 39\n  → Meilleure bande: 32 (Coefficient: 0.0014)\nSegment 5: bandes 40 à 49\n  → Meilleure bande: 40 (Coefficient: 0.0005)\nSegment 6: bandes 50 à 59\n  → Meilleure bande: 51 (Coefficient: 0.0030)\nSegment 7: bandes 60 à 69\n  → Meilleure bande: 64 (Coefficient: 0.0012)\nSegment 8: bandes 70 à 79\n  → Meilleure bande: 73 (Coefficient: 0.0027)\nSegment 9: bandes 80 à 89\n  → Meilleure bande: 81 (Coefficient: 0.0009)\nSegment 10: bandes 90 à 99\n  → Meilleure bande: 98 (Coefficient: 0.0025)\nSegment 11: bandes 100 à 109\n  → Meilleure bande: 101 (Coefficient: 0.0016)\nSegment 12: bandes 110 à 119\n  → Meilleure bande: 111 (Coefficient: 0.0010)\nSegment 13: bandes 120 à 129\n  → Meilleure bande: 120 (Coefficient: 0.0004)\nSegment 14: bandes 130 à 139\n  → Meilleure bande: 137 (Coefficient: 0.0012)\nSegment 15: bandes 140 à 149\n  → Meilleure bande: 149 (Coefficient: 0.0007)\nSegment 16: bandes 150 à 159\n  → Meilleure bande: 157 (Coefficient: 0.0086)\nSegment 17: bandes 160 à 169\n  → Meilleure bande: 160 (Coefficient: 0.0053)\nSegment 18: bandes 170 à 179\n  → Meilleure bande: 176 (Coefficient: 0.0033)\nSegment 19: bandes 180 à 189\n  → Meilleure bande: 186 (Coefficient: 0.0043)\nSegment 20: bandes 190 à 199\n  → Meilleure bande: 194 (Coefficient: 0.0023)\nInformations sur les segments sauvegardées dans 'segments_info_20.csv'\nNombre de classes (avec background): 17\nDimensions des données: (21025, 20)\nEnsemble d'entraînement: (14717, 20), (14717, 17)\nEnsemble de test: (6308, 20), (6308, 17)\n\nCréation et entraînement du modèle seg20_deflection...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_7\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_7\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ input_layer_7 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_35 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │          \u001b[38;5;34m10,752\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_28               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │           \u001b[38;5;34m2,048\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_28 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_36 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │          \u001b[38;5;34m32,832\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_29               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │             \u001b[38;5;34m256\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_29 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_37 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │           \u001b[38;5;34m4,160\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_30               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │             \u001b[38;5;34m256\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_30 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_38 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │           \u001b[38;5;34m2,080\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_31               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │             \u001b[38;5;34m128\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_31 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_39 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m)                  │             \u001b[38;5;34m561\u001b[0m │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ input_layer_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_35 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">10,752</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_28               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_36 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">32,832</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_29               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_37 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_30               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_38 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_31               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_39 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">561</span> │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m53,073\u001b[0m (207.32 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">53,073</span> (207.32 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m51,729\u001b[0m (202.07 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">51,729</span> (202.07 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,344\u001b[0m (5.25 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,344</span> (5.25 KB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.4187 - loss: 1.9861 - val_accuracy: 0.6118 - val_loss: 1.1105\nEpoch 2/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6013 - loss: 1.1371 - val_accuracy: 0.6213 - val_loss: 0.9853\nEpoch 3/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6367 - loss: 1.0137 - val_accuracy: 0.6844 - val_loss: 0.8730\nEpoch 4/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6452 - loss: 0.9759 - val_accuracy: 0.6858 - val_loss: 0.8617\nEpoch 5/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6483 - loss: 0.9655 - val_accuracy: 0.6576 - val_loss: 0.8929\nEpoch 6/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6642 - loss: 0.9002 - val_accuracy: 0.6722 - val_loss: 0.8548\nEpoch 7/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6649 - loss: 0.9024 - val_accuracy: 0.7069 - val_loss: 0.7700\nEpoch 8/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6755 - loss: 0.8769 - val_accuracy: 0.7055 - val_loss: 0.7789\nEpoch 9/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6757 - loss: 0.8629 - val_accuracy: 0.6926 - val_loss: 0.7886\nEpoch 10/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6889 - loss: 0.8367 - val_accuracy: 0.6960 - val_loss: 0.7867\nEpoch 11/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6707 - loss: 0.8748 - val_accuracy: 0.7001 - val_loss: 0.8084\nEpoch 12/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6816 - loss: 0.8487 - val_accuracy: 0.7133 - val_loss: 0.7356\nEpoch 13/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6937 - loss: 0.8140 - val_accuracy: 0.7004 - val_loss: 0.7623\nEpoch 14/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6962 - loss: 0.8094 - val_accuracy: 0.7092 - val_loss: 0.7424\nEpoch 15/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6986 - loss: 0.8001 - val_accuracy: 0.7126 - val_loss: 0.7539\nEpoch 16/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6963 - loss: 0.7977 - val_accuracy: 0.6943 - val_loss: 0.7904\nEpoch 17/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6900 - loss: 0.8032 - val_accuracy: 0.7252 - val_loss: 0.7128\nEpoch 18/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6990 - loss: 0.7942 - val_accuracy: 0.7137 - val_loss: 0.7321\nEpoch 19/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7007 - loss: 0.7954 - val_accuracy: 0.7388 - val_loss: 0.6849\nEpoch 20/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7176 - loss: 0.7620 - val_accuracy: 0.7266 - val_loss: 0.6802\nEpoch 21/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7015 - loss: 0.7912 - val_accuracy: 0.7303 - val_loss: 0.7035\nEpoch 22/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7035 - loss: 0.7705 - val_accuracy: 0.7232 - val_loss: 0.7049\nEpoch 23/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7097 - loss: 0.7601 - val_accuracy: 0.7452 - val_loss: 0.6804\nEpoch 24/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7076 - loss: 0.7628 - val_accuracy: 0.7500 - val_loss: 0.6649\nEpoch 25/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7075 - loss: 0.7554 - val_accuracy: 0.6970 - val_loss: 0.7800\nEpoch 26/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7051 - loss: 0.7630 - val_accuracy: 0.7340 - val_loss: 0.6935\nEpoch 27/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7217 - loss: 0.7497 - val_accuracy: 0.7317 - val_loss: 0.6898\nEpoch 28/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7095 - loss: 0.7467 - val_accuracy: 0.7099 - val_loss: 0.7567\nEpoch 29/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7146 - loss: 0.7584 - val_accuracy: 0.7493 - val_loss: 0.6389\nEpoch 30/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7211 - loss: 0.7369 - val_accuracy: 0.7473 - val_loss: 0.6574\nEpoch 31/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7222 - loss: 0.7326 - val_accuracy: 0.7242 - val_loss: 0.7108\nEpoch 32/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7255 - loss: 0.7332 - val_accuracy: 0.7422 - val_loss: 0.6593\nEpoch 33/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7303 - loss: 0.7131 - val_accuracy: 0.7340 - val_loss: 0.6696\nEpoch 34/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7175 - loss: 0.7278 - val_accuracy: 0.7449 - val_loss: 0.6445\nEpoch 35/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7204 - loss: 0.7222 - val_accuracy: 0.7544 - val_loss: 0.6269\nEpoch 36/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7234 - loss: 0.7258 - val_accuracy: 0.7449 - val_loss: 0.6673\nEpoch 37/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7339 - loss: 0.6976 - val_accuracy: 0.7527 - val_loss: 0.6396\nEpoch 38/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7289 - loss: 0.7183 - val_accuracy: 0.7503 - val_loss: 0.6509\nEpoch 39/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7220 - loss: 0.7277 - val_accuracy: 0.7381 - val_loss: 0.6573\nEpoch 40/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7278 - loss: 0.7085 - val_accuracy: 0.7490 - val_loss: 0.6667\nEpoch 41/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7301 - loss: 0.7156 - val_accuracy: 0.7626 - val_loss: 0.6225\nEpoch 42/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7259 - loss: 0.7066 - val_accuracy: 0.7276 - val_loss: 0.7044\nEpoch 43/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7267 - loss: 0.7058 - val_accuracy: 0.7551 - val_loss: 0.6337\nEpoch 44/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7337 - loss: 0.7027 - val_accuracy: 0.7500 - val_loss: 0.6296\nEpoch 45/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7288 - loss: 0.7096 - val_accuracy: 0.7422 - val_loss: 0.6580\nEpoch 46/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7310 - loss: 0.6973 - val_accuracy: 0.7615 - val_loss: 0.6189\nEpoch 47/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7361 - loss: 0.7068 - val_accuracy: 0.7599 - val_loss: 0.6194\nEpoch 48/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7287 - loss: 0.6945 - val_accuracy: 0.7636 - val_loss: 0.6262\nEpoch 49/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7333 - loss: 0.6990 - val_accuracy: 0.7480 - val_loss: 0.6564\nEpoch 50/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7380 - loss: 0.6764 - val_accuracy: 0.7520 - val_loss: 0.6230\nEpoch 51/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7377 - loss: 0.6862 - val_accuracy: 0.7401 - val_loss: 0.6894\nEpoch 52/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7367 - loss: 0.6890 - val_accuracy: 0.7646 - val_loss: 0.6215\nEpoch 53/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7327 - loss: 0.6923 - val_accuracy: 0.7656 - val_loss: 0.6344\nEpoch 54/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7382 - loss: 0.6797 - val_accuracy: 0.7565 - val_loss: 0.6376\nEpoch 55/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7305 - loss: 0.6922 - val_accuracy: 0.7520 - val_loss: 0.6455\nEpoch 56/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7452 - loss: 0.6720 - val_accuracy: 0.7503 - val_loss: 0.6428\nEpoch 57/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7425 - loss: 0.6692 - val_accuracy: 0.7582 - val_loss: 0.6156\nEpoch 58/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7408 - loss: 0.6801 - val_accuracy: 0.7582 - val_loss: 0.6201\nEpoch 59/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7420 - loss: 0.6707 - val_accuracy: 0.7571 - val_loss: 0.6312\nEpoch 60/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7407 - loss: 0.6825 - val_accuracy: 0.7615 - val_loss: 0.6105\nEpoch 61/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7467 - loss: 0.6650 - val_accuracy: 0.7626 - val_loss: 0.6096\nEpoch 62/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7410 - loss: 0.6721 - val_accuracy: 0.7632 - val_loss: 0.6180\nEpoch 63/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7459 - loss: 0.6601 - val_accuracy: 0.7578 - val_loss: 0.6069\nEpoch 64/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7344 - loss: 0.6774 - val_accuracy: 0.7673 - val_loss: 0.6008\nEpoch 65/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7400 - loss: 0.6761 - val_accuracy: 0.7500 - val_loss: 0.6458\nEpoch 66/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7446 - loss: 0.6623 - val_accuracy: 0.7653 - val_loss: 0.5986\nEpoch 67/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7402 - loss: 0.6626 - val_accuracy: 0.7717 - val_loss: 0.6039\nEpoch 68/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7390 - loss: 0.6736 - val_accuracy: 0.7704 - val_loss: 0.6137\nEpoch 69/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7445 - loss: 0.6690 - val_accuracy: 0.7690 - val_loss: 0.6105\nEpoch 70/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7411 - loss: 0.6777 - val_accuracy: 0.7588 - val_loss: 0.6195\nEpoch 71/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7365 - loss: 0.6867 - val_accuracy: 0.7687 - val_loss: 0.5967\nEpoch 72/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7487 - loss: 0.6443 - val_accuracy: 0.7575 - val_loss: 0.6186\nEpoch 73/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7480 - loss: 0.6548 - val_accuracy: 0.7694 - val_loss: 0.6026\nEpoch 74/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7459 - loss: 0.6596 - val_accuracy: 0.7571 - val_loss: 0.6194\nEpoch 75/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7400 - loss: 0.6537 - val_accuracy: 0.7751 - val_loss: 0.5792\nEpoch 76/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7437 - loss: 0.6580 - val_accuracy: 0.7690 - val_loss: 0.6012\nEpoch 77/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7526 - loss: 0.6562 - val_accuracy: 0.7507 - val_loss: 0.6296\nEpoch 78/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7508 - loss: 0.6548 - val_accuracy: 0.7663 - val_loss: 0.5964\nEpoch 79/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7423 - loss: 0.6676 - val_accuracy: 0.7687 - val_loss: 0.6058\nEpoch 80/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7444 - loss: 0.6523 - val_accuracy: 0.7802 - val_loss: 0.5804\nEpoch 81/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7438 - loss: 0.6622 - val_accuracy: 0.7724 - val_loss: 0.5776\nEpoch 82/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7475 - loss: 0.6547 - val_accuracy: 0.7673 - val_loss: 0.6064\nEpoch 83/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7457 - loss: 0.6432 - val_accuracy: 0.7629 - val_loss: 0.6100\nEpoch 84/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7462 - loss: 0.6582 - val_accuracy: 0.7653 - val_loss: 0.5944\nEpoch 85/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7523 - loss: 0.6447 - val_accuracy: 0.7663 - val_loss: 0.5970\nEpoch 86/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7526 - loss: 0.6465 - val_accuracy: 0.7755 - val_loss: 0.5838\nEpoch 87/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7486 - loss: 0.6467 - val_accuracy: 0.7632 - val_loss: 0.6163\nEpoch 88/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7478 - loss: 0.6542 - val_accuracy: 0.7704 - val_loss: 0.5986\nEpoch 89/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7562 - loss: 0.6283 - val_accuracy: 0.7626 - val_loss: 0.6213\nEpoch 90/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7543 - loss: 0.6457 - val_accuracy: 0.7683 - val_loss: 0.6083\nEpoch 91/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7589 - loss: 0.6369 - val_accuracy: 0.7738 - val_loss: 0.5746\nEpoch 92/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7585 - loss: 0.6254 - val_accuracy: 0.7704 - val_loss: 0.5828\nEpoch 93/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7520 - loss: 0.6542 - val_accuracy: 0.7734 - val_loss: 0.5719\nEpoch 94/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7497 - loss: 0.6369 - val_accuracy: 0.7840 - val_loss: 0.5653\nEpoch 95/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7521 - loss: 0.6407 - val_accuracy: 0.7789 - val_loss: 0.5754\nEpoch 96/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7420 - loss: 0.6457 - val_accuracy: 0.7836 - val_loss: 0.5687\nEpoch 97/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7450 - loss: 0.6494 - val_accuracy: 0.7649 - val_loss: 0.5974\nEpoch 98/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7533 - loss: 0.6335 - val_accuracy: 0.7745 - val_loss: 0.5841\nEpoch 99/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7572 - loss: 0.6299 - val_accuracy: 0.7446 - val_loss: 0.6528\nEpoch 100/100\n\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7489 - loss: 0.6439 - val_accuracy: 0.7486 - val_loss: 0.6430\nRestoring model weights from the end of the best epoch: 94.\n\nTemps d'entraînement: 86.43 secondes\n\nÉvaluation du modèle sur l'ensemble de test...\nPrécision (accuracy): 0.7850\nTemps de prédiction: 0.98 secondes\n\nRapport de classification:\n                              precision    recall  f1-score   support\n\n                  Background       0.82      0.86      0.84      3233\n                     Alfalfa       0.76      0.93      0.84        14\n                 Corn-notill       0.74      0.77      0.76       428\n                Corn-mintill       0.74      0.71      0.73       249\n                        Corn       0.72      0.69      0.71        71\n               Grass-pasture       0.84      0.69      0.76       145\n                 Grass-trees       0.73      0.83      0.77       219\n         Grass-pasture-mowed       0.73      1.00      0.84         8\n               Hay-windrowed       0.87      0.99      0.93       143\n                        Oats       0.00      0.00      0.00         6\n              Soybean-notill       0.77      0.82      0.79       292\n             Soybean-mintill       0.73      0.85      0.79       737\n               Soybean-clean       0.76      0.82      0.79       178\n                       Wheat       0.83      0.95      0.89        61\n                       Woods       0.67      0.23      0.34       380\nBuildings-Grass-Trees-Drives       0.00      0.00      0.00       116\n          Stone-Steel-Towers       0.64      0.89      0.75        28\n\n                    accuracy                           0.79      6308\n                   macro avg       0.67      0.71      0.68      6308\n                weighted avg       0.77      0.79      0.77      6308\n\nImpossible de créer la visualisation du F1-score par classe - données insuffisantes\n\nModèle sauvegardé sous 'resultats_seg20_deflection/model_seg20_deflection.h5'\n\nTableau de comparaison sauvegardé dans 'resultats_comparaison_segments_deflection/comparaison_modeles_segments.csv'\n\nAnalyse comparative terminée!\n\nRécapitulatif des précisions:\n5 segments (5 bandes): 0.7432\n10 segments (10 bandes): 0.7617\n15 segments (15 bandes): 0.7646\n20 segments (20 bandes): 0.7850\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}